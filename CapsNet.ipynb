{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist:\n",
    "    def __init__(self, path, train=True):\n",
    "        if train:\n",
    "            image_file_name = \"train-images.idx3-ubyte\"\n",
    "            label_file_name = \"train-labels.idx1-ubyte\"\n",
    "        else:\n",
    "            image_file_name = \"t10k-images.idx3-ubyte\"\n",
    "            label_file_name = \"t10k-labels.idx1-ubyte\"\n",
    "            \n",
    "        self.f = open(\"{}//{}\".format(path, image_file_name), \"rb\")\n",
    "        magic_number, number_of_images, image_rows, image_columns = int(self.f.read(4).hex(), 16), \\\n",
    "        int(self.f.read(4).hex(), 16), int(self.f.read(4).hex(), 16), int(self.f.read(4).hex(), 16)\n",
    "        print(\"Images magic no: {},  No of images: {}, Image rows: {}, Image cols: {}\".\\\n",
    "          format(magic_number, number_of_images, image_rows, image_columns))\n",
    "\n",
    "        self.flab = open(\"{}//{}\".format(path, label_file_name), \"rb\")\n",
    "        magic_number, number_of_items = int(self.flab.read(4).hex(), 16), int(self.flab.read(4).hex(), 16)\n",
    "        print(\"Labels magic no: {}, No of items: {}\".format(magic_number, number_of_items))\n",
    "        \n",
    "\n",
    "    def _get_next(self):\n",
    "        val = self.f.read(28*28)\n",
    "        if len(val)>0:\n",
    "            img = val.hex()\n",
    "            idxs = np.arange(0, len(img), 2).astype(int)\n",
    "            return(np.reshape([int(img[i:(i+2)], 16) for i in idxs], (28, 28)), int(self.flab.read(1).hex(), 16)) \n",
    "        else:\n",
    "            return(\"\", \"\")\n",
    "\n",
    "    def get_batch(self, batch_size=20, scale=True):\n",
    "        im_lst = np.zeros(shape=[batch_size, 28, 28], dtype=\"float\")\n",
    "        lbl_lst = np.zeros(shape=[batch_size], dtype=\"int\")\n",
    "        lbl_one_hot_lst = np.zeros(shape=[batch_size, 10], dtype=\"int\")\n",
    "        for i in range(batch_size):\n",
    "            im, lbl = self._get_next()\n",
    "           \n",
    "            if len(im)>0:\n",
    "                if scale:\n",
    "                    im=im/255.\n",
    "                im_lst[i,...] = im\n",
    "                lbl_lst[i] = lbl\n",
    "                lbl_one_hot_lst[i,lbl] = 1\n",
    "        return im_lst, lbl_lst, lbl_one_hot_lst\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_s(b_prior, u_hat):\n",
    "    c = tf.nn.softmax(b_prior, dim=2)\n",
    "    s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1)\n",
    "    return(s, c)\n",
    "\n",
    "\n",
    "#Squash\n",
    "def get_v(s):\n",
    "    norm_s = tf.norm(s, axis=2, keep_dims=True)\n",
    "    norm_s_2 = tf.pow(norm_s, 2)\n",
    "    v = (tf.multiply(norm_s_2,s))/(tf.multiply(1+norm_s_2, norm_s))\n",
    "\n",
    "    return(v)\n",
    "\n",
    "\n",
    "def route(n_iter, batch_item_cnt, v, b_prior, u_hat, aggr):\n",
    "  \n",
    "    for i in range(n_iter):\n",
    "        s, c = get_s(b_prior, u_hat)\n",
    "        v = get_v(s)\n",
    "        v = tf.stop_gradient(v) if i<(n_iter-1) else v\n",
    "        aggr = tf.reduce_sum(tf.multiply(v[batch_item_cnt, :, :], u_hat[batch_item_cnt, :, :]), axis=-1)\n",
    "        aggr = tf.expand_dims(aggr, axis=0)\n",
    "        aggr = tf.expand_dims(aggr, axis=3)\n",
    "        b_prior = tf.add(b_prior, aggr)\n",
    "    return (v, b_prior, aggr)\n",
    "\n",
    "def update_prior(batch_item_cnt, v, b_prior, u_hat, aggr):\n",
    "    v, b_prior, aggr = route(3, batch_item_cnt, v, b_prior, u_hat, aggr)\n",
    "    return([tf.add(batch_item_cnt, 1), v, b_prior, u_hat, aggr])\n",
    "\n",
    "def update_prior_10(batch_item_cnt, v, b_prior, u_hat, aggr):\n",
    "    v, b_prior, aggr = route(10, batch_item_cnt, v, b_prior, u_hat, aggr)\n",
    "    return([tf.add(batch_item_cnt, 1), v, b_prior, u_hat])\n",
    "\n",
    "def build_graph(batch_size, is_train=True, high_routing= False, add_summaries=False, print_shape=False):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    end_points = dict()\n",
    "\n",
    "    with graph.as_default():\n",
    "\n",
    "        X = tf.placeholder(shape=[batch_size, 28, 28, 1], name=\"X\", dtype=tf.float32)\n",
    "        end_points[\"X\"] = X\n",
    "        Y = tf.placeholder(shape=[batch_size], name=\"Y\", dtype=tf.int32)\n",
    "        end_points[\"Y\"] = Y\n",
    "        Y_one_hot = tf.one_hot(indices=Y, depth=10)\n",
    "        end_points[\"Y_one_hot\"] = Y_one_hot\n",
    "\n",
    "        #First Layer\n",
    "        with tf.name_scope(\"Conv_Layer_1\"):\n",
    "\n",
    "            conv_1 = tf.contrib.layers.conv2d(inputs=X, \n",
    "                                              num_outputs=256,  \n",
    "                                              kernel_size=9, \n",
    "                                              stride=1, padding=\"VALID\", \n",
    "                                              activation_fn=tf.nn.relu, \n",
    "                                              weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(), \n",
    "                                              biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "            end_points[\"conv_layer_1_act\"] = conv_1\n",
    "\n",
    "        capsules = list()\n",
    "\n",
    "        #In this implementation every capsule layer has unique weights\n",
    "        #I think the meaning of sharing is that in each plane [6, 6, 1]\n",
    "        #each cepsule output sharing the weights\n",
    "        with tf.variable_scope(\"Capsule_Layer\"):\n",
    "            for n in range(32):\n",
    "\n",
    "               #If we user auto resuse then the output of each capsule would be the same and it doesn't make any sense\n",
    "\n",
    "                cap_1 = tf.contrib.layers.conv2d(inputs=conv_1, \n",
    "                                                 #reuse=tf.AUTO_REUSE, \n",
    "                                                 #scope=\"Capsule_Layer\",\n",
    "                                                 num_outputs=8, kernel_size=9, \n",
    "                                                 activation_fn=None,\n",
    "                                                 stride=2, padding=\"VALID\" ,\n",
    "                                                 weights_initializer=tf.variance_scaling_initializer(),\n",
    "                                                 biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "                end_points[\"capsule_{}_act\".format(n)] = cap_1\n",
    "                capsules.append(tf.expand_dims(cap_1, axis=3))\n",
    "\n",
    "        with tf.name_scope(\"Transformation\"):\n",
    "            u = tf.concat(capsules, axis=3)\n",
    "            end_points[\"u_b_r\"] = u\n",
    "            u = tf.reshape(u, shape=[-1, 1152, 8])\n",
    "            u = tf.expand_dims(tf.expand_dims(u, axis=2), axis=2)\n",
    "            end_points[\"u\"] = u\n",
    "\n",
    "            W = tf.get_variable(shape=[1, 1152, 10, 16, 8], name=\"W_t\", \n",
    "                                initializer=tf.variance_scaling_initializer())\n",
    "\n",
    "\n",
    "            end_points[\"W_t\"] = W\n",
    "\n",
    "            b = tf.get_variable(shape=[1, 1152, 10, 1], name=\"b_t\", \n",
    "                                initializer=tf.zeros_initializer())\n",
    "\n",
    "            end_points[\"b_t\"] = b\n",
    "\n",
    "            u_hat =tf.add(tf.reduce_sum(tf.multiply(u, W), axis=4), b)\n",
    "            end_points[\"u_hat\"] = u_hat\n",
    "\n",
    "            b_prior = tf.get_variable(shape=[1, 1152, 10, 1], name=\"b_prior\", initializer=tf.zeros_initializer(), \n",
    "                                      trainable=False)\n",
    "            \n",
    "            aggr = tf.get_variable(shape=[1, 1152, 10, 1], name=\"aggr\", initializer=tf.zeros_initializer(), \n",
    "                                      trainable=False)\n",
    "\n",
    "            end_points[\"b_prior\"] = b_prior\n",
    "\n",
    "            s, c = get_s(b_prior, u_hat)\n",
    "            end_points[\"s\"] = s\n",
    "            end_points[\"c\"] = c\n",
    "\n",
    "            v = get_v(s)\n",
    "\n",
    "            end_points[\"v\"] = v\n",
    "\n",
    "            #if add_summaries:\n",
    "                #tf.summary.histogram(\"v_before_routing\", values=v)\n",
    "\n",
    "        if is_train:\n",
    "            with tf.name_scope(\"Routing\"):\n",
    "\n",
    "                #Routing\n",
    "                batch_item_cnt = tf.Variable(initial_value=0, trainable=False)\n",
    "\n",
    "                batch_item_cnt.assign(0)\n",
    "                end_of_batch = lambda batch_item_cnt, v, b_prior, u_hat, aggr: tf.less(batch_item_cnt, batch_size)\n",
    "                if high_routing:\n",
    "                    route_op = tf.while_loop(body=update_prior_10, cond=end_of_batch, \n",
    "                                         loop_vars=[batch_item_cnt, v, b_prior, u_hat, aggr])\n",
    "                    print(\"Using routing 10\")\n",
    "                else:\n",
    "                    route_op = tf.while_loop(body=update_prior, cond=end_of_batch, \n",
    "                                         loop_vars=[batch_item_cnt, v, b_prior, u_hat, aggr])\n",
    "                    print(\"Using routing 3\")\n",
    "                    \n",
    "                end_points[\"route_op\"] = route_op\n",
    "\n",
    "                [batch_item_cnt, v, b_prior_updated, u_hat, aggr] = route_op\n",
    "                end_points[\"aggr\"] = aggr\n",
    "                b_prior = tf.assign(b_prior, b_prior_updated)\n",
    "\n",
    "\n",
    "                end_points[\"batch_item_cnt\"] = batch_item_cnt\n",
    "\n",
    "\n",
    "\n",
    "        s, c = get_s(b_prior, u_hat)\n",
    "        v = get_v(s)\n",
    "        end_points[\"s_routed\"] = s\n",
    "        end_points[\"c_routed\"] = c\n",
    "\n",
    "        with tf.name_scope(\"Digicaps\"):\n",
    "\n",
    "\n",
    "            end_points[\"b_prior_routed\"] = b_prior\n",
    "            end_points[\"u_hat_routed\"] = u_hat\n",
    "            end_points[\"v_routed\"] = v\n",
    "            end_points[\"s_routed\"] = s\n",
    "            end_points[\"c_routed\"] = c\n",
    "        #if add_summaries:\n",
    "            #tf.summary.histogram(name=\"v_after_routing\", values=v)\n",
    "\n",
    "        with tf.name_scope(\"Prepare_for_FC\"):\n",
    "            Y_one_hot_ex = tf.expand_dims(Y_one_hot, axis=1)\n",
    "            end_points[\"Y_one_hot_ex\"] = Y_one_hot_ex\n",
    "\n",
    "            v_masked = tf.squeeze(tf.matmul(Y_one_hot_ex, v))\n",
    "            end_points[\"v_masked\"] = v_masked\n",
    "            \n",
    "        with tf.name_scope(\"Fully_connected\"):\n",
    "            fc_1 = tf.contrib.layers.fully_connected(inputs=v_masked, num_outputs=512, \n",
    "                                                     activation_fn=tf.nn.relu,\n",
    "                                                     weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                     biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "            end_points[\"fc_1\"] = fc_1\n",
    "            #print(\"fc_1 shape={}\".format(fc_1.shape), end =\" \")\n",
    "\n",
    "            fc_2 = tf.contrib.layers.fully_connected(inputs=fc_1, num_outputs=1024, \n",
    "                                                     activation_fn=tf.nn.relu,\n",
    "                                                     weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                     biases_initializer=tf.zeros_initializer())\n",
    "            end_points[\"fc_2\"] = fc_2\n",
    "            #print(\"fc_2 shape={}\".format(fc_2.shape), end =\" \")\n",
    "\n",
    "            fc_out = tf.contrib.layers.fully_connected(inputs=fc_2, num_outputs=784, \n",
    "                                                     activation_fn=tf.nn.sigmoid,\n",
    "                                                     weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                     biases_initializer=tf.zeros_initializer())\n",
    "            end_points[\"fc_out\"] = fc_out\n",
    "            #print(\"fc_out shape={}\".format(fc_out.shape), end =\" \")\n",
    "\n",
    "\n",
    "\n",
    "                   #Calsulatong loss for each capsule\n",
    "            lambd_a = 0.5\n",
    "\n",
    "\n",
    "            #margin_loss =Y_one_hot_ex \n",
    "            v_norm = tf.norm(v, axis=2)\n",
    "            end_points[\"v_norm\"] = v_norm\n",
    "\n",
    "\n",
    "            margin_loss_present = tf.multiply(tf.pow(tf.maximum(0., 0.9 - v_norm), 2), Y_one_hot)\n",
    "            end_points[\"margin_loss_present\"] = margin_loss_present\n",
    "\n",
    "\n",
    "            margin_loss_not_present = lambd_a * tf.multiply(tf.pow(tf.maximum(0., v_norm - 0.1), 2), (1 - Y_one_hot))\n",
    "            end_points[\"margin_loss_not_present\"] = margin_loss_not_present\n",
    "\n",
    "\n",
    "            total_margin_loss = tf.reduce_sum(tf.add(margin_loss_present, margin_loss_not_present), axis=1)\n",
    "            end_points[\"total_margin_loss\"] = total_margin_loss\n",
    "            \n",
    "            scaler = 0.0005\n",
    "            reconstruction_loss = tf.multiply(scaler, \n",
    "                                              tf.reduce_sum(\n",
    "                                                  tf.squared_difference(\n",
    "                                                      tf.reshape(X, shape=[-1, 784]), fc_out), axis=1))\n",
    "            end_points[\"reconstruction_loss\"] = reconstruction_loss\n",
    "\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.add(total_margin_loss, reconstruction_loss))\n",
    "            end_points[\"loss\"] = loss\n",
    "            \n",
    "            if add_summaries:\n",
    "                tf.summary.scalar(\"loss_avg\", loss)\n",
    "\n",
    "\n",
    "\n",
    "            if is_train:\n",
    "                optimizer = tf.train.AdamOptimizer()\n",
    "                grads = optimizer.compute_gradients(loss)\n",
    "                end_points[\"grads\"] = grads\n",
    "                optimize = optimizer.apply_gradients(grads)\n",
    "                end_points[\"optimize\"] = optimize\n",
    "\n",
    "            if add_summaries:\n",
    "                summaries_merged = tf.summary.merge_all()\n",
    "                end_points[\"summaries_merged\"] = summaries_merged\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        end_points[\"init\"] = init\n",
    "        if print_shape:\n",
    "            for k in end_points.keys():\n",
    "                k_shape=\"NA\"\n",
    "                try:\n",
    "                    k_shape = str(end_points[k].shape)\n",
    "                except AttributeError:\n",
    "                    k_shape = type(end_points[k])\n",
    "\n",
    "                print(\"\\r {} shape={}\".format(k, k_shape))\n",
    "\n",
    "        return(graph, end_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is with routing when I disable the routing everythinbg seems fine\n",
    "- Action: try with none batch of 60 and see how b_prior chnages \n",
    "    - The problem is with the routing algorithms look at the current implementations\n",
    "      some use the stop gradient function to make it work. the problem is there lloka rpouind \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using routing 3\n",
      "Images magic no: 2051,  No of images: 60000, Image rows: 28, Image cols: 28\n",
      "Labels magic no: 2049, No of items: 60000\n",
      "----- Epoch 0 Loss:0.9001719355583191-----c: [ 0.09999897  0.09999897  0.09999897  0.09999897  0.09999897  0.09999897\n",
      "  0.09999897  0.09999897  0.09999897  0.09999897] --u_hat 6\n",
      "----- Epoch 0 Loss:0.7267899513244629-----c: [ 0.10064867  0.10040376  0.10058739  0.09916154  0.10061565  0.10060498\n",
      "  0.09916152  0.09916157  0.09916154  0.10049354] --u_hat 0\n",
      "----- Epoch 0 Loss:1.0509648323059082-----c: [ 0.0976347   0.10708978  0.09756611  0.09764081  0.10766241  0.10663421\n",
      "  0.09608519  0.09608519  0.09608527  0.09751626] --u_hat 4\n",
      "----- Epoch 0 Loss:0.7843363881111145-----c: [ 0.09697973  0.10850625  0.09669541  0.10031777  0.10705373  0.10627137\n",
      "  0.09568817  0.09573886  0.09587558  0.0968732 ] --u_hat 3\n",
      "----- Epoch 0 Loss:1.210477590560913-----c: [ 0.1140355   0.10168911  0.09425597  0.09811378  0.10025128  0.09968051\n",
      "  0.09700204  0.08954023  0.09005612  0.11537527] --u_hat 9\n",
      "----- Epoch 0 Loss:1.0613712072372437-----c: [ 0.11881296  0.10512136  0.09627574  0.09598166  0.1000496   0.09388156\n",
      "  0.09210678  0.08802697  0.08505886  0.12468469] --u_hat 9\n",
      "----- Epoch 0 Loss:0.7901651859283447-----c: [ 0.11625239  0.11529866  0.09422772  0.09420867  0.1019752   0.09314469\n",
      "  0.09077018  0.0869222   0.08406179  0.12313865] --u_hat 1\n",
      "----- Epoch 0 Loss:0.9231159687042236-----c: [ 0.11396767  0.12581608  0.09328378  0.09198162  0.10134094  0.0922297\n",
      "  0.09078437  0.08623639  0.08414754  0.12021194] --u_hat 1\n",
      "----- Epoch 0 Loss:0.9582632184028625-----c: [ 0.11204757  0.13313679  0.09383398  0.08913323  0.09860809  0.09105653\n",
      "  0.09139797  0.08728103  0.08634242  0.11716255] --u_hat 1\n",
      "----- Epoch 0 Loss:0.7086355090141296-----c: [ 0.11207555  0.13536459  0.09439545  0.0860939   0.09539554  0.08939129\n",
      "  0.09195241  0.09092177  0.0878886   0.11652073] --u_hat 7\n",
      "----- Epoch 0 Loss:0.9263410568237305-----c: [ 0.11453739  0.13301149  0.09367707  0.0831392   0.09225135  0.08737483\n",
      "  0.09183456  0.09374212  0.08896721  0.12146486] --u_hat 7\n",
      "----- Epoch 0 Loss:0.7447128891944885-----c: [ 0.11871693  0.1289354   0.09173486  0.08051431  0.08981019  0.08585899\n",
      "  0.0914717   0.0943631   0.08866535  0.12992935] --u_hat 9\n",
      "----- Epoch 0 Loss:0.9259664416313171-----c: [ 0.12424966  0.12530218  0.08946484  0.07823437  0.08828364  0.08437771\n",
      "  0.09131575  0.09343623  0.08684036  0.1384948 ] --u_hat 9\n",
      "----- Epoch 0 Loss:0.7149615287780762-----c: [ 0.12920071  0.12248176  0.08766242  0.07647008  0.08792175  0.08306724\n",
      "  0.09189651  0.09177474  0.0849287   0.14459603] --u_hat 0\n",
      "----- Epoch 0 Loss:0.7223417162895203-----c: [ 0.13252394  0.12034792  0.08636198  0.07514068  0.08850412  0.08210811\n",
      "  0.09332141  0.09036914  0.08344652  0.14787611] --u_hat 6\n",
      "----- Epoch 0 Loss:0.6023616194725037-----c: [ 0.13363729  0.11898839  0.08567521  0.07431985  0.09042198  0.08149745\n",
      "  0.0951204   0.08954984  0.08258782  0.14820172] --u_hat 4\n",
      "----- Epoch 0 Loss:0.8257724642753601-----c: [ 0.1334551   0.11770634  0.08504536  0.07357409  0.09347577  0.08088806\n",
      "  0.09773272  0.088985    0.08235651  0.146781  ] --u_hat 4\n",
      "----- Epoch 0 Loss:0.7934560775756836-----c: [ 0.13197066  0.11616794  0.08425006  0.07281088  0.09669837  0.08021513\n",
      "  0.10011223  0.08914858  0.08374825  0.14487784] --u_hat 4\n",
      "----- Epoch 0 Loss:0.8939287066459656-----c: [ 0.12970233  0.11415545  0.08310816  0.07226861  0.09902503  0.0793537\n",
      "  0.10198122  0.09084553  0.08662264  0.14293724] --u_hat 8\n",
      "----- Epoch 0 Loss:0.8269405364990234-----c: [ 0.12684254  0.11163189  0.08179855  0.07277283  0.09962557  0.07825103\n",
      "  0.10244467  0.09376571  0.08980481  0.14306229] --u_hat 8\n",
      "----- Epoch 0 Loss:0.766424834728241-----c: [ 0.12349012  0.10865189  0.08071633  0.07425582  0.09887653  0.07692632\n",
      "  0.10117944  0.09692601  0.09199101  0.1469866 ] --u_hat 3\n",
      "----- Epoch 0 Loss:0.8606939315795898-----c: [ 0.12024786  0.10549723  0.08100474  0.07596955  0.0969853   0.07550392\n",
      "  0.09875327  0.10007615  0.0930215   0.15294054] --u_hat 3\n",
      "----- Epoch 0 Loss:0.6452098488807678-----c: [ 0.11778787  0.10250596  0.08276801  0.07799706  0.09514222  0.0741402\n",
      "  0.09634204  0.10215938  0.0924239   0.15873334] --u_hat 3\n",
      "----- Epoch 0 Loss:0.6765983700752258-----c: [ 0.11620823  0.09997576  0.08550189  0.0812681   0.09337708  0.07325559\n",
      "  0.09406027  0.10239809  0.09103633  0.16291848] --u_hat 3\n",
      "----- Epoch 0 Loss:0.7062346339225769-----c: [ 0.1151337   0.09821221  0.08917406  0.08450017  0.09245969  0.07339948\n",
      "  0.0920628   0.10219356  0.08923049  0.16363367] --u_hat 3\n",
      "----- Epoch 0 Loss:0.7238070368766785-----c: [ 0.11399511  0.09728528  0.09278602  0.08721597  0.09230725  0.07507458\n",
      "  0.09056179  0.1019139   0.08784885  0.16101116] --u_hat 2\n",
      "----- Epoch 0 Loss:0.5797461271286011-----c: [ 0.11242197  0.09749919  0.09515913  0.08823512  0.09305164  0.07771965\n",
      "  0.0899764   0.1011541   0.08673339  0.15804963] --u_hat 5\n",
      "----- Epoch 0 Loss:0.6377257704734802-----c: [ 0.1109788   0.09839232  0.09680265  0.08797389  0.09417406  0.08034962\n",
      "  0.0902279   0.10021269  0.08606339  0.15482493] --u_hat 5\n",
      "----- Epoch 0 Loss:0.7049635052680969-----c: [ 0.10954374  0.09943262  0.09710767  0.08668154  0.09689721  0.08211289\n",
      "  0.09169894  0.09900382  0.08577669  0.15174483] --u_hat 5\n",
      "----- Epoch 0 Loss:0.6533993482589722-----c: [ 0.10863185  0.09992065  0.09648259  0.08485902  0.10058972  0.08295247\n",
      "  0.09388537  0.09780767  0.08559047  0.14927992] --u_hat 4\n",
      "----- Epoch 0 Loss:0.6881668567657471-----c: [ 0.10784167  0.10058759  0.09535821  0.08283356  0.10430914  0.0830464\n",
      "  0.09601478  0.09645373  0.08517468  0.14838013] --u_hat 4\n",
      "----- Epoch 0 Loss:0.6829317212104797-----c: [ 0.10673882  0.10155153  0.09417126  0.08072284  0.10706764  0.08275323\n",
      "  0.09735913  0.09532151  0.08434401  0.14997   ] --u_hat 4\n",
      "----- Epoch 0 Loss:0.613045871257782-----c: [ 0.1051408   0.10221495  0.09432279  0.07863121  0.10813203  0.08250716\n",
      "  0.09800497  0.09435764  0.08355167  0.15313679] --u_hat 6\n",
      "----- Epoch 0 Loss:0.7268313765525818-----c: [ 0.10366682  0.10203291  0.09565681  0.07657954  0.10772875  0.08200856\n",
      "  0.09755963  0.09443861  0.08312863  0.15719965] --u_hat 2\n",
      "----- Epoch 0 Loss:0.7460533976554871-----c: [ 0.10220127  0.10204589  0.09838202  0.07466322  0.10608575  0.08115548\n",
      "  0.09616849  0.09534007  0.08346029  0.16049773] --u_hat 2\n",
      "----- Epoch 0 Loss:0.5825930833816528-----c: [ 0.1019286   0.10311013  0.10166364  0.07290912  0.10382012  0.07995616\n",
      "  0.09447688  0.09649337  0.08417482  0.16146716] --u_hat 2\n",
      "----- Epoch 0 Loss:0.7594006657600403-----c: [ 0.10334203  0.10528425  0.10502258  0.07131452  0.10136101  0.07846932\n",
      "  0.09254855  0.09853037  0.08468559  0.15944187] --u_hat 2\n",
      "----- Epoch 0 Loss:0.7490703463554382-----c: [ 0.10666905  0.10767131  0.10799596  0.06991635  0.09892771  0.07699791\n",
      "  0.09074727  0.10062233  0.08460471  0.15584743] --u_hat 0\n",
      "----- Epoch 0 Loss:0.4833613634109497-----c: [ 0.11064671  0.11011294  0.10960284  0.06882606  0.09665834  0.0756214\n",
      "  0.08916139  0.10269306  0.08436218  0.15231489] --u_hat 0\n",
      "----- Epoch 0 Loss:0.7685950398445129-----c: [ 0.11557633  0.11229163  0.10989804  0.0683386   0.09453926  0.07432166\n",
      "  0.08775942  0.10454834  0.08377211  0.14895456] --u_hat 0\n",
      "----- Epoch 0 Loss:0.6884273886680603-----c: [ 0.12073374  0.11343776  0.10897719  0.06908191  0.09276428  0.07360555\n",
      "  0.08689054  0.10554008  0.08294851  0.14602043] --u_hat 0\n",
      "----- Epoch 0 Loss:0.5301717519760132-----c: [ 0.12404307  0.11311589  0.10734911  0.07166484  0.09131385  0.07388807\n",
      "  0.08679162  0.10586125  0.08254668  0.14342564] --u_hat 3\n",
      "----- Epoch 0 Loss:0.6465715765953064-----c: [ 0.12582745  0.11234975  0.10530707  0.0763642   0.08996077  0.07494249\n",
      "  0.08715508  0.10503065  0.08221135  0.14085087] --u_hat 3\n",
      "----- Epoch 0 Loss:0.729218065738678-----c: [ 0.12503546  0.11093288  0.10305195  0.08416551  0.08887231  0.07642756\n",
      "  0.08817855  0.10325745  0.08175697  0.13832124] --u_hat 3\n",
      "----- Epoch 0 Loss:0.8042407631874084-----c: [ 0.12273982  0.10877657  0.10046967  0.09390542  0.08796801  0.07838514\n",
      "  0.08992271  0.1011578   0.08091453  0.13576032] --u_hat 3\n",
      "----- Epoch 0 Loss:0.6915833353996277-----c: [ 0.11946692  0.10628046  0.09759977  0.10390794  0.08720062  0.08076772\n",
      "  0.09180687  0.0987597   0.07997283  0.13423738] --u_hat 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 0 Loss:0.5985404849052429-----c: [ 0.11578533  0.10386945  0.0945731   0.1118882   0.08774342  0.08350245\n",
      "  0.09375134  0.09618071  0.07886557  0.13384052] --u_hat 3\n",
      "----- Epoch 0 Loss:0.6083140969276428-----c: [ 0.11213044  0.1021642   0.09161146  0.11667018  0.08959571  0.08646039\n",
      "  0.09512945  0.09362085  0.07760669  0.13501048] --u_hat 5\n",
      "----- Epoch 0 Loss:0.7118145823478699-----c: [ 0.10872924  0.10117492  0.08888269  0.11663088  0.09244476  0.09015572\n",
      "  0.0959783   0.09124099  0.07626155  0.13850079] --u_hat 5\n",
      "----- Epoch 0 Loss:0.7190137505531311-----c: [ 0.10565737  0.10122105  0.08639272  0.11387302  0.09669486  0.09320644\n",
      "  0.09578297  0.08924493  0.07481696  0.14310949] --u_hat 4\n",
      "----- Epoch 0 Loss:0.7633551955223083-----c: [ 0.10314551  0.10173413  0.08409546  0.11072061  0.10265194  0.09435727\n",
      "  0.09487439  0.0875688   0.07350021  0.14735179] --u_hat 4\n",
      "----- Epoch 0 Loss:0.7254934310913086-----c: [ 0.10195319  0.10206617  0.08202343  0.10774257  0.10947385  0.09327132\n",
      "  0.09375829  0.0868922   0.07260517  0.15021402] --u_hat 4\n",
      "----- Epoch 0 Loss:0.6385497450828552-----c: [ 0.10214671  0.10289106  0.08019684  0.10493476  0.11578545  0.09131908\n",
      "  0.09268108  0.0876068   0.07199917  0.15043898] --u_hat 4\n",
      "----- Epoch 0 Loss:0.7329275012016296-----c: [ 0.10321072  0.10423699  0.07878444  0.10240874  0.12064644  0.08921643\n",
      "  0.09149997  0.08948001  0.07192053  0.14859581] --u_hat 4\n",
      "----- Epoch 0 Loss:0.6228113174438477-----c: [ 0.10518822  0.10537867  0.07810607  0.10064018  0.12260593  0.08708346\n",
      "  0.09049703  0.09194598  0.07219635  0.14635806] --u_hat 7\n",
      "----- Epoch 0 Loss:0.7589938044548035-----c: [ 0.10784004  0.10562357  0.0785692   0.10073811  0.12181722  0.08499604\n",
      "  0.08947662  0.09479007  0.07253186  0.14361723] --u_hat 7\n",
      "----- Epoch 0 Loss:0.5813465714454651-----c: [ 0.10971488  0.10460391  0.08001856  0.10369635  0.11919068  0.08309175\n",
      "  0.08870118  0.09714989  0.07309511  0.14073776] --u_hat 2\n",
      "----- Epoch 0 Loss:0.7765905261039734-----c: [ 0.10965969  0.10246403  0.08254422  0.10956635  0.11613296  0.08137052\n",
      "  0.08788835  0.09929638  0.07349592  0.13758153] --u_hat 3\n",
      "----- Epoch 0 Loss:0.6785429120063782-----c: [ 0.10774978  0.09999632  0.08627226  0.11668338  0.11302298  0.07994351\n",
      "  0.08693951  0.10044596  0.07392946  0.13501695] --u_hat 2\n",
      "----- Epoch 0 Loss:0.8640872836112976-----c: [ 0.10491713  0.09747925  0.09045561  0.12275345  0.10989954  0.07881877\n",
      "  0.08573976  0.10126126  0.07507972  0.1335956 ] --u_hat 2\n",
      "----- Epoch 0 Loss:0.5309039950370789-----c: [ 0.10223334  0.09567676  0.09434789  0.12656133  0.10713696  0.07807835\n",
      "  0.08449465  0.10141475  0.07672878  0.13332742] --u_hat 2\n",
      "----- Epoch 0 Loss:0.7135933041572571-----c: [ 0.09985876  0.09474569  0.09817684  0.12739214  0.10479612  0.07759965\n",
      "  0.08323723  0.10068364  0.07832467  0.13518535] --u_hat 2\n",
      "----- Epoch 0 Loss:0.6159104704856873-----c: [ 0.09778138  0.09502708  0.1008414   0.1254629   0.10293449  0.0773249\n",
      "  0.08219856  0.10008342  0.0799578   0.13838798] --u_hat 8\n",
      "----- Epoch 0 Loss:0.5278835892677307-----c: [ 0.09585097  0.0961813   0.10198016  0.12292728  0.10195363  0.07718896\n",
      "  0.0812107   0.09978345  0.08091372  0.14200996] --u_hat 8\n",
      "----- Epoch 0 Loss:0.5983766913414001-----c: [ 0.09388933  0.09811054  0.10236175  0.12040901  0.10282572  0.07697485\n",
      "  0.08006437  0.09926929  0.08078507  0.14531043] --u_hat 1\n",
      "----- Epoch 0 Loss:0.6544021964073181-----c: [ 0.09179834  0.10012209  0.10207428  0.11871251  0.10556198  0.07635993\n",
      "  0.07882419  0.09922889  0.07987774  0.14743984] --u_hat 4\n",
      "----- Epoch 0 Loss:0.6643862128257751-----c: [ 0.08956738  0.10178071  0.10191771  0.11973597  0.10932844  0.07523851\n",
      "  0.07767531  0.09915879  0.07854993  0.14704712] --u_hat 4\n",
      "----- Epoch 0 Loss:0.5958686470985413-----c: [ 0.08765693  0.10345802  0.1021411   0.12285697  0.11304651  0.07414835\n",
      "  0.07700761  0.09866616  0.07694665  0.14407167] --u_hat 4\n",
      "----- Epoch 0 Loss:0.595520555973053-----c: [ 0.08619262  0.10517699  0.10212153  0.12725799  0.11648858  0.07356925\n",
      "  0.0767739   0.09752068  0.07532211  0.13957642] --u_hat 4\n",
      "----- Epoch 0 Loss:0.7131921648979187-----c: [ 0.08516429  0.10676513  0.10210817  0.13067526  0.11968581  0.07377859\n",
      "  0.07684788  0.09615317  0.07382989  0.1349919 ] --u_hat 5\n",
      "----- Epoch 0 Loss:0.703198254108429-----c: [ 0.08458695  0.10738499  0.10159751  0.13204624  0.12130853  0.07649978\n",
      "  0.07717484  0.09468559  0.07369962  0.13101599] --u_hat 5\n",
      "----- Epoch 0 Loss:0.7045848965644836-----c: [ 0.08414758  0.10663124  0.10045478  0.13157743  0.1206719   0.08243632\n",
      "  0.077841    0.0935961   0.07500337  0.12764026] --u_hat 5\n",
      "----- Epoch 0 Loss:0.6602783799171448-----c: [ 0.08473244  0.10524129  0.09890731  0.12913765  0.11895694  0.08896355\n",
      "  0.07951146  0.09313764  0.07661171  0.12480005] --u_hat 5\n",
      "----- Epoch 0 Loss:0.7233986854553223-----c: [ 0.0865632   0.10371116  0.09736952  0.12662475  0.11693582  0.09365163\n",
      "  0.08164635  0.09353092  0.07762822  0.12233831] --u_hat 5\n",
      "----- Epoch 0 Loss:0.6431834697723389-----c: [ 0.089636    0.10244075  0.09598073  0.12454773  0.11530524  0.09534791\n",
      "  0.08379563  0.09425231  0.07834046  0.12035321] --u_hat 0\n",
      "----- Epoch 0 Loss:0.5631282925605774-----c: [ 0.09295979  0.10168155  0.09497847  0.12320486  0.11383515  0.09457888\n",
      "  0.08569595  0.09545985  0.07896224  0.1186434 ] --u_hat 0\n",
      "----- Epoch 0 Loss:0.6118187308311462-----c: [ 0.09651409  0.10201628  0.09406498  0.12371007  0.11187536  0.09289864\n",
      "  0.08656383  0.0962017   0.07906308  0.11709229] --u_hat 0\n",
      "----- Epoch 0 Loss:0.564326286315918-----c: [ 0.09977359  0.10357744  0.09346464  0.1265365   0.10959617  0.09061755\n",
      "  0.08599684  0.09604425  0.07875012  0.1156427 ] --u_hat 0\n",
      "----- Epoch 0 Loss:0.5819892883300781-----c: [ 0.10259292  0.1068719   0.09314171  0.1306041   0.10691121  0.08793297\n",
      "  0.08441816  0.09513273  0.07786936  0.11452511] --u_hat 0\n",
      "----- Epoch 0 Loss:0.6507560014724731-----c: [ 0.10436624  0.11116944  0.09348971  0.13446482  0.10381513  0.08500355\n",
      "  0.08215688  0.0937864   0.07680991  0.11493749] --u_hat 1\n",
      "----- Epoch 0 Loss:0.727332353591919-----c: [ 0.10472288  0.11630641  0.09553207  0.13603547  0.10098194  0.08235931\n",
      "  0.07978453  0.0927507   0.07583394  0.11569292] --u_hat 1\n",
      "----- Epoch 0 Loss:0.6125747561454773-----c: [ 0.10455317  0.12105615  0.09816178  0.13394421  0.0994461   0.08075354\n",
      "  0.07795921  0.0927498   0.07551884  0.11585731] --u_hat 2\n",
      "----- Epoch 0 Loss:0.6389299631118774-----c: [ 0.10339201  0.1246056   0.10075949  0.13183039  0.09824756  0.07944788\n",
      "  0.07628238  0.09314261  0.07538081  0.11691133] --u_hat 2\n",
      "----- Epoch 0 Loss:0.6925433278083801-----c: [ 0.10148884  0.12653968  0.1033102   0.13036521  0.09732631  0.0784829\n",
      "  0.07482446  0.09404732  0.07533323  0.11828221] --u_hat 2\n",
      "----- Epoch 0 Loss:0.5784382820129395-----c: [ 0.09980224  0.12577781  0.10432135  0.12982002  0.0968361   0.07907729\n",
      "  0.07376257  0.09535372  0.07602508  0.11922395] --u_hat 8\n",
      "----- Epoch 0 Loss:0.5189961194992065-----c: [ 0.09804489  0.1242171   0.10372299  0.13058874  0.09607477  0.08108161\n",
      "  0.07292689  0.09673984  0.07717185  0.11943095] --u_hat 5\n",
      "----- Epoch 0 Loss:0.5649353265762329-----c: [ 0.09600109  0.1218918   0.10202075  0.13097674  0.09487092  0.08783825\n",
      "  0.07224983  0.09806722  0.07806086  0.11802263] --u_hat 5\n",
      "----- Epoch 0 Loss:0.7765464186668396-----c: [ 0.09339411  0.11884967  0.09938611  0.12946494  0.09302036  0.10030258\n",
      "  0.07143734  0.10008206  0.07883061  0.11523215] --u_hat 5\n",
      "----- Epoch 0 Loss:0.8859707713127136-----c: [ 0.09044954  0.11547635  0.09631121  0.12676401  0.09080619  0.11731759\n",
      "  0.07061253  0.10159313  0.0786173   0.11205208] --u_hat 5\n",
      "----- Epoch 0 Loss:0.6450382471084595-----c: [ 0.08733144  0.11278287  0.09301056  0.12295924  0.08866034  0.13711372\n",
      "  0.06976513  0.10105315  0.07849628  0.10882705] --u_hat 5\n",
      "----- Epoch 0 Loss:0.6594046354293823-----c: [ 0.08389703  0.11066005  0.08934791  0.11835297  0.08604324  0.15955061\n",
      "  0.06912386  0.09896796  0.07852112  0.10553556] --u_hat 5\n",
      "----- Epoch 0 Loss:1.0269755125045776-----c: [ 0.08026425  0.10913157  0.08541965  0.11344768  0.08336218  0.18355672\n",
      "  0.06855134  0.09568758  0.07850435  0.10207459] --u_hat 5\n",
      "----- Epoch 0 Loss:0.5952714085578918-----c: [ 0.07690004  0.11167833  0.08148304  0.10873774  0.08094724  0.20295912\n",
      "  0.06823044  0.09156556  0.07831769  0.09918075] --u_hat 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 0 Loss:0.9995203018188477-----c: [ 0.07381076  0.11327007  0.07792756  0.10536363  0.07868953  0.22039197\n",
      "  0.06818417  0.08798849  0.07760511  0.09676885] --u_hat 5\n",
      "----- Epoch 0 Loss:0.8406946063041687-----c: [ 0.07226957  0.11764942  0.07525238  0.10653722  0.07733247  0.22264436\n",
      "  0.06856988  0.08510659  0.07799364  0.0966445 ] --u_hat 6\n",
      "----- Epoch 0 Loss:0.5204634070396423-----c: [ 0.07160705  0.12826578  0.07278224  0.10773551  0.07667819  0.21612141\n",
      "  0.06889611  0.08266047  0.07820179  0.09705175] --u_hat 1\n",
      "----- Epoch 0 Loss:0.6908552050590515-----c: [ 0.07054631  0.13188653  0.07056231  0.11134589  0.07641289  0.21392663\n",
      "  0.06926415  0.08046913  0.07833728  0.09724881] --u_hat 6\n",
      "----- Epoch 0 Loss:0.8246449828147888-----c: [ 0.06871471  0.13153186  0.06784756  0.11362222  0.07599458  0.22234538\n",
      "  0.06894398  0.07774167  0.07720425  0.09605377] --u_hat 6\n",
      "----- Epoch 0 Loss:0.8205428123474121-----c: [ 0.0668152   0.12840919  0.06515156  0.11446318  0.07582212  0.23585619\n",
      "  0.06803083  0.07537791  0.07545307  0.09462109] --u_hat 4\n",
      "----- Epoch 0 Loss:0.7698172926902771-----c: [ 0.06528112  0.12492479  0.06325337  0.11452536  0.07662764  0.245901\n",
      "  0.06779175  0.07402162  0.07380956  0.09386356] --u_hat 4\n",
      "----- Epoch 0 Loss:0.7144207954406738-----c: [ 0.06414156  0.12271783  0.0619712   0.11344481  0.07834974  0.25122169\n",
      "  0.06837235  0.07344893  0.07262172  0.09371051] --u_hat 4\n",
      "----- Epoch 0 Loss:0.6203301548957825-----c: [ 0.06188554  0.13003831  0.06083764  0.11219711  0.07803105  0.24946587\n",
      "  0.06755184  0.07564078  0.07073907  0.09361288] --u_hat 7\n",
      "----- Epoch 0 Loss:0.7498089671134949-----c: [ 0.05905293  0.13804692  0.05912758  0.10926542  0.07710337  0.25027305\n",
      "  0.06639794  0.08036616  0.06819717  0.09216942] --u_hat 7\n",
      "----- Epoch 0 Loss:0.7078527808189392-----c: [ 0.05625938  0.13977283  0.05706014  0.10728186  0.07692083  0.25910649\n",
      "  0.06539903  0.08212097  0.06580148  0.09027699] --u_hat 4\n",
      "----- Epoch 0 Loss:0.6455976963043213-----c: [ 0.05437215  0.13834505  0.05558899  0.10449398  0.07788022  0.26412714\n",
      "  0.0654585   0.08644386  0.06434909  0.08894136] --u_hat 7\n",
      "----- Epoch 0 Loss:0.4853934943675995-----c: [ 0.05318614  0.13602954  0.05454822  0.10333847  0.07986842  0.26266086\n",
      "  0.06639906  0.09153607  0.0640486   0.08838447] --u_hat 4\n",
      "----- Epoch 0 Loss:0.41754722595214844-----c: [ 0.05207883  0.13500668  0.0535001   0.10306352  0.08117896  0.25913066\n",
      "  0.06746055  0.09633172  0.06430379  0.08794533] --u_hat 6\n",
      "----- Epoch 0 Loss:0.7333757877349854-----c: [ 0.05148974  0.1374504   0.05255996  0.10329816  0.08221737  0.25413036\n",
      "  0.06903291  0.09613489  0.06523885  0.0884475 ] --u_hat 6\n",
      "----- Epoch 0 Loss:0.6614577770233154-----c: [ 0.05079884  0.14221205  0.05142625  0.10480184  0.081644    0.24701691\n",
      "  0.06968185  0.09771945  0.06629171  0.0884073 ] --u_hat 8\n",
      "----- Epoch 0 Loss:0.609780490398407-----c: [ 0.05030747  0.14769951  0.05037264  0.10627849  0.08049572  0.24025768\n",
      "  0.07000293  0.09928033  0.06704027  0.08826494] --u_hat 8\n",
      "----- Epoch 0 Loss:0.7310740351676941-----c: [ 0.05026537  0.15454458  0.04966913  0.10791406  0.07938465  0.23430435\n",
      "  0.07080188  0.0975832   0.06770036  0.0878325 ] --u_hat 0\n",
      "----- Epoch 0 Loss:0.5746318697929382-----c: [ 0.0506219   0.15924975  0.04940567  0.10912295  0.07831832  0.22985297\n",
      "  0.07176502  0.09685104  0.06769569  0.08711664] --u_hat 0\n",
      "----- Epoch 0 Loss:0.5150022506713867-----c: [ 0.05278731  0.1609267   0.04970156  0.11032593  0.07770611  0.22637275\n",
      "  0.07300124  0.09503914  0.06746919  0.08666988] --u_hat 0\n",
      "----- Epoch 0 Loss:0.5583249926567078-----c: [ 0.05693099  0.1592496   0.05089012  0.11127934  0.07758471  0.22279726\n",
      "  0.07450522  0.0937508   0.06689671  0.0861151 ] --u_hat 0\n",
      "----- Epoch 0 Loss:0.5200063586235046-----c: [ 0.06230088  0.15567441  0.05271886  0.11032968  0.07720882  0.22049887\n",
      "  0.07507554  0.09613223  0.06549494  0.08456597] --u_hat 0\n",
      "----- Epoch 0 Loss:0.6916133761405945-----c: [ 0.07019351  0.15247539  0.05587931  0.10882279  0.07796482  0.21666564\n",
      "  0.07597379  0.09453427  0.06421698  0.0832731 ] --u_hat 0\n",
      "----- Epoch 0 Loss:0.5379311442375183-----c: [ 0.07910824  0.14937139  0.06050012  0.10640644  0.07896211  0.21302904\n",
      "  0.07591271  0.09275633  0.06265041  0.08130308] --u_hat 0\n",
      "----- Epoch 0 Loss:0.7560081481933594-----c: [ 0.0887557   0.14474748  0.06685415  0.10307481  0.08111335  0.21016201\n",
      "  0.07506064  0.0906411   0.06070205  0.0788887 ] --u_hat 2\n",
      "----- Epoch 0 Loss:0.5102797746658325-----c: [ 0.09707785  0.14197984  0.07523349  0.09963708  0.0829024   0.20605481\n",
      "  0.07351721  0.08837673  0.05888452  0.07633622] --u_hat 2\n",
      "----- Epoch 0 Loss:0.5728968381881714-----c: [ 0.10491368  0.13746268  0.0858179   0.09610903  0.08541796  0.20072713\n",
      "  0.07112822  0.08732591  0.05698125  0.07411619] --u_hat 2\n",
      "----- Epoch 0 Loss:0.773958683013916-----c: [ 0.10743234  0.13474821  0.0987631   0.0922032   0.08643977  0.19396624\n",
      "  0.06818081  0.0918036   0.05508422  0.07137856] --u_hat 2\n",
      "----- Epoch 0 Loss:0.6359738707542419-----c: [ 0.10517893  0.1307707   0.11437556  0.08928853  0.08761719  0.18853277\n",
      "  0.06551135  0.09525457  0.05402417  0.06944629] --u_hat 2\n",
      "----- Epoch 0 Loss:0.783790647983551-----c: [ 0.10254028  0.12796     0.12961501  0.08688161  0.08952469  0.18331255\n",
      "  0.06405781  0.09376983  0.05438225  0.06795586] --u_hat 2\n",
      "----- Epoch 0 Loss:0.6702151894569397-----c: [ 0.0995606   0.12662651  0.14140588  0.0846041   0.09224827  0.1782501\n",
      "  0.06291442  0.09134126  0.05650685  0.06654203] --u_hat 8\n",
      "----- Epoch 0 Loss:0.7269863486289978-----c: [ 0.09735553  0.1336779   0.14548703  0.08196656  0.09218322  0.17335218\n",
      "  0.0613419   0.08889911  0.060399    0.06533731] --u_hat 8\n",
      "----- Epoch 0 Loss:0.6716369986534119-----c: [ 0.09696452  0.13502897  0.14388753  0.08124218  0.09206033  0.16897671\n",
      "  0.06014752  0.08810346  0.06697317  0.06661595] --u_hat 8\n",
      "----- Epoch 0 Loss:0.49834537506103516-----c: [ 0.09709028  0.13507861  0.14060061  0.08021051  0.09241883  0.16565719\n",
      "  0.05995231  0.08650333  0.07495344  0.06753503] --u_hat 8\n",
      "----- Epoch 0 Loss:0.5539938807487488-----c: [ 0.09831291  0.13852626  0.13961579  0.0799746   0.08911987  0.16072279\n",
      "  0.05791967  0.0829685   0.0839294   0.06891041] --u_hat 8\n",
      "----- Epoch 0 Loss:0.6227548718452454-----c: [ 0.09827273  0.13403764  0.14088327  0.07910014  0.08416311  0.15755646\n",
      "  0.05565194  0.08477016  0.09495223  0.07061261] --u_hat 8\n",
      "----- Epoch 0 Loss:0.5980224609375-----c: [ 0.0962274   0.13475293  0.14539471  0.07669477  0.08003458  0.15325467\n",
      "  0.05466416  0.08219083  0.10478666  0.07199924] --u_hat 8\n",
      "----- Epoch 0 Loss:0.5363048315048218-----c: [ 0.09396309  0.13975811  0.1488418   0.07316015  0.07644531  0.14745231\n",
      "  0.05566491  0.07907811  0.11362922  0.07200702] --u_hat 8\n",
      "----- Epoch 0 Loss:0.5712434649467468-----c: [ 0.09182658  0.13676819  0.15040183  0.07128433  0.07329535  0.14660032\n",
      "  0.05739999  0.07963129  0.11883725  0.07395489] --u_hat 6\n",
      "----- Epoch 0 Loss:0.41792193055152893-----c: [ 0.09118953  0.13551232  0.14862952  0.06937991  0.07146112  0.14522642\n",
      "  0.06036713  0.08119459  0.12022041  0.0768191 ] --u_hat 6\n",
      "----- Epoch 0 Loss:0.31635385751724243-----c: [ 0.09078345  0.13260686  0.1441492   0.06753737  0.06936658  0.14463648\n",
      "  0.06309614  0.09041629  0.11753922  0.07986836] --u_hat 6\n",
      "----- Epoch 0 Loss:0.47733330726623535-----c: [ 0.09074675  0.13521397  0.14101882  0.06619111  0.06809929  0.14411077\n",
      "  0.06774794  0.08909806  0.11515658  0.08261681] --u_hat 6\n",
      "----- Epoch 0 Loss:0.3719303607940674-----c: [ 0.09039825  0.13293973  0.14116926  0.0651896   0.06673963  0.14464469\n",
      "  0.07176342  0.08990229  0.11255603  0.08469698] --u_hat 6\n",
      "----- Epoch 0 Loss:0.41759204864501953-----c: [ 0.08921041  0.13160047  0.14569822  0.06353201  0.06502392  0.1409834\n",
      "  0.07579926  0.09418762  0.10968016  0.08428458] --u_hat 6\n",
      "----- Epoch 0 Loss:0.6763534545898438-----c: [ 0.08718868  0.12960911  0.15404841  0.06184248  0.06327546  0.13723022\n",
      "  0.07793333  0.09760472  0.10736213  0.08390534] --u_hat 6\n",
      "----- Epoch 0 Loss:0.5768861174583435-----c: [ 0.08551048  0.12910844  0.16455141  0.06014113  0.06172838  0.13472591\n",
      "  0.07930881  0.09638527  0.10611802  0.08242217] --u_hat 6\n",
      "----- Epoch 0 Loss:0.570429265499115-----c: [ 0.08271828  0.12808031  0.17167054  0.0581084   0.05975264  0.13256189\n",
      "  0.07757466  0.09806339  0.10958141  0.0818885 ] --u_hat 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 0 Loss:0.6891588568687439-----c: [ 0.08113972  0.12558635  0.17595537  0.05695274  0.05889788  0.13007955\n",
      "  0.07644743  0.09686743  0.11644112  0.08163252] --u_hat 8\n",
      "----- Epoch 0 Loss:0.6098141670227051-----c: [ 0.08031894  0.12332499  0.17478885  0.05588611  0.06077207  0.12759903\n",
      "  0.07592679  0.09651358  0.12352439  0.08134513] --u_hat 4\n",
      "----- Epoch 0 Loss:0.5793821215629578-----c: [ 0.07932644  0.12094731  0.17114632  0.0558446   0.0619587   0.12584268\n",
      "  0.07460514  0.09775923  0.13167211  0.08089771] --u_hat 4\n",
      "----- Epoch 0 Loss:0.5757226347923279-----c: [ 0.07912029  0.11906968  0.16887818  0.05622406  0.06537653  0.1251469\n",
      "  0.07365818  0.09761299  0.13359307  0.08132008] --u_hat 4\n",
      "----- Epoch 0 Loss:0.4751546382904053-----c: [ 0.07972871  0.11718837  0.1721466   0.05719011  0.06692722  0.12429444\n",
      "  0.07191346  0.09957443  0.13120395  0.0798327 ] --u_hat 4\n",
      "----- Epoch 0 Loss:0.4852518141269684-----c: [ 0.0805515   0.11590365  0.1776454   0.05992308  0.06899688  0.1234234\n",
      "  0.06958694  0.09848729  0.12731552  0.07816644] --u_hat 3\n",
      "----- Epoch 0 Loss:0.659416675567627-----c: [ 0.08167544  0.1127741   0.18077295  0.06254067  0.07514587  0.11946085\n",
      "  0.06688473  0.10198457  0.1228463   0.07591437] --u_hat 4\n",
      "----- Epoch 0 Loss:0.6543384194374084-----c: [ 0.0841049   0.10940103  0.18107969  0.06612156  0.08288918  0.11540345\n",
      "  0.06416135  0.10439119  0.11905844  0.07338914] --u_hat 4\n",
      "----- Epoch 0 Loss:0.5009730458259583-----c: [ 0.08570927  0.10796066  0.17740597  0.07079946  0.08616959  0.11192337\n",
      "  0.06228283  0.10529366  0.12054791  0.0719073 ] --u_hat 3\n",
      "----- Epoch 0 Loss:0.4574768841266632-----c: [ 0.08594945  0.10437783  0.17118412  0.07689647  0.09627536  0.10809818\n",
      "  0.06002565  0.10615402  0.12234914  0.06868959] --u_hat 3\n",
      "----- Epoch 0 Loss:0.6240447163581848-----c: [ 0.08651313  0.10335884  0.16812542  0.08209395  0.09663573  0.10421214\n",
      "  0.05866943  0.10824009  0.12502064  0.06713049] --u_hat 3\n",
      "----- Epoch 0 Loss:0.45789989829063416-----c: [ 0.08481374  0.09922051  0.17220969  0.09023692  0.09667158  0.10390415\n",
      "  0.0556479   0.11253825  0.12105522  0.06370234] --u_hat 3\n",
      "----- Epoch 0 Loss:0.43827390670776367-----c: [ 0.08378059  0.09859108  0.17439593  0.09398394  0.09888829  0.10440927\n",
      "  0.05507357  0.11060402  0.11857887  0.06169437] --u_hat 3\n",
      "----- Epoch 0 Loss:0.4390724003314972-----c: [ 0.08212747  0.10471288  0.17626302  0.09494221  0.09698942  0.10301048\n",
      "  0.05496509  0.10923766  0.1173325   0.06041919] --u_hat 6\n",
      "----- Epoch 0 Loss:0.49803057312965393-----c: [ 0.08023671  0.10244831  0.17707308  0.09879138  0.09939464  0.10451631\n",
      "  0.05511819  0.10930768  0.11458385  0.05852979] --u_hat 6\n",
      "----- Epoch 0 Loss:0.6961517333984375-----c: [ 0.07863957  0.10127015  0.17789923  0.09879886  0.09971577  0.10670298\n",
      "  0.0566551   0.10869975  0.11420254  0.05741585] --u_hat 6\n",
      "----- Epoch 0 Loss:0.33234405517578125-----c: [ 0.07713048  0.10342346  0.17731467  0.09734626  0.09998772  0.10550345\n",
      "  0.05889584  0.10855344  0.11526521  0.05657935] --u_hat 6\n",
      "----- Epoch 0 Loss:0.5293550491333008-----c: [ 0.07576796  0.10207444  0.17461193  0.09773245  0.09955989  0.10659264\n",
      "  0.06263211  0.10895227  0.11611146  0.05596479] --u_hat 6\n",
      "----- Epoch 0 Loss:0.3906216323375702-----c: [ 0.07525533  0.10216495  0.17080349  0.10093652  0.0971657   0.10368805\n",
      "  0.06710242  0.11188959  0.11541685  0.05557726] --u_hat 6\n",
      "----- Epoch 0 Loss:0.2863992154598236-----c: [ 0.07394144  0.10123066  0.16686466  0.10365614  0.09468073  0.10570661\n",
      "  0.07138771  0.11361472  0.11381742  0.05509983] --u_hat 6\n",
      "----- Epoch 0 Loss:0.3126799166202545-----c: [ 0.07262735  0.10103096  0.16284639  0.10594536  0.0921266   0.10863577\n",
      "  0.07457713  0.11608572  0.11157947  0.05454521] --u_hat 6\n",
      "----- Epoch 0 Loss:0.31854745745658875-----c: [ 0.07122334  0.09897421  0.15911718  0.10832831  0.09072209  0.10927001\n",
      "  0.07945674  0.12043366  0.10880689  0.05366747] --u_hat 6\n",
      "----- Epoch 0 Loss:0.45053672790527344-----c: [ 0.07057036  0.09768774  0.15735011  0.11092552  0.08913152  0.10928959\n",
      "  0.0830633   0.11992797  0.10741455  0.05463968] --u_hat 6\n",
      "----- Epoch 0 Loss:0.2561444938182831-----c: [ 0.07156309  0.0988529   0.15490679  0.11054176  0.08768237  0.10711761\n",
      "  0.08613903  0.12065683  0.1058758   0.05666367] --u_hat 6\n",
      "----- Epoch 0 Loss:0.49957212805747986-----c: [ 0.07822592  0.09692892  0.14924106  0.11053172  0.09524164  0.10333821\n",
      "  0.09143401  0.11871632  0.10152896  0.05481301] --u_hat 6\n",
      "----- Epoch 0 Loss:0.2663549482822418-----c: [ 0.07752021  0.09735767  0.14629656  0.11191694  0.09895832  0.10168383\n",
      "  0.09204816  0.12064612  0.09949706  0.05407515] --u_hat 9\n",
      "----- Epoch 0 Loss:0.4288102686405182-----c: [ 0.08550307  0.09677395  0.14330846  0.11126958  0.09795327  0.0995316\n",
      "  0.09360466  0.12050429  0.09712052  0.05443071] --u_hat 0\n",
      "----- Epoch 0 Loss:0.3420366048812866-----c: [ 0.08417806  0.09824531  0.14127222  0.11383796  0.09846675  0.09699944\n",
      "  0.09209491  0.11976559  0.09567307  0.05946656] --u_hat 9\n",
      "----- Epoch 0 Loss:0.4115349054336548-----c: [ 0.08292586  0.10141217  0.14008258  0.11321974  0.10160664  0.0949631\n",
      "  0.09052179  0.12042841  0.09406634  0.06077327] --u_hat 9\n",
      "----- Epoch 0 Loss:0.5539883375167847-----c: [ 0.08452213  0.10278596  0.1390975   0.1155038   0.10239642  0.09283387\n",
      "  0.08910977  0.11943482  0.09227161  0.06204418] --u_hat 9\n",
      "----- Epoch 0 Loss:0.5766482949256897-----c: [ 0.08327249  0.1030746   0.13942264  0.11751497  0.10113908  0.09064185\n",
      "  0.08960751  0.11979695  0.0913194   0.06421033] --u_hat 9\n",
      "----- Epoch 0 Loss:0.3533344566822052-----c: [ 0.08521053  0.10421894  0.13934644  0.11635602  0.10049943  0.08874211\n",
      "  0.08900958  0.11887126  0.09178921  0.06595661] --u_hat 9\n",
      "----- Epoch 0 Loss:0.5447049736976624-----c: [ 0.08488926  0.10290417  0.1382833   0.11543439  0.10186035  0.08711671\n",
      "  0.08758434  0.12019898  0.09437834  0.06735004] --u_hat 9\n",
      "----- Epoch 0 Loss:0.4339239299297333-----c: [ 0.08881047  0.10088651  0.13707042  0.11361216  0.10210862  0.08579639\n",
      "  0.08590128  0.12380888  0.09499411  0.0670112 ] --u_hat 7\n",
      "----- Epoch 0 Loss:0.2179075926542282-----c: [ 0.0959884   0.10048865  0.13772279  0.11164369  0.1004011   0.08439671\n",
      "  0.08358569  0.12322536  0.09680641  0.06574129] --u_hat 0\n",
      "----- Epoch 0 Loss:0.2871449291706085-----c: [ 0.09858873  0.10035001  0.13682577  0.10992739  0.10313994  0.0828706\n",
      "  0.0811837   0.12508799  0.09848548  0.06354032] --u_hat 8\n",
      "----- Epoch 0 Loss:0.3740585148334503-----c: [ 0.09747434  0.09993771  0.13646577  0.10899471  0.10200212  0.08227028\n",
      "  0.07958322  0.12567568  0.10434275  0.06325327] --u_hat 8\n",
      "----- Epoch 0 Loss:0.3342565596103668-----c: [ 0.09665386  0.10016356  0.1354568   0.10827821  0.10055706  0.08417097\n",
      "  0.0783055   0.12518317  0.10872013  0.06251079] --u_hat 8\n",
      "----- Epoch 0 Loss:0.28234243392944336-----c: [ 0.09550474  0.0993653   0.13343774  0.10800147  0.10031912  0.08455165\n",
      "  0.07761021  0.12707879  0.11227679  0.06185422] --u_hat 8\n",
      "----- Epoch 0 Loss:0.4203360974788666-----c: [ 0.09912145  0.09879652  0.13421337  0.10788408  0.10022999  0.08382354\n",
      "  0.07716303  0.12639099  0.11154183  0.06083518] --u_hat 6\n",
      "----- Epoch 0 Loss:0.16039103269577026-----c: [ 0.09792417  0.10428104  0.13303532  0.10652147  0.09905243  0.08284561\n",
      "  0.07727242  0.12687021  0.11210259  0.06009467] --u_hat 1\n",
      "----- Epoch 0 Loss:0.4047635495662689-----c: [ 0.10266227  0.10369527  0.13106395  0.10522096  0.09801119  0.08443353\n",
      "  0.07941742  0.12613265  0.11006865  0.05929415] --u_hat 6\n",
      "----- Epoch 0 Loss:0.3295435905456543-----c: [ 0.10460374  0.10426401  0.12907946  0.10431573  0.09897956  0.08299007\n",
      "  0.0815571   0.12613752  0.10824225  0.05983044] --u_hat 6\n",
      "----- Epoch 0 Loss:0.2963031232357025-----c: [ 0.10649025  0.10295478  0.12694341  0.10302439  0.09980462  0.08179352\n",
      "  0.0851579   0.12584193  0.10651508  0.06147436] --u_hat 6\n",
      "----- Epoch 0 Loss:0.4168453514575958-----c: [ 0.11207383  0.10106535  0.12564708  0.10318184  0.09806315  0.0841758\n",
      "  0.08503637  0.12604517  0.10394403  0.06076715] --u_hat 6\n",
      "----- Epoch 0 Loss:0.22019845247268677-----c: [ 0.11119609  0.10162672  0.1241129   0.10276581  0.09765866  0.08360451\n",
      "  0.08515701  0.12806812  0.10257718  0.06323284] --u_hat 9\n",
      "----- Epoch 0 Loss:0.46905526518821716-----c: [ 0.1102291   0.10323484  0.12279633  0.10159197  0.09875642  0.08272314\n",
      "  0.08630079  0.12877777  0.10129953  0.06429029] --u_hat 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 0 Loss:0.3667741119861603-----c: [ 0.11006358  0.10309723  0.12247136  0.10048988  0.10057548  0.08097097\n",
      "  0.08530938  0.13086767  0.09950289  0.06665169] --u_hat 9\n",
      "----- Epoch 0 Loss:0.20984865725040436-----c: [ 0.11251362  0.10580336  0.1216228   0.0995044   0.10059282  0.07979515\n",
      "  0.08442421  0.13103373  0.0976155   0.06709459] --u_hat 9\n",
      "----- Epoch 0 Loss:0.3006325960159302-----c: [ 0.11463448  0.10534853  0.12185504  0.09916768  0.09979878  0.07956205\n",
      "  0.08452848  0.13140576  0.09611221  0.06758735] --u_hat 9\n",
      "----- Epoch 0 Loss:0.45734819769859314-----c: [ 0.11339863  0.10420983  0.1236051   0.10072878  0.09939256  0.08118713\n",
      "  0.08304304  0.13103682  0.09475615  0.06864211] --u_hat 9\n",
      "----- Epoch 0 Loss:0.33005672693252563-----c: [ 0.11335569  0.10350165  0.12359076  0.10339013  0.09904667  0.08099099\n",
      "  0.08200774  0.13077153  0.09302755  0.07031732] --u_hat 9\n",
      "----- Epoch 0 Loss:0.20812834799289703-----c: [ 0.11851401  0.10282325  0.12396879  0.10397483  0.09680822  0.0788397\n",
      "  0.07980649  0.13188607  0.09210181  0.07127684] --u_hat 8\n",
      "----- Epoch 0 Loss:0.32475632429122925-----c: [ 0.11754142  0.10143334  0.12264629  0.10667679  0.09817813  0.07739954\n",
      "  0.08065321  0.13237695  0.09104207  0.07205208] --u_hat 9\n",
      "----- Epoch 0 Loss:0.2857464551925659-----c: [ 0.1207163   0.10053144  0.1211798   0.10582838  0.09926429  0.07613039\n",
      "  0.081331    0.13233812  0.08943679  0.07324317] --u_hat 6\n",
      "----- Epoch 0 Loss:0.27529287338256836-----c: [ 0.11878371  0.1015716   0.12125426  0.10457609  0.10248984  0.07491287\n",
      "  0.08211334  0.13191119  0.08774593  0.07464099] --u_hat 9\n",
      "----- Epoch 0 Loss:0.25695064663887024-----c: [ 0.11920995  0.10271049  0.11988836  0.1036306   0.10795765  0.0733743\n",
      "  0.08286762  0.13086094  0.08549545  0.07400449] --u_hat 4\n",
      "----- Epoch 0 Loss:0.34151819348335266-----c: [ 0.11900028  0.10133042  0.11926586  0.10338157  0.11094043  0.07216337\n",
      "  0.08421507  0.13085537  0.08396865  0.07487883] --u_hat 6\n",
      "----- Epoch 0 Loss:0.4247314929962158-----c: [ 0.11914834  0.10020662  0.11824135  0.10189617  0.11503427  0.07060025\n",
      "  0.08663341  0.13113926  0.08226535  0.07483493] --u_hat 6\n",
      "----- Epoch 0 Loss:0.388966828584671-----c: [ 0.1182574   0.10100064  0.11784095  0.10046696  0.11825299  0.06944987\n",
      "  0.08679775  0.13088289  0.0811822   0.07586823] --u_hat 9\n",
      "----- Epoch 0 Loss:0.24212197959423065-----c: [ 0.11805713  0.10098734  0.11670317  0.09927683  0.11991327  0.06826773\n",
      "  0.08851316  0.13186976  0.08020128  0.07621041] --u_hat 6\n",
      "----- Epoch 0 Loss:0.4017350971698761-----c: [ 0.11741666  0.10152885  0.1160161   0.10117021  0.11885721  0.06811867\n",
      "  0.08831659  0.13357411  0.07965086  0.07535078] --u_hat 6\n",
      "----- Epoch 0 Loss:0.28917214274406433-----c: [ 0.11640842  0.1007129   0.11520296  0.10078169  0.12145225  0.0669785\n",
      "  0.08923601  0.13534895  0.07940742  0.0744706 ] --u_hat 6\n",
      "----- Epoch 0 Loss:0.3484745919704437-----c: [ 0.11533438  0.09995957  0.11584444  0.10314419  0.12112847  0.06616196\n",
      "  0.08860049  0.13499743  0.08065623  0.07417282] --u_hat 8\n",
      "----- Epoch 0 Loss:0.40226268768310547-----c: [ 0.11331823  0.09851258  0.11698657  0.10362428  0.12048813  0.06598312\n",
      "  0.0887246   0.13634223  0.08327878  0.07274155] --u_hat 8\n",
      "----- Epoch 0 Loss:0.4000561237335205-----c: [ 0.11193421  0.09828199  0.11618894  0.10445105  0.11955437  0.06545618\n",
      "  0.08734909  0.13668726  0.08826375  0.07183304] --u_hat 8\n",
      "----- Epoch 0 Loss:0.2787979543209076-----c: [ 0.11033136  0.09778161  0.11617926  0.10464671  0.11836551  0.06572137\n",
      "  0.08660851  0.13611364  0.09375605  0.07049603] --u_hat 8\n",
      "----- Epoch 0 Loss:0.3906202018260956-----c: [ 0.10798945  0.09750196  0.11568245  0.10339759  0.11839591  0.06590708\n",
      "  0.08487683  0.13510862  0.10173463  0.06940538] --u_hat 8\n",
      "----- Epoch 0 Loss:0.2819620370864868-----c: [ 0.10550804  0.09786325  0.11432196  0.10178018  0.12405709  0.06414238\n",
      "  0.08259656  0.13461258  0.10691604  0.068202  ] --u_hat 8\n",
      "----- Epoch 0 Loss:0.3658907413482666-----c: [ 0.10360315  0.09842461  0.11450728  0.10022368  0.12644486  0.06320398\n",
      "  0.08294222  0.13329458  0.110127    0.06722858] --u_hat 8\n",
      "----- Epoch 0 Loss:0.3743393123149872-----c: [ 0.10166697  0.09737344  0.11500965  0.09919915  0.12709141  0.06717238\n",
      "  0.08271959  0.13239929  0.11230096  0.06506756] --u_hat 5\n",
      "----- Epoch 0 Loss:0.29042789340019226-----c: [ 0.10108929  0.09770878  0.11538661  0.09882876  0.12740453  0.06768188\n",
      "  0.08298937  0.13168068  0.11282378  0.06440625] --u_hat 6\n",
      "----- Epoch 0 Loss:0.28613102436065674-----c: [ 0.09989449  0.09754567  0.11533393  0.09935503  0.12913257  0.0681529\n",
      "  0.08183754  0.13099565  0.11141831  0.06633394] --u_hat 9\n",
      "----- Epoch 0 Loss:0.3601100444793701-----c: [ 0.09825358  0.09599395  0.11517511  0.09929971  0.12963775  0.07019985\n",
      "  0.0820661   0.13085614  0.10994207  0.06857561] --u_hat 9\n",
      "----- Epoch 0 Loss:0.27597782015800476-----c: [ 0.09589509  0.09517402  0.11453818  0.09744553  0.12840155  0.07247804\n",
      "  0.07968941  0.1303442   0.10922548  0.07680846] --u_hat 9\n",
      "----- Epoch 0 Loss:0.40086567401885986-----c: [ 0.09315623  0.09472606  0.11548642  0.09651739  0.12660559  0.07029772\n",
      "  0.07782825  0.12933856  0.10974845  0.0862951 ] --u_hat 9\n",
      "----- Epoch 0 Loss:0.4957953989505768-----c: [ 0.09052359  0.09253463  0.11550544  0.09409077  0.12474247  0.07073153\n",
      "  0.07817934  0.12939602  0.11123574  0.0930602 ] --u_hat 9\n",
      "----- Epoch 0 Loss:0.307604044675827-----c: [ 0.0896441   0.0905723   0.11407769  0.09259739  0.12400571  0.0688983\n",
      "  0.0792235   0.12895857  0.11300296  0.09901927] --u_hat 9\n",
      "----- Epoch 0 Loss:0.24961335957050323-----c: [ 0.08864412  0.08897948  0.11322168  0.09295403  0.1258671   0.06925892\n",
      "  0.08033218  0.12795737  0.1140946   0.09869041] --u_hat 9\n",
      "----- Epoch 0 Loss:0.4089430570602417-----c: [ 0.08845398  0.08794496  0.11323476  0.09278727  0.12567483  0.06776843\n",
      "  0.08205494  0.12802237  0.11389894  0.10015957] --u_hat 6\n",
      "----- Epoch 0 Loss:0.23235313594341278-----c: [ 0.08901411  0.0885748   0.1127784   0.09341116  0.1239932   0.06916757\n",
      "  0.08306371  0.12887369  0.11355534  0.09756793] --u_hat 0\n",
      "----- Epoch 0 Loss:0.27255094051361084-----c: [ 0.09032345  0.08966242  0.11254197  0.09376228  0.12288957  0.06938636\n",
      "  0.08341587  0.12962608  0.11216153  0.09623042] --u_hat 0\n",
      "----- Epoch 0 Loss:0.24614281952381134-----c: [ 0.08984581  0.08912246  0.1121601   0.09502627  0.12179653  0.06831211\n",
      "  0.08554816  0.13129956  0.11158085  0.09530801] --u_hat 6\n",
      "----- Epoch 0 Loss:0.22025835514068604-----c: [ 0.08907832  0.08863133  0.11142454  0.09652042  0.12071984  0.06974817\n",
      "  0.08478713  0.13219474  0.11283854  0.0940568 ] --u_hat 5\n",
      "----- Epoch 0 Loss:0.39542683959007263-----c: [ 0.08870158  0.08783465  0.110728    0.09626772  0.12196419  0.06908944\n",
      "  0.08571719  0.13264322  0.11380956  0.09324452] --u_hat 6\n",
      "----- Epoch 0 Loss:0.4019012153148651-----c: [ 0.08747631  0.08715287  0.10955756  0.09784313  0.12062207  0.06826969\n",
      "  0.08660274  0.1335253   0.11654246  0.09240817] --u_hat 8\n",
      "----- Epoch 0 Loss:0.6711392402648926-----c: [ 0.08652726  0.08647189  0.10885558  0.09829093  0.12029681  0.06901497\n",
      "  0.0863106   0.13313085  0.11938158  0.09171939] --u_hat 5\n",
      "----- Epoch 0 Loss:0.7055408954620361-----c: [ 0.0857738   0.08586874  0.10824489  0.09819286  0.12152997  0.06979374\n",
      "  0.08541748  0.13278884  0.12017241  0.09221716] --u_hat 5\n",
      "----- Epoch 0 Loss:0.4065845012664795-----c: [ 0.08814877  0.08583993  0.10796881  0.09754839  0.12154005  0.06988514\n",
      "  0.08421879  0.13249986  0.11955583  0.09279419] --u_hat 0\n",
      "----- Epoch 0 Loss:0.6162551641464233-----c: [ 0.08773504  0.08541069  0.10824811  0.09783887  0.12214524  0.06952217\n",
      "  0.08478424  0.13239621  0.11940371  0.09251591] --u_hat 9\n",
      "----- Epoch 0 Loss:0.31303858757019043-----c: [ 0.08704902  0.08509929  0.1099827   0.09712394  0.12128798  0.07056963\n",
      "  0.08351617  0.13388459  0.11770443  0.09378207] --u_hat 9\n",
      "----- Epoch 0 Loss:0.4609719514846802-----c: [ 0.08987847  0.08557922  0.11202583  0.09714598  0.12039374  0.0694778\n",
      "  0.0825285   0.13352077  0.11560346  0.09384608] --u_hat 0\n",
      "----- Epoch 0 Loss:0.28569480776786804-----c: [ 0.09061947  0.08744444  0.11425613  0.09707854  0.11856236  0.06837553\n",
      "  0.08132715  0.13348064  0.11300659  0.0958491 ] --u_hat 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 0 Loss:0.26957669854164124-----c: [ 0.09474055  0.08705369  0.1141009   0.09574497  0.11795545  0.06761346\n",
      "  0.08134813  0.13325854  0.11082378  0.09736052] --u_hat 9\n",
      "----- Epoch 0 Loss:0.43706345558166504-----c: [ 0.09613983  0.0864736   0.11545204  0.09499647  0.11725227  0.06860563\n",
      "  0.07978226  0.13335735  0.10867651  0.09926412] --u_hat 9\n",
      "----- Epoch 0 Loss:0.4075956344604492-----c: [ 0.09908758  0.08532362  0.11415192  0.09383383  0.11783314  0.06953366\n",
      "  0.08209208  0.1327327   0.10765132  0.09776007] --u_hat 6\n",
      "----- Epoch 0 Loss:0.3074035942554474-----c: [ 0.09882631  0.08490945  0.1131091   0.09339055  0.11714672  0.06915295\n",
      "  0.08315252  0.13583475  0.10719094  0.09728635] --u_hat 6\n",
      "----- Epoch 0 Loss:0.39857327938079834-----c: [ 0.0983487   0.08482257  0.11305266  0.09231343  0.11716281  0.06906858\n",
      "  0.08306073  0.1380285   0.10688479  0.09725744] --u_hat 6\n",
      "----- Epoch 0 Loss:0.3397235870361328-----c: [ 0.09730717  0.08421084  0.11210258  0.09225871  0.11694062  0.07181717\n",
      "  0.08284696  0.13882124  0.10695682  0.09673794] --u_hat 5\n",
      "----- Epoch 0 Loss:0.2578263282775879-----c: [ 0.09759928  0.08357432  0.11302409  0.09315705  0.11604096  0.07122876\n",
      "  0.08341846  0.13982214  0.1065736   0.09556163] --u_hat 6\n",
      "----- Epoch 0 Loss:0.3593873977661133-----c: [ 0.09777812  0.08336851  0.11354396  0.09465215  0.11660673  0.07126419\n",
      "  0.08295815  0.1392349   0.10586489  0.09472796] --u_hat 9\n",
      "----- Epoch 0 Loss:0.19231049716472626-----c: [ 0.09749596  0.08341764  0.11349607  0.0960181   0.11820368  0.07030171\n",
      "  0.08210769  0.13948019  0.10504303  0.09443599] --u_hat 9\n",
      "----- Epoch 0 Loss:0.4458991587162018-----c: [ 0.09701604  0.08288142  0.11341369  0.09668671  0.11784487  0.06974895\n",
      "  0.08322671  0.13968973  0.10584369  0.09364832] --u_hat 9\n",
      "----- Epoch 0 Loss:0.22862009704113007-----c: [ 0.09616214  0.08362394  0.11306209  0.09602786  0.11994166  0.06903622\n",
      "  0.08236063  0.13978986  0.10553844  0.09445708] --u_hat 9\n",
      "----- Epoch 0 Loss:0.2845053970813751-----c: [ 0.09505754  0.08316424  0.112663    0.09612701  0.12123687  0.07058206\n",
      "  0.0815502   0.13982411  0.10540502  0.09439012] --u_hat 9\n",
      "----- Epoch 0 Loss:0.21847455203533173-----c: [ 0.09729042  0.08315866  0.11251824  0.09801879  0.12031765  0.06909823\n",
      "  0.08006786  0.13939756  0.10475755  0.0953751 ] --u_hat 9\n",
      "----- Epoch 0 Loss:0.3313966691493988-----c: [ 0.1022946   0.08257708  0.11213753  0.09798899  0.12006809  0.06874863\n",
      "  0.07870193  0.13916913  0.10341018  0.09490367] --u_hat 0\n",
      "----- Epoch 0 Loss:0.2620958983898163-----c: [ 0.10114095  0.08240778  0.11057864  0.09925955  0.12073065  0.06922308\n",
      "  0.07699957  0.13861552  0.1052454   0.09579865] --u_hat 9\n",
      "----- Epoch 0 Loss:0.3644505739212036-----c: [ 0.10327157  0.08140969  0.10898285  0.09918734  0.12167507  0.06953725\n",
      "  0.07882147  0.13915992  0.1040835   0.09387133] --u_hat 0\n",
      "----- Epoch 0 Loss:0.2889699637889862-----c: [ 0.10305603  0.08281329  0.10718562  0.09899047  0.12466249  0.06821635\n",
      "  0.07790297  0.13995731  0.10250571  0.09470978] --u_hat 9\n",
      "----- Epoch 0 Loss:0.1682632714509964-----c: [ 0.10605078  0.08182756  0.10586499  0.0983049   0.12471615  0.06764465\n",
      "  0.07929888  0.13995594  0.10288509  0.09345107] --u_hat 6\n",
      "----- Epoch 0 Loss:0.19001783430576324-----c: [ 0.10497509  0.08101038  0.10630909  0.09773631  0.12470046  0.06933539\n",
      "  0.0807288   0.13935277  0.10291243  0.09293948] --u_hat 6\n",
      "----- Epoch 0 Loss:0.24827225506305695-----c: [ 0.1054715   0.08009022  0.10587253  0.09674631  0.12361582  0.0728455\n",
      "  0.08221198  0.13994682  0.10265264  0.09054659] --u_hat 5\n",
      "----- Epoch 0 Loss:0.40921518206596375-----c: [ 0.10494875  0.08086244  0.1056819   0.09602138  0.12339696  0.073276\n",
      "  0.08331974  0.14064609  0.10253993  0.08930685] --u_hat 1\n",
      "----- Epoch 0 Loss:0.14245881140232086-----c: [ 0.10419141  0.08199903  0.10537454  0.09661634  0.12278774  0.07491039\n",
      "  0.08303355  0.14099422  0.1021639   0.08792895] --u_hat 5\n",
      "----- Epoch 0 Loss:0.282746821641922-----c: [ 0.10523619  0.08203354  0.10746336  0.09788723  0.12176536  0.07412945\n",
      "  0.08222565  0.14124179  0.10090383  0.08711372] --u_hat 3\n",
      "----- Epoch 0 Loss:0.35096463561058044-----c: [ 0.104781    0.0815761   0.1091977   0.09742453  0.1211888   0.07381919\n",
      "  0.08419214  0.14123215  0.10033611  0.08625232] --u_hat 6\n",
      "----- Epoch 0 Loss:0.18293528258800507-----c: [ 0.104141    0.08174947  0.10855403  0.09699039  0.1222504   0.07345305\n",
      "  0.08450069  0.14144628  0.10068444  0.08623016] --u_hat 9\n",
      "----- Epoch 0 Loss:0.13213922083377838-----c: [ 0.10327399  0.08184856  0.1101099   0.09884465  0.12164739  0.07242355\n",
      "  0.0854127   0.14074835  0.09972328  0.08596775] --u_hat 9\n",
      "----- Epoch 0 Loss:0.227408304810524-----c: [ 0.10212413  0.08121222  0.11060841  0.09973306  0.1239017   0.07256094\n",
      "  0.08553962  0.14030458  0.09839798  0.08561739] --u_hat 9\n",
      "----- Epoch 0 Loss:0.2478983849287033-----c: [ 0.10055707  0.08038969  0.11301132  0.09839532  0.12795597  0.07151547\n",
      "  0.08546391  0.14057107  0.09647813  0.08566204] --u_hat 4\n",
      "----- Epoch 0 Loss:0.20138788223266602-----c: [ 0.10036735  0.07932827  0.11148659  0.09794844  0.13174669  0.07015446\n",
      "  0.08588395  0.14107603  0.09528388  0.08672437] --u_hat 9\n",
      "----- Epoch 0 Loss:0.37365958094596863-----c: [ 0.09883148  0.07924049  0.11194254  0.09656551  0.13503125  0.07121775\n",
      "  0.08437804  0.14027433  0.09419474  0.08832374] --u_hat 9\n",
      "----- Epoch 0 Loss:0.21242539584636688-----c: [ 0.09807379  0.07893965  0.11087266  0.0982376   0.13635886  0.06951401\n",
      "  0.08523662  0.13924915  0.09266601  0.09085171] --u_hat 9\n",
      "----- Epoch 0 Loss:0.22781889140605927-----c: [ 0.09803677  0.07859349  0.1122006   0.09815039  0.13491823  0.06978632\n",
      "  0.08573917  0.13957313  0.09160402  0.0913976 ] --u_hat 6\n",
      "----- Epoch 0 Loss:0.16623081266880035-----c: [ 0.09837631  0.07789186  0.11342291  0.09813831  0.13393509  0.0707408\n",
      "  0.0858618   0.13938975  0.09129788  0.0909453 ] --u_hat 6\n",
      "----- Epoch 0 Loss:0.26375865936279297-----c: [ 0.10017887  0.07847579  0.11404101  0.09871513  0.13383444  0.07028265\n",
      "  0.08490109  0.13896789  0.09000999  0.09059291] --u_hat 0\n",
      "----- Epoch 0 Loss:0.25028494000434875-----c: [ 0.10631599  0.07875381  0.11400288  0.09887055  0.13353755  0.0695642\n",
      "  0.08364716  0.13831946  0.08812714  0.088861  ] --u_hat 0\n",
      "----- Epoch 0 Loss:0.34976473450660706-----c: [ 0.10823299  0.07880004  0.11437823  0.09893098  0.13518003  0.06921688\n",
      "  0.08286641  0.13771416  0.08661907  0.08806096] --u_hat 0\n",
      "----- Epoch 0 Loss:0.45836353302001953-----c: [ 0.10977992  0.07880501  0.11407956  0.09813122  0.1365698   0.06965604\n",
      "  0.08267985  0.13833801  0.085634    0.08632675] --u_hat 0\n",
      "----- Epoch 0 Loss:0.2835765779018402-----c: [ 0.112232    0.07886342  0.11489084  0.09748596  0.13603319  0.06878717\n",
      "  0.0818738   0.13841186  0.08500422  0.08641736] --u_hat 0\n",
      "----- Epoch 0 Loss:0.10502278059720993-----c: [ 0.11273616  0.07917616  0.1152916   0.09833357  0.1370974   0.06791902\n",
      "  0.08077246  0.13810386  0.08356377  0.08700594] --u_hat 9\n",
      "----- Epoch 0 Loss:0.14473962783813477-----c: [ 0.11169986  0.07846595  0.11415204  0.09793227  0.13809226  0.06815231\n",
      "  0.0808614   0.13916065  0.08336593  0.08811667] --u_hat 9\n",
      "----- Epoch 0 Loss:0.15484340488910675-----c: [ 0.11058905  0.07888138  0.11327215  0.0971382   0.13806184  0.06825157\n",
      "  0.08006881  0.13990013  0.0837774   0.09005941] --u_hat 9\n",
      "----- Epoch 0 Loss:0.2105586677789688-----c: [ 0.11162898  0.07912588  0.11333004  0.09678654  0.14034447  0.06721061\n",
      "  0.07880434  0.13932575  0.08303408  0.09040931] --u_hat 9\n",
      "----- Epoch 0 Loss:0.2482936978340149-----c: [ 0.11090098  0.07905993  0.11231074  0.09704593  0.14160226  0.06689873\n",
      "  0.07801896  0.13971615  0.08258568  0.09186038] --u_hat 9\n",
      "----- Epoch 0 Loss:0.29780662059783936-----c: [ 0.11009091  0.07989297  0.11314043  0.09768576  0.14112115  0.06617091\n",
      "  0.07703831  0.13931675  0.08261135  0.0929315 ] --u_hat 9\n",
      "----- Epoch 0 Loss:0.19233469665050507-----c: [ 0.10989377  0.07995199  0.112457    0.0970371   0.14398135  0.06558256\n",
      "  0.07700803  0.13881405  0.08204604  0.09322807] --u_hat 9\n",
      "----- Epoch 0 Loss:0.10571590811014175-----c: [ 0.10938688  0.07939354  0.11185601  0.09800252  0.14374202  0.06527957\n",
      "  0.07934273  0.13888501  0.08154492  0.09256671] --u_hat 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 0 Loss:0.3397844731807709-----c: [ 0.11057916  0.07914664  0.11294761  0.10074674  0.14257698  0.06470861\n",
      "  0.07912646  0.13831729  0.0805299   0.09132081] --u_hat 3\n",
      "----- Epoch 0 Loss:0.29852747917175293-----c: [ 0.11299235  0.07993878  0.1124538   0.1000939   0.14340757  0.0641247\n",
      "  0.07911903  0.13791798  0.07981545  0.09013628] --u_hat 6\n",
      "----- Epoch 0 Loss:0.15570460259914398-----c: [ 0.113365    0.07974076  0.11180913  0.10106279  0.1426709   0.06523181\n",
      "  0.07966816  0.13831525  0.07929218  0.08884399] --u_hat 5\n",
      "----- Epoch 0 Loss:0.13284747302532196-----c: [ 0.11279324  0.08074578  0.11119772  0.10067371  0.14461501  0.06460433\n",
      "  0.07975662  0.13820836  0.07905546  0.08835001] --u_hat 1\n",
      "----- Epoch 0 Loss:0.06459647417068481-----c: [ 0.11376251  0.08278029  0.11077378  0.10156394  0.14409716  0.06448895\n",
      "  0.0791603   0.13803875  0.07842324  0.08691092] --u_hat 1\n",
      "----- Epoch 0 Loss:0.19664783775806427-----c: [ 0.11492152  0.08282773  0.1103284   0.10126835  0.1440853   0.06368444\n",
      "  0.08164927  0.13784739  0.07787296  0.08551458] --u_hat 6\n",
      "----- Epoch 0 Loss:0.15132558345794678-----c: [ 0.11460376  0.08441632  0.11297639  0.10090846  0.1435761   0.06328063\n",
      "  0.08082794  0.13754259  0.07725745  0.08461016] --u_hat 1\n",
      "----- Epoch 0 Loss:0.07123371958732605-----c: [ 0.11404935  0.08575872  0.11257083  0.10094413  0.14290863  0.06403664\n",
      "  0.0801384   0.13844673  0.07687061  0.08427611] --u_hat 1\n",
      "----- Epoch 0 Loss:0.15533070266246796-----c: [ 0.1158056   0.08547195  0.11376706  0.10097198  0.14227866  0.06403652\n",
      "  0.08022062  0.13804802  0.07629991  0.08309967] --u_hat 0\n",
      "----- Epoch 0 Loss:0.14631830155849457-----c: [ 0.11607912  0.08523379  0.11375608  0.1006482   0.14154229  0.06325105\n",
      "  0.08026423  0.13813235  0.07574503  0.08534762] --u_hat 9\n",
      "----- Epoch 0 Loss:0.07215464860200882-----c: [ 0.11511633  0.08571769  0.11464063  0.10071716  0.14152354  0.06257898\n",
      "  0.07925972  0.13865905  0.07486913  0.08691771] --u_hat 9\n",
      "----- Epoch 0 Loss:0.24921123683452606-----c: [ 0.11597092  0.08511536  0.1141867   0.10004464  0.14120939  0.06183125\n",
      "  0.07909717  0.13842852  0.07459928  0.08951685] --u_hat 9\n",
      "----- Epoch 0 Loss:0.11678788810968399-----c: [ 0.11483906  0.08434596  0.11343953  0.0990229   0.14010106  0.06061782\n",
      "  0.07826799  0.1398752   0.07424247  0.0952479 ] --u_hat 9\n",
      "----- Epoch 0 Loss:0.189227893948555-----c: [ 0.11354227  0.08391393  0.11746535  0.09997175  0.13813843  0.06013328\n",
      "  0.07702588  0.13908523  0.07271612  0.09800746] --u_hat 9\n",
      "----- Epoch 0 Loss:0.2892549932003021-----c: [ 0.11241465  0.08308372  0.11664482  0.09904363  0.13991159  0.05927806\n",
      "  0.07743616  0.13866609  0.07260925  0.10091207] --u_hat 9\n",
      "----- Epoch 0 Loss:0.11326869577169418-----c: [ 0.11231676  0.08234088  0.1177223   0.09817445  0.14227678  0.05822192\n",
      "  0.07796454  0.1380032   0.07143894  0.10154018] --u_hat 4\n",
      "----- Epoch 0 Loss:0.09865275770425797-----c: [ 0.11443588  0.08214261  0.11920696  0.09760807  0.14142415  0.0578219\n",
      "  0.07738257  0.13855799  0.0707903   0.10062943] --u_hat 0\n",
      "----- Epoch 0 Loss:0.06510338187217712-----c: [ 0.11383304  0.08145554  0.11848329  0.09842309  0.14056823  0.05739111\n",
      "  0.08018295  0.1381291   0.07235099  0.09918261] --u_hat 6\n",
      "----- Epoch 0 Loss:0.26626184582710266-----c: [ 0.11338982  0.08109728  0.11799647  0.09799089  0.14030375  0.05887036\n",
      "  0.0808074   0.13791198  0.07307168  0.0985603 ] --u_hat 6\n",
      "----- Epoch 0 Loss:0.18907292187213898-----c: [ 0.11281366  0.08049699  0.11690566  0.0993485   0.13938913  0.05810509\n",
      "  0.08129376  0.13862115  0.07499828  0.09802768] --u_hat 6\n",
      "----- Epoch 0 Loss:0.14552627503871918-----c: [ 0.11649288  0.0797447   0.11620378  0.10010408  0.13826841  0.05831113\n",
      "  0.08063591  0.13814387  0.07401908  0.09807617] --u_hat 0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "graph, end_points = build_graph(batch_size, is_train=True, add_summaries=True, print_shape=False)\n",
    "n_epochs = 1\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(end_points[\"init\"])\n",
    "    saver = tf.summary.FileWriter(\"..//log\", graph=graph)\n",
    "    \n",
    "    other_variables = [v for v in graph.get_collection(\"variables\") if v.name=='b_prior:0']\n",
    "    trainable_variables = graph.get_collection(\"trainable_variables\")\n",
    "    variables_to_restore = trainable_variables.extend(other_variables)\n",
    "    \n",
    "    model_saver = tf.train.Saver(variables_to_restore)\n",
    "   \n",
    "    loss_report_counter = 0\n",
    "    nan_caps=list()\n",
    "    c_list=list()\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        mnist = Mnist(path=\"..//\", train=True)\n",
    "        data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)  \n",
    "        \n",
    "        i=0 \n",
    "     \n",
    "        while i<1000:\n",
    "            i+=1\n",
    "      \n",
    "            data = np.expand_dims(data, -1)\n",
    "            feed_dict = {end_points[\"X\"]:data, end_points[\"Y\"]:lbls}\n",
    "            _, loss, grads, u_hat, c_bef, c_aft, summaries_merged = sess.run([end_points[\"optimize\"], \n",
    "                                                        end_points[\"loss\"],\n",
    "                                                        end_points[\"grads\"],\n",
    "                                                        end_points[\"u_hat\"],\n",
    "                                                        end_points[\"c\"],\n",
    "                                                        end_points[\"c_routed\"],\n",
    "                                                        end_points[\"summaries_merged\"]], feed_dict=feed_dict)\n",
    "\n",
    "            loss_report_counter +=1\n",
    "            if(loss_report_counter%1==0):\n",
    "                print(\"----- Epoch {} Loss:{}-----c: {} --u_hat {}\".\n",
    "                      format(epoch, loss, c_aft.mean(axis=1).squeeze(), \n",
    "                     np.argmax(np.linalg.norm(u_hat.mean(axis=0).mean(axis=0), axis=1))))\n",
    "                c_list.append([c_bef, c_aft])\n",
    "                \n",
    "                #if np.any(np.isnan(grads[0][0])==True):\n",
    "                    #nan_caps.append((prob, Y))\n",
    "                    #break;\n",
    "                saver.add_summary(summaries_merged, global_step=loss_report_counter)\n",
    "            data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True) \n",
    "        \n",
    "                \n",
    "\n",
    "                \n",
    "    \n",
    "    print(\"Model saves under path {}\".format(model_saver.save(sess,save_path=\"..//saved_model//capsnet\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the structure based on mnist with batch size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "graph, end_points = build_graph(batch_size, is_train=True, high_routing=True, add_summaries=False, print_shape=False)\n",
    "\n",
    "mnist = Mnist(path=\"..//\", train=True)\n",
    "#with scale there is a problem beacuse weights are still low and outputs gets small\n",
    "#for testing we simply override scaling to test the functionality\n",
    "\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)    \n",
    "data = np.expand_dims(data, -1)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "   \n",
    "    init = end_points[\"init\"]\n",
    "    X = end_points[\"X\"]\n",
    "    Y = end_points[\"Y\"]\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    feed_dict = {X:data, Y:lbls}\n",
    "\n",
    "    \n",
    "    \n",
    "    #\"Test output of conv 1\"\n",
    "    x, w, b, conv_1_act =  sess.run([X, graph.get_tensor_by_name('Conv/weights:0'), \n",
    "                                     graph.get_tensor_by_name('Conv/biases:0'), \n",
    "                                     end_points[\"conv_layer_1_act\"]], \n",
    "                                     feed_dict=feed_dict)\n",
    "    \n",
    "    assert(np.max([0, np.sum((np.matmul(np.expand_dims(x[0, 0:9, 0:9], axis=3), w) + b)[:,:,:,0])]) - \n",
    "           conv_1_act[0, 0, 0, 0]<0.0001)\n",
    "    assert(conv_1_act.shape == (2, 20, 20, 256))\n",
    "    \n",
    "    #Test for capsule out values\"\n",
    "    #Capsule 8\n",
    "    \n",
    "    w, b, conv_1_act, capsule_7_act, capsule_2_act =  sess.run([graph.get_tensor_by_name('Capsule_Layer/Conv_7/weights:0'), \n",
    "                                     graph.get_tensor_by_name('Capsule_Layer/Conv_7/biases:0'), \n",
    "                                                 end_points[\"conv_layer_1_act\"],\n",
    "                                                 end_points[\"capsule_7_act\"],\n",
    "                                                 end_points[\"capsule_2_act\"]],\n",
    "                                                 feed_dict=feed_dict)\n",
    "    \n",
    "    assert(np.linalg.norm(np.sum((np.matmul(np.expand_dims(conv_1_act[0, 0:9, 0:9], axis=2), w) + b), axis=(0, 1, 2)) - \n",
    "           capsule_7_act[0, 0, 0, :])<0.00005)\n",
    "    \n",
    "    assert(capsule_7_act.shape==(2, 6, 6, 8))\n",
    "    \n",
    "    #Test for not sharing the weights\n",
    "    assert(np.linalg.norm(capsule_7_act[0, 0, 0, :] - capsule_2_act[0, 0, 0, :])>0.01)\n",
    "    \n",
    "    #Test the squash function before routing\n",
    "    v, s, b_prior, W_t, b_t, capsule_7_act, u, u_hat = sess.run([end_points[\"v\"], \n",
    "                                                                 end_points[\"s\"],\n",
    "                                                                 end_points[\"b_prior\"],\n",
    "                                                                 end_points[\"W_t\"],\n",
    "                                                                 end_points[\"b_t\"],\n",
    "                                                                 end_points[\"capsule_7_act\"],\n",
    "                                                                 end_points[\"u\"],\n",
    "                                                                 end_points[\"u_hat\"]], feed_dict=feed_dict)\n",
    "    \n",
    "    #Test for the first one digit 5\n",
    "    u = np.squeeze(u[0,252:288,...], axis=1)\n",
    "    W_t = np.squeeze(W_t[:, 252:288, 5, ...])\n",
    "    b_t = b_t[:, 252:288, 5, ...]\n",
    "    u_hat_7_5 = np.add(np.sum(np.multiply(u, W_t), axis=2), b_t)\n",
    "    \n",
    "    assert(np.linalg.norm(u_hat_7_5 - u_hat[0,252:288, 5,...])<0.0001)\n",
    "    b_prior_all_10= np.squeeze(b_prior[0,:,...])\n",
    "    b_prior_7_5 =np.squeeze(b_prior[0,:, 5,...])\n",
    "    c_7_5 = np.exp(b_prior_7_5)/np.sum(np.exp(b_prior_all_10), axis=1)\n",
    "    c_7_5 = np.expand_dims(c_7_5, axis=1)\n",
    "    u_hat_5 =  np.squeeze(u_hat[0, :, 5,...])\n",
    "    s_5 = np.sum(np.multiply(c_7_5, u_hat_5), axis=0)\n",
    "    \n",
    "    #Testing the vector for element 5\n",
    "    assert(np.linalg.norm(s_5 - s[0, 5,...])<0.00001)\n",
    "    \n",
    "    #test squash\n",
    "    v_5 = (np.power(np.linalg.norm(s_5), 2)/(1+np.power(np.linalg.norm(s_5), 2)))*(s_5/np.linalg.norm(s_5))\n",
    "    assert(np.linalg.norm(v_5) - np.linalg.norm(v[0, 5, ...])<0.00001)\n",
    "  \n",
    "    #Test routing\n",
    "    \n",
    "    _, v_before, s_before, c_before, v_routed,s_routed, \\\n",
    "    c_routed, u_hat_routed, b_prior, b_prior_routed, batch_item_count = sess.run([\n",
    "                                     end_points[\"route_op\"],\n",
    "                                     end_points[\"v\"],\n",
    "                                     end_points[\"s\"],\n",
    "                                     end_points[\"c\"],\n",
    "                                     end_points[\"v_routed\"],\n",
    "                                     end_points[\"s_routed\"],\n",
    "                                     end_points[\"c_routed\"],\n",
    "                                     end_points[\"u_hat_routed\"],\n",
    "                                     end_points[\"b_prior\"],\n",
    "                                     end_points[\"b_prior_routed\"],\n",
    "                                     end_points[\"batch_item_cnt\"]], feed_dict=feed_dict)\n",
    "  \n",
    "    assert(np.all(u_hat_routed==u_hat))\n",
    "    \n",
    "    ###############################################################################################\n",
    "    # Here we are measuring the agreements before and after applying the routing algorithms and   #\n",
    "    # we are testing that the expected value for each class in fact increased meaning that        #\n",
    "    # the routing algorithm helped to route them to better digiCaps                               #\n",
    "    ###############################################################################################\n",
    "    \n",
    "    aggr_before, aggr_after = np.sum(np.multiply(v_before[0,...], u_hat[0,...]), axis=2), \\\n",
    "        np.sum(np.multiply(v_routed[0,...], u_hat[0,...]), axis=2)\n",
    "  \n",
    "    assert(np.min(aggr_after.mean(axis=0) - aggr_before.mean(axis=0))>0)\n",
    "    \n",
    "    #Test the losses\n",
    "    \n",
    "    X, v_out, margin_loss_present, margin_loss_not_present, total_margin_loss, fc_out, reconstruction_loss, loss, v_norm =\\\n",
    "                                                            sess.run([ end_points[k] \n",
    "                                                              for k in [\"X\", \"v_routed\", \n",
    "                                                                        \"margin_loss_present\",\n",
    "                                                                        \"margin_loss_not_present\",\n",
    "                                                                        'total_margin_loss', \n",
    "                                                                        \"fc_out\",\n",
    "                                                                        'reconstruction_loss', \n",
    "                                                                        'loss', \"v_norm\"]], feed_dict=feed_dict)\n",
    "    #print(v_out)\n",
    "    \n",
    "    assert(np.all(np.where(margin_loss_present != 0.)[1] == np.array([5, 0])))\n",
    "    \n",
    "    #assert(np.all(np.where(margin_loss_not_present != 0.)[1] == np.array([0, 1, 2, 3, 4,6,7,8,9,1,2,3,4,5,6,7,8,9])))\n",
    "    \n",
    "    assert(margin_loss_present[0][5] - np.power(np.max([0, 0.9 - np.linalg.norm(v_out[0][5])]), 2)<0.00001)\n",
    "    \n",
    "    assert(np.linalg.norm(margin_loss_not_present[0][0] - \n",
    "                          0.5*np.power(np.max([0., np.linalg.norm(v_out[0][0])-0.1]), 2))<0.00001)\n",
    "    \n",
    "    assert(np.linalg.norm(total_margin_loss[0] - np.sum(margin_loss_present[0]+ margin_loss_not_present[0]))<0.00001)\n",
    "    \n",
    "    assert(np.abs(np.sum(np.square(fc_out[0] - X[0].reshape(784,)))*0.0005- reconstruction_loss[0])<0.01)\n",
    "    \n",
    "    assert(loss == np.mean(np.add(reconstruction_loss, total_margin_loss)))\n",
    "    print(\"All done\")\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the capsule network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper mentiones shifting images by two pixels but it is not done here<br/>\n",
    "I also did not add noise for reconstruction make sure the essentials work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "tf.reset_default_graph()\n",
    "graph, end_points = build_graph(batch_size, is_train=False, add_summaries=False, print_shape=True)\n",
    "\n",
    "mnist = Mnist(path=\"..\\\\\", train=True)\n",
    "#with scale there is a problem beacuse weights are still low and outputs gets small\n",
    "#for testing we simply override scaling to test the functionality\n",
    "\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)    \n",
    "data = np.expand_dims(data, -1)\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    restorer = tf.train.Saver()\n",
    "    restorer.restore(sess, save_path=\"..\\\\saved_model\\\\capsnet\")\n",
    "    feed_dict = {end_points[\"X\"]:data, end_points[\"Y\"]:lbls}\n",
    "    print(sess.run(end_points[\"b_prior\"], feed_dict=feed_dict))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1152, 10, 1)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_caps[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.isnan(o)==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.where(np.isnan(nan_caps[3]) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1152, 10, 1)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_prior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margin_loss_present.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 6, 5, 7, 7, 8, 8, 9, 7, 4, 7, 3, 2, 0, 8, 6, 8, 6, 1, 6, 8, 9, 4,\n",
       "       0, 9, 0, 4, 1, 5, 4, 7, 5, 3, 7, 4, 9, 8, 5, 8, 6, 3, 8, 6, 9, 9, 1,\n",
       "       8, 3, 5, 8, 6, 5, 9, 7, 2, 5, 0, 8, 5, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.linalg.norm(u_hat.mean(axis=0).mean(axis=0), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
