{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mnist:\n",
    "    def __init__(self, path, train=True):\n",
    "        if train:\n",
    "            image_file_name = \"train-images.idx3-ubyte\"\n",
    "            label_file_name = \"train-labels.idx1-ubyte\"\n",
    "        else:\n",
    "            image_file_name = \"t10k-images.idx3-ubyte\"\n",
    "            label_file_name = \"t10k-labels.idx1-ubyte\"\n",
    "            \n",
    "        self.f = open(\"{}//{}\".format(path, image_file_name), \"rb\")\n",
    "        magic_number, number_of_images, image_rows, image_columns = int(self.f.read(4).hex(), 16), \\\n",
    "        int(self.f.read(4).hex(), 16), int(self.f.read(4).hex(), 16), int(self.f.read(4).hex(), 16)\n",
    "        print(\"Images magic no: {},  No of images: {}, Image rows: {}, Image cols: {}\".\\\n",
    "          format(magic_number, number_of_images, image_rows, image_columns))\n",
    "\n",
    "        self.flab = open(\"{}//{}\".format(path, label_file_name), \"rb\")\n",
    "        magic_number, number_of_items = int(self.flab.read(4).hex(), 16), int(self.flab.read(4).hex(), 16)\n",
    "        print(\"Labels magic no: {}, No of items: {}\".format(magic_number, number_of_items))\n",
    "        \n",
    "\n",
    "    def _get_next(self):\n",
    "        val = self.f.read(28*28)\n",
    "        if len(val)>0:\n",
    "            img = val.hex()\n",
    "            idxs = np.arange(0, len(img), 2).astype(int)\n",
    "            return(np.reshape([int(img[i:(i+2)], 16) for i in idxs], (28, 28)), int(self.flab.read(1).hex(), 16)) \n",
    "        else:\n",
    "            return(\"\", \"\")\n",
    "\n",
    "    def get_batch(self, batch_size=20, scale=True):\n",
    "        im_lst = np.zeros(shape=[batch_size, 28, 28], dtype=\"float\")\n",
    "        lbl_lst = np.zeros(shape=[batch_size], dtype=\"int\")\n",
    "        lbl_one_hot_lst = np.zeros(shape=[batch_size, 10], dtype=\"int\")\n",
    "        for i in range(batch_size):\n",
    "            im, lbl = self._get_next()\n",
    "           \n",
    "            if len(im)>0:\n",
    "                if scale:\n",
    "                    im=im/255.\n",
    "                im_lst[i,...] = im\n",
    "                lbl_lst[i] = lbl\n",
    "                lbl_one_hot_lst[i,lbl] = 1\n",
    "        return im_lst, lbl_lst, lbl_one_hot_lst\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_s(b_prior, u_hat):\n",
    "    c = tf.nn.softmax(b_prior, dim=2)\n",
    "    s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1)\n",
    "    return(s, c)\n",
    "\n",
    "\n",
    "#Squash\n",
    "def get_v(s):\n",
    "    norm_s = tf.norm(s, axis=2, keep_dims=True)\n",
    "    norm_s_2 = tf.pow(norm_s, 2)\n",
    "    v = (tf.multiply(norm_s_2,s))/(tf.multiply(1+norm_s_2, norm_s))\n",
    "\n",
    "    return(v)\n",
    "\n",
    "\n",
    "def route(n_iter, batch_item_cnt, v, b_prior, u_hat, aggr):\n",
    "    u_hat_stopped = tf.stop_gradient(u_hat)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "    #I don't think we need at all gradient activity at this layer\n",
    "    \n",
    "        #if i == n_iter-1:\n",
    "        #    with tf.name_scope(\"Iteration_{}\".format(i)):\n",
    "        #        s, c = get_s(b_prior, u_hat)\n",
    "        #        v = get_v(s)\n",
    "        #        aggr = tf.reduce_sum(tf.multiply(v[batch_item_cnt, :, :], u_hat[batch_item_cnt, :, :]), axis=-1)\n",
    "        #        aggr = tf.expand_dims(aggr, axis=0)\n",
    "        #        aggr = tf.expand_dims(aggr, axis=3)\n",
    "        #        b_prior = tf.add(b_prior, aggr)\n",
    "        #else:\n",
    "        with tf.name_scope(\"Iteration_{}\".format(i)):\n",
    "            s, c = get_s(b_prior, u_hat_stopped)\n",
    "            v = get_v(s)\n",
    "            aggr = tf.reduce_sum(tf.multiply(v[batch_item_cnt, :, :], u_hat_stopped[batch_item_cnt, :, :]), axis=-1)\n",
    "            aggr = tf.expand_dims(aggr, axis=0)\n",
    "            aggr = tf.expand_dims(aggr, axis=3)\n",
    "            b_prior = tf.add(b_prior, aggr)\n",
    "            \n",
    "    return (v, b_prior, aggr)\n",
    "\n",
    "def update_prior(batch_item_cnt, v, b_prior, u_hat, aggr):\n",
    "    v, b_prior, aggr = route(3, batch_item_cnt, v, b_prior, u_hat, aggr)\n",
    "    return([tf.add(batch_item_cnt, 1), v, b_prior, u_hat, aggr])\n",
    "\n",
    "def update_prior_10(batch_item_cnt, v, b_prior, u_hat, aggr):\n",
    "    v, b_prior, aggr = route(10, batch_item_cnt, v, b_prior, u_hat, aggr)\n",
    "    return([tf.add(batch_item_cnt, 1), v, b_prior, u_hat])\n",
    "\n",
    "def build_graph(batch_size, is_train=True, high_routing= False, add_summaries=False, print_shape=False):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    end_points = dict()\n",
    "\n",
    "    with graph.as_default():\n",
    "\n",
    "        X = tf.placeholder(shape=[batch_size, 28, 28, 1], name=\"X\", dtype=tf.float32)\n",
    "        end_points[\"X\"] = X\n",
    "        Y = tf.placeholder(shape=[batch_size], name=\"Y\", dtype=tf.int32)\n",
    "        end_points[\"Y\"] = Y\n",
    "        Y_one_hot = tf.one_hot(indices=Y, depth=10)\n",
    "        end_points[\"Y_one_hot\"] = Y_one_hot\n",
    "\n",
    "        #First Layer\n",
    "        with tf.name_scope(\"Conv_Layer_1\"):\n",
    "\n",
    "            conv_1 = tf.contrib.layers.conv2d(inputs=X, \n",
    "                                              num_outputs=256,  \n",
    "                                              kernel_size=9, \n",
    "                                              stride=1, padding=\"VALID\", \n",
    "                                              activation_fn=tf.nn.relu, \n",
    "                                              weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(), \n",
    "                                              biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "            end_points[\"conv_layer_1_act\"] = conv_1\n",
    "\n",
    "        capsules = list()\n",
    "\n",
    "        #In this implementation every capsule layer has unique weights\n",
    "        #I think the meaning of sharing is that in each plane [6, 6, 1]\n",
    "        #each cepsule output sharing the weights\n",
    "        with tf.variable_scope(\"Capsule_Layer\"):\n",
    "            for n in range(32):\n",
    "\n",
    "               #If we user auto resuse then the output of each capsule would be the same and it doesn't make any sense\n",
    "\n",
    "                cap_1 = tf.contrib.layers.conv2d(inputs=conv_1, \n",
    "                                                 #reuse=tf.AUTO_REUSE, \n",
    "                                                 #scope=\"Capsule_Layer\",\n",
    "                                                 num_outputs=8, kernel_size=9, \n",
    "                                                 activation_fn=None,\n",
    "                                                 stride=2, padding=\"VALID\" ,\n",
    "                                                 weights_initializer=tf.random_uniform_initializer(),\n",
    "                                                 biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "                end_points[\"capsule_{}_act\".format(n)] = cap_1\n",
    "                capsules.append(tf.expand_dims(cap_1, axis=3))\n",
    "\n",
    "        with tf.name_scope(\"Transformation\"):\n",
    "            u = tf.concat(capsules, axis=3)\n",
    "            end_points[\"u_b_r\"] = u\n",
    "            u = tf.reshape(u, shape=[-1, 1152, 8])\n",
    "            u = tf.expand_dims(tf.expand_dims(u, axis=2), axis=2)\n",
    "            end_points[\"u\"] = u\n",
    "\n",
    "            W = tf.get_variable(shape=[1, 1152, 10, 16, 8], name=\"W_t\", \n",
    "                                initializer=tf.random_normal_initializer())\n",
    "\n",
    "\n",
    "            end_points[\"W_t\"] = W\n",
    "\n",
    "            b = tf.get_variable(shape=[1, 1152, 10, 1], name=\"b_t\", \n",
    "                                initializer=tf.zeros_initializer())\n",
    "\n",
    "            end_points[\"b_t\"] = b\n",
    "\n",
    "            u_hat =tf.add(tf.reduce_sum(tf.multiply(u, W), axis=4), b)\n",
    "            end_points[\"u_hat\"] = u_hat\n",
    "\n",
    "            b_prior = tf.get_variable(shape=[1, 1152, 10, 1], name=\"b_prior\", initializer=tf.zeros_initializer(), \n",
    "                                      trainable=False)\n",
    "        \n",
    "            \n",
    "            aggr = tf.get_variable(shape=[1, 1152, 10, 1], name=\"aggr\", initializer=tf.zeros_initializer(), \n",
    "                                      trainable=False)\n",
    "            \n",
    "           \n",
    "            end_points[\"b_prior\"] = b_prior\n",
    "\n",
    "            s, c = get_s(b_prior, u_hat)\n",
    "            end_points[\"s\"] = s\n",
    "            end_points[\"c\"] = c\n",
    "\n",
    "            v = get_v(s)\n",
    "\n",
    "            end_points[\"v\"] = v\n",
    "\n",
    "          \n",
    "\n",
    "        if is_train:\n",
    "            with tf.name_scope(\"Routing\"):\n",
    "\n",
    "                #Routing\n",
    "                batch_item_cnt = tf.Variable(initial_value=0, trainable=False)\n",
    "\n",
    "                batch_item_cnt.assign(0)\n",
    "                end_of_batch = lambda batch_item_cnt, v, b_prior, u_hat, aggr: tf.less(batch_item_cnt, batch_size)\n",
    "                if high_routing:\n",
    "                    route_op = tf.while_loop(body=update_prior_10, cond=end_of_batch, \n",
    "                                         loop_vars=[batch_item_cnt, v, b_prior, u_hat, aggr])\n",
    "                    print(\"Using routing 10\")\n",
    "                else:\n",
    "                    route_op = tf.while_loop(body=update_prior, cond=end_of_batch, \n",
    "                                         loop_vars=[batch_item_cnt, v, b_prior, u_hat, aggr])\n",
    "                    print(\"Using routing 3\")\n",
    "                    \n",
    "                end_points[\"route_op\"] = route_op\n",
    "\n",
    "                [batch_item_cnt, v, b_prior_updated, u_hat, aggr] = route_op\n",
    "                end_points[\"aggr\"] = aggr\n",
    "                \n",
    "                b_prior = tf.assign(b_prior, b_prior_updated)\n",
    "               \n",
    "                end_points[\"batch_item_cnt\"] = batch_item_cnt\n",
    "\n",
    "\n",
    "\n",
    "        s, c = get_s(b_prior, u_hat)\n",
    "        v = get_v(s)\n",
    "        end_points[\"s_routed\"] = s\n",
    "        end_points[\"c_routed\"] = c\n",
    "\n",
    "        with tf.name_scope(\"Digicaps\"):\n",
    "\n",
    "\n",
    "            end_points[\"b_prior_routed\"] = b_prior\n",
    "            end_points[\"u_hat_routed\"] = u_hat\n",
    "            end_points[\"v_routed\"] = v\n",
    "            end_points[\"s_routed\"] = s\n",
    "            end_points[\"c_routed\"] = c\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"Prepare_for_FC\"):\n",
    "            Y_one_hot_ex = tf.expand_dims(Y_one_hot, axis=1)\n",
    "            end_points[\"Y_one_hot_ex\"] = Y_one_hot_ex\n",
    "\n",
    "            v_masked = tf.squeeze(tf.matmul(Y_one_hot_ex, v))\n",
    "            end_points[\"v_masked\"] = v_masked\n",
    "            \n",
    "        with tf.name_scope(\"Fully_connected\"):\n",
    "            fc_1 = tf.contrib.layers.fully_connected(inputs=v_masked, num_outputs=512, \n",
    "                                                     activation_fn=tf.nn.relu,\n",
    "                                                     weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                     biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "            end_points[\"fc_1\"] = fc_1\n",
    "            #print(\"fc_1 shape={}\".format(fc_1.shape), end =\" \")\n",
    "\n",
    "            fc_2 = tf.contrib.layers.fully_connected(inputs=fc_1, num_outputs=1024, \n",
    "                                                     activation_fn=tf.nn.relu,\n",
    "                                                     weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                     biases_initializer=tf.zeros_initializer())\n",
    "            end_points[\"fc_2\"] = fc_2\n",
    "            #print(\"fc_2 shape={}\".format(fc_2.shape), end =\" \")\n",
    "\n",
    "            fc_out = tf.contrib.layers.fully_connected(inputs=fc_2, num_outputs=784, \n",
    "                                                     activation_fn=tf.nn.sigmoid,\n",
    "                                                     weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                     biases_initializer=tf.zeros_initializer())\n",
    "            end_points[\"fc_out\"] = fc_out\n",
    "            #print(\"fc_out shape={}\".format(fc_out.shape), end =\" \")\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #Calsulatong loss for each capsule\n",
    "        lambd_a = 0.5\n",
    "\n",
    "\n",
    "        #margin_loss =Y_one_hot_ex \n",
    "        v_norm = tf.norm(v, axis=2)\n",
    "        end_points[\"v_norm\"] = v_norm\n",
    "\n",
    "\n",
    "        margin_loss_present = tf.multiply(tf.pow(tf.maximum(0., 0.9 - v_norm), 2), Y_one_hot)\n",
    "        end_points[\"margin_loss_present\"] = margin_loss_present\n",
    "\n",
    "\n",
    "        margin_loss_not_present = lambd_a * tf.multiply(tf.pow(tf.maximum(0., v_norm - 0.1), 2), (1 - Y_one_hot))\n",
    "        end_points[\"margin_loss_not_present\"] = margin_loss_not_present\n",
    "\n",
    "\n",
    "        total_margin_loss = tf.reduce_sum(tf.add(margin_loss_present, margin_loss_not_present), axis=1)\n",
    "        end_points[\"total_margin_loss\"] = total_margin_loss\n",
    "\n",
    "        scaler = 0.0005\n",
    "        reconstruction_loss = tf.multiply(scaler, \n",
    "                                          tf.reduce_sum(\n",
    "                                              tf.squared_difference(\n",
    "                                                  tf.reshape(X, shape=[-1, 784]), fc_out), axis=1))\n",
    "        end_points[\"reconstruction_loss\"] = reconstruction_loss\n",
    "\n",
    "\n",
    "        loss = tf.reduce_mean(tf.add(total_margin_loss, reconstruction_loss))\n",
    "        end_points[\"loss\"] = loss\n",
    "        \n",
    "        predicted_labels = tf.argmax(v_norm, axis=1, output_type=tf.int32)\n",
    "        end_points[\"predicted_labels\"] = predicted_labels\n",
    "        \n",
    "        no_of_corrects = tf.reduce_sum(tf.cast(tf.equal(predicted_labels, Y), tf.int32))\n",
    "        \n",
    "        accuracy = no_of_corrects/batch_size\n",
    "        end_points[\"accuracy\"] = accuracy\n",
    "            \n",
    "        if add_summaries:\n",
    "            tf.summary.scalar(\"loss_avg\", loss)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        if is_train:\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            grads = optimizer.compute_gradients(loss)\n",
    "            end_points[\"grads\"] = grads\n",
    "            optimize = optimizer.apply_gradients(grads)\n",
    "            end_points[\"optimize\"] = optimize\n",
    "\n",
    "        if add_summaries:\n",
    "            summaries_merged = tf.summary.merge_all()\n",
    "            end_points[\"summaries_merged\"] = summaries_merged\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        end_points[\"init\"] = init\n",
    "        if print_shape:\n",
    "            for k in end_points.keys():\n",
    "                k_shape=\"NA\"\n",
    "                try:\n",
    "                    k_shape = str(end_points[k].shape)\n",
    "                except AttributeError:\n",
    "                    k_shape = type(end_points[k])\n",
    "\n",
    "                print(\"\\r {} shape={}\".format(k, k_shape))\n",
    "\n",
    "        return(graph, end_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is with routing when I disable the routing everythinbg seems fine\n",
    "- Action: try with none batch of 60 and see how b_prior chnages \n",
    "    - The problem is with the routing algorithms look at the current implementations\n",
    "      some use the stop gradient function to make it work. the problem is there lloka rpouind \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using routing 3\n",
      "Images magic no: 2051,  No of images: 60000, Image rows: 28, Image cols: 28\n",
      "Labels magic no: 2049, No of items: 60000\n",
      "----- Epoch 0 Loss:3.69437313079834 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.6716465950012207 Accuracy:0.1\n",
      "----- Epoch 0 Loss:3.671055555343628 Accuracy:0.11666666666666667\n",
      "----- Epoch 0 Loss:3.671674966812134 Accuracy:0.11666666666666667\n",
      "----- Epoch 0 Loss:3.6666672229766846 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.670855760574341 Accuracy:0.1\n",
      "----- Epoch 0 Loss:3.6665518283843994 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.671088695526123 Accuracy:0.1\n",
      "----- Epoch 0 Loss:3.6688435077667236 Accuracy:0.13333333333333333\n",
      "----- Epoch 0 Loss:3.6682963371276855 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.664651393890381 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6652302742004395 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.6676392555236816 Accuracy:0.11666666666666667\n",
      "----- Epoch 0 Loss:3.666656732559204 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6655123233795166 Accuracy:0.13333333333333333\n",
      "----- Epoch 0 Loss:3.664630651473999 Accuracy:0.1\n",
      "----- Epoch 0 Loss:3.664263963699341 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.666109800338745 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.6648457050323486 Accuracy:0.03333333333333333\n",
      "----- Epoch 0 Loss:3.6669318675994873 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6674141883850098 Accuracy:0.15\n",
      "----- Epoch 0 Loss:3.6673972606658936 Accuracy:0.15\n",
      "----- Epoch 0 Loss:3.6649277210235596 Accuracy:0.11666666666666667\n",
      "----- Epoch 0 Loss:3.6685845851898193 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6628708839416504 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.664447069168091 Accuracy:0.11666666666666667\n",
      "----- Epoch 0 Loss:3.6656413078308105 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.668006658554077 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.6652235984802246 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6658132076263428 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6653716564178467 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.66516375541687 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.6646387577056885 Accuracy:0.11666666666666667\n",
      "----- Epoch 0 Loss:3.664710760116577 Accuracy:0.13333333333333333\n",
      "----- Epoch 0 Loss:3.6668922901153564 Accuracy:0.11666666666666667\n",
      "----- Epoch 0 Loss:3.6616709232330322 Accuracy:0.11666666666666667\n",
      "----- Epoch 0 Loss:3.666585683822632 Accuracy:0.1\n",
      "----- Epoch 0 Loss:3.6654391288757324 Accuracy:0.11666666666666667\n",
      "----- Epoch 0 Loss:3.6650500297546387 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.6625635623931885 Accuracy:0.03333333333333333\n",
      "----- Epoch 0 Loss:3.664503574371338 Accuracy:0.03333333333333333\n",
      "----- Epoch 0 Loss:3.663337469100952 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.6611433029174805 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.6630778312683105 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6624224185943604 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.661163806915283 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.6643240451812744 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.6626217365264893 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.663632869720459 Accuracy:0.016666666666666666\n",
      "----- Epoch 0 Loss:3.6619009971618652 Accuracy:0.1\n",
      "----- Epoch 0 Loss:3.662262201309204 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.661151647567749 Accuracy:0.03333333333333333\n",
      "----- Epoch 0 Loss:3.662663698196411 Accuracy:0.1\n",
      "----- Epoch 0 Loss:3.663398265838623 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.6612257957458496 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6609976291656494 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.6599650382995605 Accuracy:0.11666666666666667\n",
      "----- Epoch 0 Loss:3.6612823009490967 Accuracy:0.15\n",
      "----- Epoch 0 Loss:3.6602909564971924 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6621224880218506 Accuracy:0.03333333333333333\n",
      "----- Epoch 0 Loss:3.664156436920166 Accuracy:0.1\n",
      "----- Epoch 0 Loss:3.662137746810913 Accuracy:0.03333333333333333\n",
      "----- Epoch 0 Loss:3.6642253398895264 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.660256862640381 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.6594138145446777 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6591415405273438 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.6604743003845215 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.6580705642700195 Accuracy:0.1\n",
      "----- Epoch 0 Loss:3.662339687347412 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6612160205841064 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.660135507583618 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.659106492996216 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.6584155559539795 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.65979266166687 Accuracy:0.15\n",
      "----- Epoch 0 Loss:3.659677743911743 Accuracy:0.16666666666666666\n",
      "----- Epoch 0 Loss:3.6578242778778076 Accuracy:0.016666666666666666\n",
      "----- Epoch 0 Loss:3.6581175327301025 Accuracy:0.016666666666666666\n",
      "----- Epoch 0 Loss:3.658036231994629 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.6592555046081543 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.6583175659179688 Accuracy:0.1\n",
      "----- Epoch 0 Loss:3.6602284908294678 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.661043882369995 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.660236358642578 Accuracy:0.03333333333333333\n",
      "----- Epoch 0 Loss:3.660748243331909 Accuracy:0.15\n",
      "----- Epoch 0 Loss:3.6578609943389893 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.6589012145996094 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.6594271659851074 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.657257080078125 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6584014892578125 Accuracy:0.016666666666666666\n",
      "----- Epoch 0 Loss:3.660606861114502 Accuracy:0.06666666666666667\n",
      "----- Epoch 0 Loss:3.657916307449341 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.658388376235962 Accuracy:0.1\n",
      "----- Epoch 0 Loss:3.657637119293213 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.6575307846069336 Accuracy:0.03333333333333333\n",
      "----- Epoch 0 Loss:3.657057046890259 Accuracy:0.08333333333333333\n",
      "----- Epoch 0 Loss:3.658046007156372 Accuracy:0.05\n",
      "----- Epoch 0 Loss:3.657327175140381 Accuracy:0.03333333333333333\n",
      "----- Epoch 0 Loss:3.6568655967712402 Accuracy:0.11666666666666667\n",
      "----- Epoch 0 Loss:3.6664090156555176 Accuracy:0.18333333333333332\n",
      "----- Epoch 0 Loss:3.658430337905884 Accuracy:0.15\n",
      "----- Epoch 0 Loss:3.6443934440612793 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.643954277038574 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.6433279514312744 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.641430616378784 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.6228060722351074 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.0133814811706543 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.160118818283081 Accuracy:1.0\n",
      "----- Epoch 0 Loss:0.9148995876312256 Accuracy:1.0\n",
      "----- Epoch 0 Loss:0.24956175684928894 Accuracy:1.0\n",
      "----- Epoch 0 Loss:0.020542899146676064 Accuracy:1.0\n",
      "----- Epoch 0 Loss:0.002322064945474267 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.748154894215986e-05 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.692750967980828e-06 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.1931782612227835e-07 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.6165488503361303e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.600247578892322e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.584285946876207e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.568648144711915e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.553317829916523e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.538280258728264e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.5235295691695683e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.509052971471192e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.494844603655565e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.4808894555073948e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.4671864612125773e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.4537247849943924e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.4404919923549642e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.4274906590117098e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.4147063964742301e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.4021348526682686e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.3897685668950999e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.3776014995414698e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.365632051886223e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.353853118502002e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.342254662972664e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.3308348201235276e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.3195883497019167e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.3085143635294116e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.297605045635919e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.2868543564081847e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.2762638945673643e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.265821669704792e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.2555315009876722e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.2453853059923858e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.2353803313658318e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.2255145342976448e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.2157829409886745e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.2061843079891332e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.1967156154923941e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.1873696692532576e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.1781495778961926e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.1690470813618958e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.1600628013752612e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.151195583304343e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.142440364532149e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.1337939476163683e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.1252573095532625e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.1168259206328912e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.1084959616880496e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.100269830800471e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0921431758958988e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0841120001714444e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.076178168801789e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0683359086272048e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0605860190082694e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0529269012238274e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0453559795564615e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0378684578427055e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0304709086028652e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0231532954207978e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0159181940139206e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0087630286648164e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.0016878881913271e-08 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.946906409652456e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.877670237301572e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.809197010213211e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.741472517532657e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.67446567301522e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.60815249584357e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.542542755980321e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.477613360786563e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.413358981191777e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.349792939872259e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.28685217616021e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.224568664478738e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.162891778657922e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.101861486726648e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:9.04143249158551e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.981600352342411e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.92236684535419e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.863707101625096e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.805637108366682e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.74812222662058e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.691158015494693e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.634748027702699e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.578878940568302e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.523524996917331e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.468711065745538e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.41441227805717e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.36062863385223e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.307332599599704e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.254526839834853e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.202211354557676e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.150384367411334e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.099027226649014e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:8.04812394505916e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.997690509853328e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.947721591960999e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.898186105137484e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.849093819345399e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.800434076443707e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.752218422751866e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.70440333752731e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.657032341512604e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.610046814932048e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.56347962038717e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.517313882487997e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.471554930305047e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.426186332537554e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.381205868739471e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.33657445906033e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.292335180153486e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.248482702948422e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.204976171237831e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.161837345392996e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.119051570469992e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.0766303927882745e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:7.03455693695787e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.992834755692456e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.951434539104184e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.910385152991694e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.869650182039777e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.8292527188873464e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.7891905430883526e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.749441450182303e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.710021871469962e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.670892727100863e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.632083771052066e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.593573242952289e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.555372689120986e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.517464790078975e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.47986375668097e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.442533173611764e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.4055090120973546e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.3687735085693475e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.33230623492409e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.296116517034989e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.2602079076157224e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.224574633506563e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.189210033369363e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.154114107204123e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.11926109783667e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.084702519615348e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.050385970013394e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:6.016322107171845e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.98251448380438e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.948958214929689e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.915647083298836e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.882578424376561e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.849758011322592e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.817179182798782e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.784834389288562e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.752710752204848e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.72083624916786e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.689194448876833e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.657776913636781e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.626570764860617e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.595594210205945e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.564840144245409e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.534306346532958e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.503991040711753e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.473880904105499e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.443997697085479e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.414318771101989e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.384831247567945e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.355567100906455e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.32649879758651e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.297635663481515e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.268986136286458e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.240511136150872e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.2122435256762856e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.184155771331689e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.156275850737302e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.128585556235521e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.101077782398988e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.0737574142090125e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.046624895754803e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:5.019675786144262e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.992912749912648e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.966319355759197e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.939904485468105e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.9136703594854225e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.88761431327589e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.861717251003483e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.836014699804991e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.81045869804575e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.7850883255762255e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.7598804897575064e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.73484274010616e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.7099586453214215e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.685234422652229e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.6606856152209275e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.636289574477814e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.612052961761037e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.587971780267708e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.564049582711505e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.5402694937024535e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.516655049968676e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.493189820209409e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.469875136692281e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.4467109994172915e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.423683197529726e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.4008059418843e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.378074791588915e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.3554839734838424e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.333049918869847e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.310741541502239e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.2885877071796585e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.2665635469063545e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.244677942466524e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.222931337949376e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.201309966589406e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.179836032847106e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.158488220440404e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.137279407956385e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.116189167291395e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.095238814727509e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.07441325123159e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.053716473606528e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.033149370030742e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:4.012716381396331e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.9923748751391486e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.972180806499637e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.952113747374142e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.93215682237269e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.912330459598934e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.892612898681591e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.873024567724315e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.8535512558723894e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.834196071750284e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.814950133573802e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.795819658591881e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.77680864360741e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.757907318657772e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.739125897794793e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.7204463954054745e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.7018854648351862e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.6834260086493487e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.6650775747659736e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.6468335018469134e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.628711331415957e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.610682197674464e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.5927560926296565e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.574947893270064e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.5572340628675647e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.53962459342938e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.5221228156245843e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.5047189594905603e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.4874121368488886e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.4702101192607415e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.453113350815329e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.4361102851931946e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.419205585331042e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.402391257623094e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.3856788483888067e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.369067691494365e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.3525462406203133e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.3361220452832185e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.319777786003897e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.3035396640457293e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.2873945787770253e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.2713383113502914e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.255372638122367e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.2395037763421897e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.223719069467279e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.208026955192622e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.1924141108419235e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.176895857492923e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.1614633133614234e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.146119587071894e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.1308620140890753e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.115689262145338e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.1006026635083117e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.0855926702599845e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.0706686082737633e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.0558260366575496e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.0410731710617256e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.0263980210776253e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:3.0118083582664212e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.997290415862608e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.9828557401856415e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.9685112146182746e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.9542330803877803e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.9400371026611083e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.9259146216986665e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.911877183819911e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.8979079136348673e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.8840259069795593e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.870212512107173e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.8564774989803254e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.8428117637702144e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.829227518930111e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.8157161047204227e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.8022677511785332e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.7889055509433547e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.7756135168033325e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.7623827669742695e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.749233285470609e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.7361528598390805e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.7231394916782392e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.7102009525492576e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.6973330236046422e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.6845290435062452e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.671796561770634e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.6591300272826857e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.646532548666869e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.6340019054771346e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.621540762248742e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.609139349019074e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.596812542776661e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.5845452444883676e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.5723425611801076e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.5602076014763497e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.5481392551540694e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.536132859276563e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.5241844170409422e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.512305474766663e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.5004864845357133e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.488731443150982e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.477033023140507e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.4654056574036076e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.4538333587287298e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.442319901874157e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.4308712820442224e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.4194761749640747e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.4081450167301455e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.3968762530302e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.3856581155001777e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.3744992638796703e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.3634063595068255e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.35236030654562e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.3413855299025954e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.330460713295679e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.3195885212601297e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.308774504911071e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.2980179981146875e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.287318334737165e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.27667129593101e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.266076215562407e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.255534647943591e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.2450541425911297e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.234624485453196e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.22424656470821e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.2139234889806403e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.2036545921366724e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.193434767150393e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.1832671226462708e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.1731525468027257e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.163090817575153e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.153077049982244e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.1431147967376774e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.133205612153688e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.123345721471992e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.1135353467371942e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.1037724895478505e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.0940613687514542e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.0843995418573513e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.0747845663748876e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.065220217062347e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.0557047175628895e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.046233849029022e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.036815383021917e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.027438883445143e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.0181192272872295e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:2.0088393171135976e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.999608922886864e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.9904193848674367e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.9812795848395126e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.9721870803124375e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.9631378744833228e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.954132633485983e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.945179350926196e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.9362627057262216e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.927393578071701e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.9185721900072394e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.9097907699716643e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.901054647035494e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.892361156663469e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.8837109649894046e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.875102739745671e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.866537591155293e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.8580177396643194e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.8495366349569053e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.8411032698395502e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.8327064310597052e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.8243506705317714e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.8160388748356127e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.8077632724100567e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.799530191526344e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.791338855028357e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.7831891518937937e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.7750766412305552e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.7670067631314623e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.758979517596515e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.7509875771537509e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.7430359378067806e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.7351198255965983e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.727246679017469e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.7194106138873622e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.7116117412285803e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.7038507271749381e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.6961342330645834e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.6884459386190542e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.6807997216261583e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.6731891427923529e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.665616644608292e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.6580794515164143e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.6505813382750034e-09 Accuracy:1.0\n",
      "----- Epoch 0 Loss:1.6431208615941273e-09 Accuracy:1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-317984b5267a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                         \u001b[0mend_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                                         \u001b[0mend_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"c_routed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                                         end_points[\"summaries_merged\"]], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mloss_report_counter\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawnr/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawnr/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawnr/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawnr/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shawnr/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 60\n",
    "graph, end_points = build_graph(batch_size, is_train=True, add_summaries=True, print_shape=False)\n",
    "n_epochs = 10\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(end_points[\"init\"])\n",
    "    saver = tf.summary.FileWriter(\"..//log\", graph=graph)\n",
    "    \n",
    "    other_variables = [v for v in graph.get_collection(\"variables\") if v.name=='b_prior:0']\n",
    "    trainable_variables = graph.get_collection(\"trainable_variables\")\n",
    "    variables_to_restore = trainable_variables.extend(other_variables)\n",
    "    \n",
    "    model_saver = tf.train.Saver(variables_to_restore)\n",
    "   \n",
    "    loss_report_counter = 0\n",
    "    nan_caps=list()\n",
    "    c_list=list()\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        mnist = Mnist(path=\"..//\", train=True)\n",
    "        data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)  \n",
    "        loss_report_counter = 0\n",
    "        \n",
    "    \n",
    "     \n",
    "        while data.shape[0]==60:\n",
    "       \n",
    "      \n",
    "            data = np.expand_dims(data, -1)\n",
    "            feed_dict = {end_points[\"X\"]:data, end_points[\"Y\"]:lbls}\n",
    "            _, loss, grads, u_hat, accuracy, c_aft, summaries_merged = sess.run([end_points[\"optimize\"], \n",
    "                                                        end_points[\"loss\"],\n",
    "                                                        end_points[\"grads\"],\n",
    "                                                        end_points[\"u_hat\"],\n",
    "                                                        end_points[\"accuracy\"],\n",
    "                                                        end_points[\"c_routed\"],\n",
    "                                                        end_points[\"summaries_merged\"]], feed_dict=feed_dict)\n",
    "\n",
    "            loss_report_counter +=1\n",
    "\n",
    "            if(loss_report_counter%10==0):\n",
    "                print(\"----- Epoch {} Loss:{} Accuracy:{}\".format(epoch, loss, accuracy))\n",
    "                \n",
    "                \n",
    "                if np.any(np.isnan(grads[0][0])==True):\n",
    "                    print(\"Nan detected\")\n",
    "                 \n",
    "                    break;\n",
    "                saver.add_summary(summaries_merged, global_step=loss_report_counter)\n",
    "            data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True) \n",
    "        \n",
    "                \n",
    "\n",
    "   \n",
    "    \n",
    "    print(\"Model saves under path {}\".format(model_saver.save(sess,save_path=\"..//saved_model//capsnet\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the structure based on mnist with batch size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "graph, end_points = build_graph(batch_size, is_train=True, high_routing=True, add_summaries=False, print_shape=False)\n",
    "\n",
    "mnist = Mnist(path=\"..//\", train=True)\n",
    "#with scale there is a problem beacuse weights are still low and outputs gets small\n",
    "#for testing we simply override scaling to test the functionality\n",
    "\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)    \n",
    "data = np.expand_dims(data, -1)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "   \n",
    "    init = end_points[\"init\"]\n",
    "    X = end_points[\"X\"]\n",
    "    Y = end_points[\"Y\"]\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    feed_dict = {X:data, Y:lbls}\n",
    "\n",
    "    \n",
    "    \n",
    "    #\"Test output of conv 1\"\n",
    "    x, w, b, conv_1_act =  sess.run([X, graph.get_tensor_by_name('Conv/weights:0'), \n",
    "                                     graph.get_tensor_by_name('Conv/biases:0'), \n",
    "                                     end_points[\"conv_layer_1_act\"]], \n",
    "                                     feed_dict=feed_dict)\n",
    "    \n",
    "    assert(np.max([0, np.sum((np.matmul(np.expand_dims(x[0, 0:9, 0:9], axis=3), w) + b)[:,:,:,0])]) - \n",
    "           conv_1_act[0, 0, 0, 0]<0.0001)\n",
    "    assert(conv_1_act.shape == (2, 20, 20, 256))\n",
    "    \n",
    "    #Test for capsule out values\"\n",
    "    #Capsule 8\n",
    "    \n",
    "    w, b, conv_1_act, capsule_7_act, capsule_2_act =  sess.run([graph.get_tensor_by_name('Capsule_Layer/Conv_7/weights:0'), \n",
    "                                     graph.get_tensor_by_name('Capsule_Layer/Conv_7/biases:0'), \n",
    "                                                 end_points[\"conv_layer_1_act\"],\n",
    "                                                 end_points[\"capsule_7_act\"],\n",
    "                                                 end_points[\"capsule_2_act\"]],\n",
    "                                                 feed_dict=feed_dict)\n",
    "    \n",
    "    assert(np.linalg.norm(np.sum((np.matmul(np.expand_dims(conv_1_act[0, 0:9, 0:9], axis=2), w) + b), axis=(0, 1, 2)) - \n",
    "           capsule_7_act[0, 0, 0, :])<0.00005)\n",
    "    \n",
    "    assert(capsule_7_act.shape==(2, 6, 6, 8))\n",
    "    \n",
    "    #Test for not sharing the weights\n",
    "    assert(np.linalg.norm(capsule_7_act[0, 0, 0, :] - capsule_2_act[0, 0, 0, :])>0.01)\n",
    "    \n",
    "    #Test the squash function before routing\n",
    "    v, s, b_prior, W_t, b_t, capsule_7_act, u, u_hat = sess.run([end_points[\"v\"], \n",
    "                                                                 end_points[\"s\"],\n",
    "                                                                 end_points[\"b_prior\"],\n",
    "                                                                 end_points[\"W_t\"],\n",
    "                                                                 end_points[\"b_t\"],\n",
    "                                                                 end_points[\"capsule_7_act\"],\n",
    "                                                                 end_points[\"u\"],\n",
    "                                                                 end_points[\"u_hat\"]], feed_dict=feed_dict)\n",
    "    \n",
    "    #Test for the first one digit 5\n",
    "    u = np.squeeze(u[0,252:288,...], axis=1)\n",
    "    W_t = np.squeeze(W_t[:, 252:288, 5, ...])\n",
    "    b_t = b_t[:, 252:288, 5, ...]\n",
    "    u_hat_7_5 = np.add(np.sum(np.multiply(u, W_t), axis=2), b_t)\n",
    "    \n",
    "    assert(np.linalg.norm(u_hat_7_5 - u_hat[0,252:288, 5,...])<0.0001)\n",
    "    b_prior_all_10= np.squeeze(b_prior[0,:,...])\n",
    "    b_prior_7_5 =np.squeeze(b_prior[0,:, 5,...])\n",
    "    c_7_5 = np.exp(b_prior_7_5)/np.sum(np.exp(b_prior_all_10), axis=1)\n",
    "    c_7_5 = np.expand_dims(c_7_5, axis=1)\n",
    "    u_hat_5 =  np.squeeze(u_hat[0, :, 5,...])\n",
    "    s_5 = np.sum(np.multiply(c_7_5, u_hat_5), axis=0)\n",
    "    \n",
    "    #Testing the vector for element 5\n",
    "    assert(np.linalg.norm(s_5 - s[0, 5,...])<0.00001)\n",
    "    \n",
    "    #test squash\n",
    "    v_5 = (np.power(np.linalg.norm(s_5), 2)/(1+np.power(np.linalg.norm(s_5), 2)))*(s_5/np.linalg.norm(s_5))\n",
    "    assert(np.linalg.norm(v_5) - np.linalg.norm(v[0, 5, ...])<0.00001)\n",
    "  \n",
    "    #Test routing\n",
    "    \n",
    "    _, v_before, s_before, c_before, v_routed,s_routed, \\\n",
    "    c_routed, u_hat_routed, b_prior, b_prior_routed, batch_item_count = sess.run([\n",
    "                                     end_points[\"route_op\"],\n",
    "                                     end_points[\"v\"],\n",
    "                                     end_points[\"s\"],\n",
    "                                     end_points[\"c\"],\n",
    "                                     end_points[\"v_routed\"],\n",
    "                                     end_points[\"s_routed\"],\n",
    "                                     end_points[\"c_routed\"],\n",
    "                                     end_points[\"u_hat_routed\"],\n",
    "                                     end_points[\"b_prior\"],\n",
    "                                     end_points[\"b_prior_routed\"],\n",
    "                                     end_points[\"batch_item_cnt\"]], feed_dict=feed_dict)\n",
    "  \n",
    "    assert(np.all(u_hat_routed==u_hat))\n",
    "    \n",
    "    ###############################################################################################\n",
    "    # Here we are measuring the agreements before and after applying the routing algorithms and   #\n",
    "    # we are testing that the expected value for each class in fact increased meaning that        #\n",
    "    # the routing algorithm helped to route them to better digiCaps                               #\n",
    "    ###############################################################################################\n",
    "    \n",
    "    aggr_before, aggr_after = np.sum(np.multiply(v_before[0,...], u_hat[0,...]), axis=2), \\\n",
    "        np.sum(np.multiply(v_routed[0,...], u_hat[0,...]), axis=2)\n",
    "  \n",
    "    assert(np.min(aggr_after.mean(axis=0) - aggr_before.mean(axis=0))>0)\n",
    "    \n",
    "    #Test the losses\n",
    "    \n",
    "    X, v_out, margin_loss_present, margin_loss_not_present, total_margin_loss, fc_out, reconstruction_loss, loss, v_norm =\\\n",
    "                                                            sess.run([ end_points[k] \n",
    "                                                              for k in [\"X\", \"v_routed\", \n",
    "                                                                        \"margin_loss_present\",\n",
    "                                                                        \"margin_loss_not_present\",\n",
    "                                                                        'total_margin_loss', \n",
    "                                                                        \"fc_out\",\n",
    "                                                                        'reconstruction_loss', \n",
    "                                                                        'loss', \"v_norm\"]], feed_dict=feed_dict)\n",
    "    #print(v_out)\n",
    "    \n",
    "    assert(np.all(np.where(margin_loss_present != 0.)[1] == np.array([5, 0])))\n",
    "    \n",
    "    #assert(np.all(np.where(margin_loss_not_present != 0.)[1] == np.array([0, 1, 2, 3, 4,6,7,8,9,1,2,3,4,5,6,7,8,9])))\n",
    "    \n",
    "    assert(margin_loss_present[0][5] - np.power(np.max([0, 0.9 - np.linalg.norm(v_out[0][5])]), 2)<0.00001)\n",
    "    \n",
    "    assert(np.linalg.norm(margin_loss_not_present[0][0] - \n",
    "                          0.5*np.power(np.max([0., np.linalg.norm(v_out[0][0])-0.1]), 2))<0.00001)\n",
    "    \n",
    "    assert(np.linalg.norm(total_margin_loss[0] - np.sum(margin_loss_present[0]+ margin_loss_not_present[0]))<0.00001)\n",
    "    \n",
    "    assert(np.abs(np.sum(np.square(fc_out[0] - X[0].reshape(784,)))*0.0005- reconstruction_loss[0])<0.01)\n",
    "    \n",
    "    assert(loss == np.mean(np.add(reconstruction_loss, total_margin_loss)))\n",
    "    print(\"All done\")\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the capsule network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper mentiones shifting images by two pixels but it is not done here<br/>\n",
    "I also did not add noise for reconstruction make sure the essentials work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "tf.reset_default_graph()\n",
    "graph, end_points = build_graph(batch_size, is_train=False, add_summaries=False, print_shape=True)\n",
    "\n",
    "mnist = Mnist(path=\"..\\\\\", train=True)\n",
    "#with scale there is a problem beacuse weights are still low and outputs gets small\n",
    "#for testing we simply override scaling to test the functionality\n",
    "\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)    \n",
    "data = np.expand_dims(data, -1)\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    restorer = tf.train.Saver()\n",
    "    restorer.restore(sess, save_path=\"..\\\\saved_model\\\\capsnet\")\n",
    "    feed_dict = {end_points[\"X\"]:data, end_points[\"Y\"]:lbls}\n",
    "    print(sess.run(end_points[\"b_prior\"], feed_dict=feed_dict))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 110.,  111.,  128.,  105.,  123.,  117.,  102.,  100.,  130.,  126.], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_aft.squeeze().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.all(np.isnan(o)==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idxs = np.where(np.isnan(nan_caps[3]) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1152, 10, 1)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_prior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margin_loss_present.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.linalg.norm(u_hat.mean(axis=0).mean(axis=0), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
