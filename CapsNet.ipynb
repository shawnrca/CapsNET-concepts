{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mnist:\n",
    "    def __init__(self, path, train=True):\n",
    "        if train:\n",
    "            image_file_name = \"train-images.idx3-ubyte\"\n",
    "            label_file_name = \"train-labels.idx1-ubyte\"\n",
    "        else:\n",
    "            image_file_name = \"t10k-images.idx3-ubyte\"\n",
    "            label_file_name = \"t10k-labels.idx1-ubyte\"\n",
    "            \n",
    "        self.f = open(\"{}\\\\{}\".format(path, image_file_name), \"rb\")\n",
    "        magic_number, number_of_images, image_rows, image_columns = int(self.f.read(4).hex(), 16), \\\n",
    "        int(self.f.read(4).hex(), 16), int(self.f.read(4).hex(), 16), int(self.f.read(4).hex(), 16)\n",
    "        print(\"Images magic no: {},  No of images: {}, Image rows: {}, Image cols: {}\".\\\n",
    "          format(magic_number, number_of_images, image_rows, image_columns))\n",
    "\n",
    "        self.flab = open(\"{}\\\\{}\".format(path, label_file_name), \"rb\")\n",
    "        magic_number, number_of_items = int(self.flab.read(4).hex(), 16), int(self.flab.read(4).hex(), 16)\n",
    "        print(\"Labels magic no: {}, No of items: {}\".format(magic_number, number_of_items))\n",
    "\n",
    "    def _get_next(self):\n",
    "        val = self.f.read(28*28)\n",
    "        if len(val)>0:\n",
    "            img = val.hex()\n",
    "            idxs = np.arange(0, len(img), 2).astype(int)\n",
    "            return(np.reshape([int(img[i:(i+2)], 16) for i in idxs], (28, 28)), int(self.flab.read(1).hex(), 16)) \n",
    "        else:\n",
    "            return(\"\", \"\")\n",
    "\n",
    "    def get_batch(self, batch_size=20):\n",
    "        im_lst = np.zeros(shape=[batch_size, 28, 28], dtype=\"float\")\n",
    "        lbl_lst = np.zeros(shape=[batch_size], dtype=\"int\")\n",
    "        lbl_one_hot_lst = np.zeros(shape=[batch_size, 10], dtype=\"int\")\n",
    "        for i in range(batch_size):\n",
    "            im, lbl = self._get_next()\n",
    "            if len(im)>0:\n",
    "                im_lst[i,...] = im\n",
    "                lbl_lst[i] = lbl\n",
    "                lbl_one_hot_lst[i,lbl] = 1\n",
    "        return im_lst, lbl_lst, lbl_one_hot_lst\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_one_hot shape=(2, 10)\n",
      "\n",
      "Conv_1 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_0 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_1 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_2 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_3 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_4 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_5 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_6 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_7 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_8 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_9 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_10 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_11 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_12 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_13 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_14 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_15 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_16 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_17 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_18 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_19 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_20 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_21 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_22 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_23 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_24 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_25 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_26 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_27 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_28 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_29 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_30 shape=(2, 20, 20, 256)\n",
      "\n",
      "Cap_31 shape=(2, 20, 20, 256)\n",
      "\n",
      "Caps_all shape=(2, 1152, 1, 8)\n",
      "\n",
      "u_hat shape=(2, 1152, 10, 16)\n",
      "\n",
      "v shape (2, 10, 16)\n",
      "v shape (2, 10, 16)\n",
      "v_out shape=(2, 10, 16)\n",
      "\n",
      "v_out_masked shape=(2, 16)\n",
      "\n",
      "fc_1 shape=(2, 512)\n",
      "\n",
      "fc_2 shape=(2, 1024)\n",
      "\n",
      "fc_out shape=(2, 784)\n",
      "\n",
      "v_out_norm shape=(2, 10)\n",
      "\n",
      "margin_loss_present shape=(2, 10)\n",
      "\n",
      "margin_loss_not_present shape=(2, 10)\n",
      "\n",
      "total_margin_loss shape=(2, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_s(b_prior, u_hat):\n",
    "    c = tf.nn.softmax(b_prior, dim=-1)\n",
    "    s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1)\n",
    "    return(s)\n",
    "\n",
    "\n",
    "#Squash\n",
    "def get_v(s):\n",
    "    norm_s = tf.norm(s)\n",
    "    norm_s_2 = tf.pow(norm_s, 2)\n",
    "    v = (norm_s_2*s)/((1+norm_s_2)*norm_s)\n",
    "    print(\"v shape {}\".format(v.shape))\n",
    "    return(v)\n",
    "\n",
    "def update_prior(batch_item_cnt, b_prior, u_hat):\n",
    "\n",
    "    for i in range(3):\n",
    "        aggr = tf.reduce_sum(tf.multiply(v[batch_item_cnt, :, :], u_hat[batch_item_cnt, :, :]), axis=-1)\n",
    "        aggr = tf.expand_dims(aggr, axis=0)\n",
    "        aggr = tf.expand_dims(aggr, axis=3)\n",
    "        b_prior = tf.add(b_prior, aggr)\n",
    "\n",
    "    return([tf.add(batch_item_cnt, 1), b_prior, u_hat])\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "batch_size=2\n",
    "end_points = dict()\n",
    "\n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(shape=[batch_size, 28, 28, 1], name=\"X\", dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=[batch_size], name=\"Y\", dtype=tf.int32)\n",
    "    Y_one_hot = tf.one_hot(indices=Y, depth=10)\n",
    "    print(\"Y_one_hot shape={}\\n\".format(Y_one_hot.shape))\n",
    "\n",
    "    \n",
    "    #First Layer\n",
    "    with tf.name_scope(\"Conv_Layer_1\"):\n",
    "      \n",
    "        \n",
    "        conv_1 = tf.contrib.layers.conv2d(inputs=X, \n",
    "                                          num_outputs=256,  \n",
    "                                          kernel_size=9, \n",
    "                                          stride=1, padding=\"VALID\", \n",
    "                                          activation_fn=tf.nn.relu, \n",
    "                                          weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                          biases_initializer=tf.zeros_initializer())\n",
    "        end_points[\"conv_layer_1_act\"] = conv_1\n",
    "        print(\"Conv_1 shape={}\\n\".format(conv_1.shape))\n",
    "    capsules = list()\n",
    "    \n",
    "    #In this implementation every capsule layer has unique weights\n",
    "    #I think the meaning of sharing is that in each plane [6, 6, 1]\n",
    "    #each cepsule output sharing the weights\n",
    "    with tf.variable_scope(\"Capsule_Layer\"):\n",
    "        for n in range(32):\n",
    "         \n",
    "           #If we user auto resuse then the output of each capsule would be the same and it doesn't make any sense\n",
    "        \n",
    "            cap_1 = tf.contrib.layers.conv2d(inputs=conv_1, \n",
    "                                             #reuse=tf.AUTO_REUSE, \n",
    "                                             #scope=\"Capsule_Layer\",\n",
    "                                             num_outputs=8, kernel_size=9, \n",
    "                                             stride=2, padding=\"VALID\", \n",
    "                                             activation_fn=None, \n",
    "                                             weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                             biases_initializer=tf.zeros_initializer())\n",
    "            print(\"Cap_{} shape={}\\n\".format(n, conv_1.shape))\n",
    "            end_points[\"capsule_{}_act\".format(n)] = cap_1\n",
    "         \n",
    "            capsules.append(tf.expand_dims(cap_1, axis=3))\n",
    "     \n",
    "    with tf.name_scope(\"DigitalCaps\"):\n",
    "        caps_all = tf.concat(capsules, axis=3)\n",
    "        end_points[\"caps_all\"] = caps_all\n",
    "        \n",
    "        caps_all = tf.reshape(caps_all, shape=[-1, 1152, 8])\n",
    "        caps_all = tf.expand_dims(caps_all, axis=2)\n",
    "        end_points[\"caps_all\"] = caps_all\n",
    "        print(\"Caps_all shape={}\\n\".format(caps_all.shape))\n",
    "\n",
    "        W = tf.get_variable(shape=[10, 1152, 8, 16], name=\"W\")\n",
    "        end_points[\"W_t\"] = W\n",
    "        \n",
    "        b = tf.get_variable(shape=[10, 1152, 1, 1], name=\"b\", initializer=tf.zeros_initializer())\n",
    "        end_points[\"b\"] = b\n",
    "        \n",
    "        b_prior = tf.get_variable(shape=[1, 1152, 10, 1], name=\"b_prior\", initializer=tf.zeros_initializer(), trainable=False)\n",
    "        end_points[\"b_prior\"] = b_prior\n",
    "        \n",
    "        caps_all_list = list()\n",
    "    \n",
    "        for cl in range(10):\n",
    "            caps_all_list.append(tf.map_fn(fn=lambda x: tf.add(tf.matmul(x, W[cl,:,:,:]), b[cl,:,:,:]), elems=caps_all))\n",
    "\n",
    "        u_hat = tf.concat(caps_all_list, axis=2)\n",
    "        end_points[\"u_hat\"] = u_hat\n",
    "        print(\"u_hat shape={}\\n\".format(u_hat.shape))\n",
    "        \n",
    "    s = get_s(b_prior, u_hat)\n",
    "    v = get_v(s)\n",
    " \n",
    "    with tf.name_scope(\"Routing\"):\n",
    "        \n",
    "        #Routing\n",
    "        batch_item_cnt = tf.constant(0)\n",
    "        end_of_batch = lambda batch_item_cnt, b_prior, u_hat: tf.less(batch_item_cnt, batch_size)\n",
    "        route_op = tf.while_loop(body=update_prior, cond=end_of_batch, loop_vars=[batch_item_cnt, b_prior, u_hat])\n",
    "        \n",
    "    with tf.control_dependencies(route_op):\n",
    "        v_out = get_v(get_s(b_prior, u_hat))\n",
    "        end_points[\"v_out\"] = v_out\n",
    "        print(\"v_out shape={}\\n\".format(v_out.shape))\n",
    "    \n",
    "    with tf.name_scope(\"Prepare_for_FC\"):\n",
    "        Y_one_hot_ex = tf.expand_dims(Y_one_hot, axis=1)\n",
    "        end_points[\"Y_one_hot_ex\"] = Y_one_hot_ex\n",
    "        \n",
    "        v_out_masked = tf.squeeze(tf.matmul(Y_one_hot_ex, v_out))\n",
    "        end_points[\"v_out_masked\"] = v_out_masked\n",
    "        print(\"v_out_masked shape={}\\n\".format(v_out_masked.shape))\n",
    "    \n",
    "    with tf.name_scope(\"Fully_connected\"):\n",
    "        fc_1 = tf.contrib.layers.fully_connected(inputs=v_out_masked, num_outputs=512, \n",
    "                                                 activation_fn=tf.nn.relu,\n",
    "                                                 weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                 biases_initializer=tf.zeros_initializer())\n",
    "        \n",
    "        end_points[\"fc_1\"] = fc_1\n",
    "        print(\"fc_1 shape={}\\n\".format(fc_1.shape))\n",
    "        \n",
    "        fc_2 = tf.contrib.layers.fully_connected(inputs=v_out_masked, num_outputs=1024, \n",
    "                                                 activation_fn=tf.nn.relu,\n",
    "                                                 weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                 biases_initializer=tf.zeros_initializer())\n",
    "        end_points[\"fc_2\"] = fc_2\n",
    "        print(\"fc_2 shape={}\\n\".format(fc_2.shape))\n",
    "        \n",
    "        fc_out = tf.contrib.layers.fully_connected(inputs=v_out_masked, num_outputs=784, \n",
    "                                                 activation_fn=tf.nn.sigmoid,\n",
    "                                                 weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                 biases_initializer=tf.zeros_initializer())\n",
    "        end_points[\"fc_out\"] = fc_out\n",
    "        print(\"fc_out shape={}\\n\".format(fc_out.shape))\n",
    "        \n",
    "        \n",
    "        #Calsulatong loss for each capsule\n",
    "        lambd_a = 0.5\n",
    "        #margin_loss =Y_one_hot_ex \n",
    "        v_out_norm = tf.norm(v_out, axis=2)\n",
    "        end_points[\"v_out_norm\"] = v_out_norm\n",
    "        print(\"v_out_norm shape={}\\n\".format(v_out_norm.shape))\n",
    "        \n",
    "        margin_loss_present = tf.multiply(tf.pow(tf.maximum(0., 0.9 - v_out_norm), 2), Y_one_hot)\n",
    "        end_points[\"margin_loss_present\"] = margin_loss_present\n",
    "        print(\"margin_loss_present shape={}\\n\".format(margin_loss_present.shape))\n",
    "        \n",
    "        margin_loss_not_present = lambd_a * tf.multiply(tf.pow(tf.maximum(0., v_out_norm - 0.1), 2), (1 - Y_one_hot))\n",
    "        end_points[\"margin_loss_not_present\"] = margin_loss_not_present\n",
    "        print(\"margin_loss_not_present shape={}\\n\".format(margin_loss_not_present.shape))\n",
    "        \n",
    "        total_margin_loss = tf.add(margin_loss_present, margin_loss_not_present)\n",
    "        end_points[\"total_margin_loss\"] = total_margin_loss\n",
    "        print(\"total_margin_loss shape={}\\n\".format(total_margin_loss.shape))\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lk = Tk max(0;m+ 􀀀 jjvkjj)2 + \u0015 (1 􀀀 Tk) max(0; jjvkjj 􀀀 m􀀀)2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images magic no: 2051,  No of images: 60000, Image rows: 28, Image cols: 28\n",
      "Labels magic no: 2049, No of items: 60000\n"
     ]
    }
   ],
   "source": [
    "mnist = Mnist(path=\".\\\\\", train=True)\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size)    \n",
    "data = np.expand_dims(data, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.4468953 ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.49729478,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]], dtype=float32), array([[ 0.00904059,  0.00757149,  0.00328853,  0.00535245,  0.00924961,\n",
      "         0.        ,  0.00210414,  0.01467124,  0.0071143 ,  0.00482973],\n",
      "       [ 0.        ,  0.00775989,  0.00282875,  0.00294855,  0.0170508 ,\n",
      "         0.01836263,  0.00634866,  0.0115443 ,  0.00478459,  0.01005425]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "   # saver = tf.summary.FileWriter(filename_suffix=\"capsNet\", logdir=\"log\", graph=graph)\n",
    "   # saver.flush()\n",
    "    sess.run(init)\n",
    "\n",
    "    feed_dict = {X:data, Y:lbls}\n",
    "    before = datetime.now()\n",
    "    print(sess.run([margin_loss_present, margin_loss_not_present], feed_dict=feed_dict))\n",
    "    #cap_1, cap_2 = sess.run([end_points['capsule_0_act'], end_points['capsule_1_act']], feed_dict = feed_dict)\n",
    "    \n",
    "    #assert(np.all(cap_1==cap_2))\n",
    "    #print(\"Time {}\\n{}\".format((datetime.now() - before).total_seconds(), rt.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conv_layer_1_act', 'capsule_0_act', 'capsule_1_act', 'capsule_2_act', 'capsule_3_act', 'capsule_4_act', 'capsule_5_act', 'capsule_6_act', 'capsule_7_act', 'capsule_8_act', 'capsule_9_act', 'capsule_10_act', 'capsule_11_act', 'capsule_12_act', 'capsule_13_act', 'capsule_14_act', 'capsule_15_act', 'capsule_16_act', 'capsule_17_act', 'capsule_18_act', 'capsule_19_act', 'capsule_20_act', 'capsule_21_act', 'capsule_22_act', 'capsule_23_act', 'capsule_24_act', 'capsule_25_act', 'capsule_26_act', 'capsule_27_act', 'capsule_28_act', 'capsule_29_act', 'capsule_30_act', 'capsule_31_act', 'caps_all', 'W_t', 'b', 'b_prior', 'u_hat', 'v_out', 'Y_one_hot_ex', 'v_out_masked', 'fc_1', 'fc_2', 'fc_out', 'v_out_norm', 'margin_loss_present', 'margin_loss_not_present', 'total_margin_loss'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_points.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
