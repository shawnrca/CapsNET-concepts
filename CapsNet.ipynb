{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from mnist import Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_s(b_prior, u_hat):\n",
    "    c = tf.nn.softmax(b_prior, dim=2)\n",
    "    s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1)\n",
    "    return(s, c)\n",
    "\n",
    "#Squash\n",
    "def get_v(s):\n",
    "    norm_s = tf.norm(s, axis=2, keep_dims=True)\n",
    "    norm_s_2 = tf.pow(norm_s, 2)\n",
    "    v = (tf.multiply(norm_s_2,s))/(tf.multiply(1+norm_s_2, norm_s))\n",
    "    return(v)\n",
    "\n",
    "def route(n_iter, batch_item_cnt, v, b_prior, u_hat_stopped, aggr, enable_b_prior_update):\n",
    "    for i in range(n_iter):\n",
    "        with tf.name_scope(\"Iteration_{}\".format(i)):\n",
    "            s, c = get_s(b_prior, u_hat_stopped)\n",
    "            v = get_v(s)\n",
    "            aggr = tf.reduce_sum(tf.multiply(v[batch_item_cnt, :, :], u_hat_stopped[batch_item_cnt, :, :]),\n",
    "                                 axis=-1, keep_dims=True)\n",
    "            aggr = tf.expand_dims(aggr, axis=0)\n",
    "            aggr = tf.multiply(aggr, enable_b_prior_update)\n",
    "            b_prior = tf.add(b_prior, aggr)\n",
    "            \n",
    "    return (v, b_prior, aggr)\n",
    "\n",
    "def update_prior(batch_item_cnt, v, b_prior, u_hat_stopped, aggr, enable_b_prior_update):\n",
    "    v, b_prior, aggr = route(2, batch_item_cnt, v, b_prior, u_hat_stopped, aggr, enable_b_prior_update)\n",
    "    return([tf.add(batch_item_cnt, 1), v, b_prior, u_hat_stopped, aggr, enable_b_prior_update])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CapsNet architecture\n",
    "32 layer Capsules each with (6, 6) 8D vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_graph(batch_size, is_train=True, add_summaries=False, print_shape=False):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    end_points = dict()\n",
    "\n",
    "    with graph.as_default():\n",
    "      \n",
    "\n",
    "        X = tf.placeholder(shape=[batch_size, 28, 28, 1], name=\"X\", dtype=tf.float32)\n",
    "        end_points[\"X\"] = X\n",
    "        if is_train:\n",
    "            Y = tf.placeholder(shape=[batch_size], name=\"Y\", dtype=tf.int32)\n",
    "            end_points[\"Y\"] = Y\n",
    "            Y_one_hot = tf.one_hot(indices=Y, depth=10)\n",
    "            end_points[\"Y_one_hot\"] = Y_one_hot\n",
    "        \n",
    "            enable_b_prior_update = tf.placeholder(shape = [], dtype=tf.float32)\n",
    "            end_points[\"enable_b_prior_update\"] = enable_b_prior_update\n",
    "        \n",
    "   \n",
    "\n",
    "        #First Layer\n",
    "        with tf.name_scope(\"Conv_Layer_1\"):\n",
    "\n",
    "            conv_1 = tf.contrib.layers.conv2d(inputs=X, \n",
    "                                              num_outputs=256,  \n",
    "                                              kernel_size=9, \n",
    "                                              stride=1, padding=\"VALID\", \n",
    "                                              activation_fn=tf.nn.relu, \n",
    "                                              weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(), \n",
    "                                              biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "            end_points[\"conv_layer_1_act\"] = conv_1\n",
    "\n",
    "        capsules = list()\n",
    "        with tf.variable_scope(\"Capsule_Layer\"):\n",
    "            for n in range(32):\n",
    "                cap_1 = tf.contrib.layers.conv2d(inputs=conv_1, \n",
    "                                                 num_outputs=8, kernel_size=9, \n",
    "                                                 activation_fn=None,\n",
    "                                                 stride=2, padding=\"VALID\" ,\n",
    "                                                 weights_initializer=tf.random_uniform_initializer(minval=-0.1, \n",
    "                                                                                                   maxval=0.1),\n",
    "                                                 biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "                end_points[\"capsule_{}_act\".format(n)] = cap_1\n",
    "                capsules.append(tf.expand_dims(cap_1, axis=3))\n",
    "\n",
    "        with tf.name_scope(\"Transformation\"):\n",
    "            u = tf.concat(capsules, axis=3)\n",
    "            end_points[\"u_b_r\"] = u\n",
    "\n",
    "            u = tf.reshape(u, shape=[-1, 1152, 8])\n",
    "            u = tf.expand_dims(tf.expand_dims(u, axis=2), axis=2)\n",
    "            end_points[\"u\"] = u\n",
    "\n",
    "            W = tf.get_variable(shape=[1, 1152, 10, 16, 8], name=\"W_t\", \n",
    "                                initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "            end_points[\"W_t\"] = W\n",
    "           \n",
    "            u_hat =tf.reduce_sum(tf.multiply(u, W), axis=4)\n",
    "            end_points[\"u_hat\"] = u_hat\n",
    "\n",
    "            b_prior= tf.get_variable(shape=[1, 1152, 10, 1], name=\"b_prior\", initializer=tf.zeros_initializer(), \n",
    "                                      trainable=False)\n",
    "            end_points[\"b_prior\"] = b_prior\n",
    "           \n",
    "            aggr = tf.constant(value=0, shape=[1, 1152, 10, 1], dtype=tf.float32, name=\"aggr\")\n",
    "            \n",
    "            s, c = get_s(b_prior, u_hat)\n",
    "            end_points[\"s\"],  end_points[\"c\"] = s, c\n",
    "            \n",
    "            v = get_v(s)\n",
    "            end_points[\"v\"] = v\n",
    "\n",
    "        if is_train:\n",
    "            with tf.name_scope(\"Routing\"):\n",
    "\n",
    "                #Routing\n",
    "                batch_item_cnt = tf.constant(0)\n",
    "                end_points[\"batch_item_cnt_before\"] = batch_item_cnt  \n",
    "                               \n",
    "                u_hat_stopped = tf.stop_gradient(u_hat) \n",
    "                \n",
    "                end_of_batch = lambda batch_item_cnt, v, b_prior, u_hat_stopped, aggr, enable_b_prior_update: \\\n",
    "                                        tf.less(batch_item_cnt, batch_size)\n",
    "                route_op = tf.while_loop(body=update_prior, cond=end_of_batch, \n",
    "                                         loop_vars=[batch_item_cnt, \n",
    "                                                    v, b_prior, u_hat_stopped, aggr, enable_b_prior_update])\n",
    "                end_points[\"route_op\"] = route_op\n",
    "\n",
    "                [batch_item_cnt, _, b_prior_updated, _, aggr, enable_b_prior_update] = route_op\n",
    "                end_points[\"aggr\"] = aggr\n",
    "                end_points[\"batch_item_cnt_after\"] = batch_item_cnt\n",
    "             \n",
    "                b_prior_op = tf.assign(b_prior, b_prior_updated)\n",
    "                \n",
    "                s, c = get_s(b_prior_op, u_hat)\n",
    "                v = get_v(s)\n",
    "                \n",
    "                end_points[\"s_routed\"] = s\n",
    "                end_points[\"c_routed\"] = c\n",
    "                end_points[\"b_prior_routed\"] = b_prior_op\n",
    "                end_points[\"u_hat_routed\"] = u_hat\n",
    "                end_points[\"v_routed\"] = v\n",
    "                \n",
    "           \n",
    "            with tf.name_scope(\"Prepare_for_FC\"):\n",
    "                Y_one_hot_ex = tf.expand_dims(Y_one_hot, axis=1)\n",
    "                end_points[\"Y_one_hot_ex\"] = Y_one_hot_ex\n",
    "\n",
    "                v_masked = tf.squeeze(tf.matmul(Y_one_hot_ex, v))\n",
    "                end_points[\"v_masked\"] = v_masked\n",
    "\n",
    "            with tf.name_scope(\"Fully_connected\"):\n",
    "                fc_1 = tf.contrib.layers.fully_connected(inputs=v_masked, num_outputs=512, \n",
    "                                                         activation_fn=tf.nn.relu,\n",
    "                                                         weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                         biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "                end_points[\"fc_1\"] = fc_1\n",
    "\n",
    "                fc_2 = tf.contrib.layers.fully_connected(inputs=fc_1, num_outputs=1024, \n",
    "                                                         activation_fn=tf.nn.relu,\n",
    "                                                         weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                         biases_initializer=tf.zeros_initializer())\n",
    "                end_points[\"fc_2\"] = fc_2\n",
    "\n",
    "                fc_out = tf.contrib.layers.fully_connected(inputs=fc_2, num_outputs=784, \n",
    "                                                         activation_fn=tf.nn.sigmoid,\n",
    "                                                         weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                         biases_initializer=tf.zeros_initializer())\n",
    "                end_points[\"fc_out\"] = fc_out\n",
    "        \n",
    "        with tf.name_scope(\"Predictions\"):\n",
    "            v_norm = tf.norm(v, axis=2)\n",
    "            end_points[\"v_norm\"] = v_norm\n",
    "            predicted_labels = tf.argmax(v_norm, axis=1, output_type=tf.int32)\n",
    "            end_points[\"predicted_labels\"] = predicted_labels\n",
    "            \n",
    "        if is_train:\n",
    "\n",
    "            with tf.name_scope(\"Calculate_Losses\"):\n",
    "                lambd_a = 0.5\n",
    "                scaler = 0.0005\n",
    "\n",
    "                #margin_loss =Y_one_hot_ex \n",
    "\n",
    "\n",
    "                margin_loss_present = tf.multiply(tf.pow(tf.maximum(0., 0.9 - v_norm), 2), Y_one_hot)\n",
    "                end_points[\"margin_loss_present\"] = margin_loss_present\n",
    "\n",
    "                margin_loss_not_present = lambd_a * tf.multiply(tf.pow(tf.maximum(0., v_norm - 0.1), 2), (1 - Y_one_hot))\n",
    "                end_points[\"margin_loss_not_present\"] = margin_loss_not_present\n",
    "\n",
    "                total_margin_loss = tf.reduce_sum(tf.add(margin_loss_present, margin_loss_not_present), axis=1)\n",
    "                end_points[\"total_margin_loss\"] = total_margin_loss\n",
    "\n",
    "                reconstruction_loss = tf.multiply(scaler, \n",
    "                                                  tf.reduce_sum(\n",
    "                                                      tf.squared_difference(\n",
    "                                                          tf.reshape(X, shape=[-1, 784]), fc_out), axis=1))\n",
    "                end_points[\"reconstruction_loss\"] = reconstruction_loss\n",
    "\n",
    "\n",
    "                loss = tf.reduce_mean(tf.add(total_margin_loss, reconstruction_loss))\n",
    "                end_points[\"loss\"] = loss\n",
    "\n",
    "            with tf.name_scope(\"Calculate_Accuracy\"):\n",
    "\n",
    "\n",
    "\n",
    "                no_of_corrects = tf.cast(tf.reduce_sum(tf.cast(tf.equal(predicted_labels, Y), tf.int32)), tf.float32)\n",
    "\n",
    "                accuracy = no_of_corrects/batch_size\n",
    "                end_points[\"accuracy\"] = accuracy\n",
    "\n",
    "            if add_summaries:\n",
    "                tf.summary.scalar(\"loss_avg\", loss)\n",
    "                tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            grads = optimizer.compute_gradients(loss)\n",
    "            end_points[\"grads\"] = grads\n",
    "            optimize = optimizer.apply_gradients(grads)\n",
    "            end_points[\"optimize\"] = optimize\n",
    "\n",
    "            if add_summaries:\n",
    "                summaries_merged = tf.summary.merge_all()\n",
    "                end_points[\"summaries_merged\"] = summaries_merged\n",
    "\n",
    "            init = tf.global_variables_initializer()\n",
    "            end_points[\"init\"] = init\n",
    "\n",
    "        if print_shape:\n",
    "            for k in end_points.keys():\n",
    "                k_shape=\"NA\"\n",
    "                try:\n",
    "                    k_shape = str(end_points[k].shape)\n",
    "                except AttributeError:\n",
    "                    k_shape = type(end_points[k])\n",
    "\n",
    "                print(\"\\r {} shape={}\".format(k, k_shape))\n",
    "\n",
    "        return(graph, end_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the capsule network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data in memory and spliting to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images magic no: 2051,  No of images: 60000, Image rows: 28, Image cols: 28\n",
      "Labels magic no: 2049, No of items: 60000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "mnist = Mnist(path=\"..//\", train=True)\n",
    "data, lbls, lbls_one = mnist.get_batch(60000, scale=True)  \n",
    "all_idxs = np.arange(0, 60000)\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(all_idxs)\n",
    "number_of_training_samples = 60000 - batch_size\n",
    "assert(number_of_training_samples == int(number_of_training_samples))\n",
    "\n",
    "number_of_splits = number_of_training_samples/batch_size\n",
    "assert(number_of_splits == int(number_of_splits))\n",
    "\n",
    "train_idxs, test_idxs = all_idxs[0:number_of_training_samples], all_idxs[number_of_training_samples::]\n",
    "batches = np.split(train_idxs, number_of_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Training </strong><br/>\n",
    "Paper mentiones shifting images by two pixels but it is not done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "add_summaries = True\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph, end_points = build_graph(batch_size, is_train=True, add_summaries=add_summaries, print_shape=False)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(end_points[\"init\"])\n",
    "    \n",
    "    saver_train = tf.summary.FileWriter(\"..//log//train\", graph=graph)\n",
    "    saver_test = tf.summary.FileWriter(\"..//log//test\", graph=graph)\n",
    "    \n",
    "    other_variables = [v for v in graph.get_collection(\"variables\") if v.name=='b_prior:0']\n",
    "    trainable_variables = graph.get_collection(\"trainable_variables\")\n",
    "    trainable_variables.extend(other_variables)\n",
    "    \n",
    "    enable_b_prior_update = end_points[\"enable_b_prior_update\"]\n",
    "    model_saver = tf.train.Saver(trainable_variables)\n",
    "   \n",
    "    loss_report_counter = 0\n",
    "    report_period = 1\n",
    "          \n",
    "    for epoch in range(n_epochs):\n",
    "     \n",
    " \n",
    "        for batch_idxs in batches:\n",
    "            train_data=np.expand_dims(data[batch_idxs,:], -1)\n",
    "            train_lbls = lbls[batch_idxs]\n",
    "            \n",
    "            feed_dict = {end_points[\"X\"]:train_data, end_points[\"Y\"]:train_lbls, enable_b_prior_update:1}\n",
    "            \n",
    "            if add_summaries:\n",
    "                _, loss_train, accuracy_train, summaries_train = sess.run([end_points[\"optimize\"],\n",
    "                                                                           end_points[\"loss\"],\n",
    "                                                                           end_points[\"accuracy\"],\n",
    "                                                                           end_points[\"summaries_merged\"]], feed_dict=feed_dict)\n",
    "            else:\n",
    "                _, loss_train, accuracy_train = sess.run([end_points[\"optimize\"], \n",
    "                                                          end_points[\"loss\"],\n",
    "                                                          end_points[\"accuracy\"]], feed_dict=feed_dict)\n",
    "\n",
    "            loss_report_counter +=1\n",
    "   \n",
    "                   \n",
    "            if loss_report_counter%report_period==0:\n",
    "                report_period = 50\n",
    "                test_data=np.expand_dims(data[test_idxs,:], -1)\n",
    "                test_lbls = lbls[test_idxs]\n",
    "\n",
    "                feed_dict = {end_points[\"X\"]:test_data, end_points[\"Y\"]:test_lbls, enable_b_prior_update:0}\n",
    "                \n",
    "                if add_summaries:\n",
    "                    loss_test, accuracy_test, summaries_test = sess.run([end_points[\"loss\"], \n",
    "                                                        end_points[\"accuracy\"],\n",
    "                                                        end_points[\"summaries_merged\"]], feed_dict=feed_dict)\n",
    "                else:\n",
    "                    loss_test, accuracy_test = sess.run([end_points[\"loss\"], \n",
    "                                                        end_points[\"accuracy\"]], feed_dict=feed_dict)\n",
    "                               \n",
    "                print(\"--Epoch:{} Loss train:{} Accuracy train:{}  Accuracy test:{}--\".format(epoch,\n",
    "                                                                                    loss_train,\n",
    "                                                                                    accuracy_train,\n",
    "                                                                                    accuracy_test))\n",
    "                if add_summaries:\n",
    "                    saver_train.add_summary(summaries_train, global_step=loss_report_counter)\n",
    "                    saver_test.add_summary(summaries_test, global_step=loss_report_counter)\n",
    "                \n",
    "    np.save(\"../saved_model/trainable_variables.npy\", [v.name for v in trainable_variables])\n",
    "    print(\"Model saves under path {}\".format(model_saver.save(sess,save_path=\"../saved_model/capsnet\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unit testing for CapsNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong/>Testing the output of the conv 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 2\n",
    "graph, end_points = build_graph(batch_size, is_train=True, add_summaries=False, print_shape=False)\n",
    "\n",
    "mnist = Mnist(path=\"..//\", train=True)\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)    \n",
    "data = np.expand_dims(data, -1)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "   \n",
    "    init = end_points[\"init\"]\n",
    "    X = end_points[\"X\"]\n",
    "    Y = end_points[\"Y\"]\n",
    "    enable_b_prior_update = end_points[\"enable_b_prior_update\"]\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    feed_dict = {X:data, Y:lbls, enable_b_prior_update:1}\n",
    "\n",
    "    \n",
    "    \n",
    "    #\"Test output of conv 1\"\n",
    "    x, w, b, conv_1_act =  sess.run([X, graph.get_tensor_by_name('Conv/weights:0'), \n",
    "                                     graph.get_tensor_by_name('Conv/biases:0'), \n",
    "                                     end_points[\"conv_layer_1_act\"]], \n",
    "                                     feed_dict=feed_dict)\n",
    "    \n",
    "    assert(np.max([0, np.sum((np.matmul(np.expand_dims(x[0, 0:9, 0:9], axis=3), w) + b)[:,:,:,0])]) - \n",
    "           conv_1_act[0, 0, 0, 0]<0.0001)\n",
    "    assert(conv_1_act.shape == (2, 20, 20, 256))\n",
    "    print(\"Test output of conv1 passed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong/>Testing Capsules output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 2\n",
    "graph, end_points = build_graph(batch_size, is_train=True, add_summaries=False, print_shape=False)\n",
    "\n",
    "mnist = Mnist(path=\"..//\", train=True)\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)    \n",
    "data = np.expand_dims(data, -1)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "   \n",
    "    init = end_points[\"init\"]\n",
    "    X = end_points[\"X\"]\n",
    "    Y = end_points[\"Y\"]\n",
    "    enable_b_prior_update = end_points[\"enable_b_prior_update\"]\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    feed_dict = {X:data, Y:lbls, enable_b_prior_update:1}\n",
    "\n",
    "    #Test for capsule out values\"\n",
    "    #Capsule 8\n",
    "    \n",
    "    w, b, conv_1_act, capsule_7_act, capsule_2_act =  sess.run([graph.get_tensor_by_name('Capsule_Layer/Conv_7/weights:0'), \n",
    "                                     graph.get_tensor_by_name('Capsule_Layer/Conv_7/biases:0'), \n",
    "                                                 end_points[\"conv_layer_1_act\"],\n",
    "                                                 end_points[\"capsule_7_act\"],\n",
    "                                                 end_points[\"capsule_2_act\"]],\n",
    "                                                 feed_dict=feed_dict)\n",
    "    \n",
    "    assert(np.linalg.norm(np.sum((np.matmul(np.expand_dims(conv_1_act[0, 0:9, 0:9], axis=2), w) + b), axis=(0, 1, 2)) - \n",
    "           capsule_7_act[0, 0, 0, :])<0.0005)\n",
    "   \n",
    "    assert(capsule_7_act.shape==(2, 6, 6, 8))\n",
    "    \n",
    "\n",
    "    #Test for not sharing the weights\n",
    "    assert(np.linalg.norm(capsule_7_act[0, 0, 0, :] - capsule_2_act[0, 0, 0, :])>0.01)\n",
    "print(\"Capsule 8 output test passed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong/>Test for Squash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 2\n",
    "graph, end_points = build_graph(batch_size, is_train=True, add_summaries=False, print_shape=False)\n",
    "\n",
    "mnist = Mnist(path=\"..//\", train=True)\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)    \n",
    "data = np.expand_dims(data, -1)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "   \n",
    "    init = end_points[\"init\"]\n",
    "    X = end_points[\"X\"]\n",
    "    Y = end_points[\"Y\"]\n",
    "    enable_b_prior_update = end_points[\"enable_b_prior_update\"]\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    feed_dict = {X:data, Y:lbls, enable_b_prior_update:1}\n",
    "\n",
    "    #Test the squash function before routing\n",
    "    v, s, b_prior, W_t, capsule_7_act, u, u_hat = sess.run([end_points[\"v\"], \n",
    "                                                                 end_points[\"s\"],\n",
    "                                                                 end_points[\"b_prior\"],\n",
    "                                                                 end_points[\"W_t\"],\n",
    "                                                          \n",
    "                                                                 end_points[\"capsule_7_act\"],\n",
    "                                                                 end_points[\"u\"],\n",
    "                                                                 end_points[\"u_hat\"]], feed_dict=feed_dict)\n",
    "    \n",
    "    #Test for the first one digit 5\n",
    "    u = np.squeeze(u[0,252:288,...], axis=1)\n",
    "    W_t = np.squeeze(W_t[:, 252:288, 5, ...])\n",
    "   \n",
    "    u_hat_7_5 = np.sum(np.multiply(u, W_t), axis=2)\n",
    "    \n",
    "    assert(np.linalg.norm(u_hat_7_5 - u_hat[0,252:288, 5,...])<0.001)\n",
    "    b_prior_all_10= np.squeeze(b_prior[0,:,...])\n",
    "    b_prior_7_5 =np.squeeze(b_prior[0,:, 5,...])\n",
    "    c_7_5 = np.exp(b_prior_7_5)/np.sum(np.exp(b_prior_all_10), axis=1)\n",
    "    c_7_5 = np.expand_dims(c_7_5, axis=1)\n",
    "    u_hat_5 =  np.squeeze(u_hat[0, :, 5,...])\n",
    "    s_5 = np.sum(np.multiply(c_7_5, u_hat_5), axis=0)\n",
    "    \n",
    "    #Testing the vector for element 5\n",
    "    assert(np.linalg.norm(s_5 - s[0, 5,...])<0.00001)\n",
    "    \n",
    "    #test squash\n",
    "    v_5 = (np.power(np.linalg.norm(s_5), 2)/(1+np.power(np.linalg.norm(s_5), 2)))*(s_5/np.linalg.norm(s_5))\n",
    "    assert(np.linalg.norm(v_5) - np.linalg.norm(v[0, 5, ...])<0.00001)\n",
    "    \n",
    "print(\"Squash test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong/>Testing for routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 2\n",
    "graph, end_points = build_graph(batch_size, is_train=True, add_summaries=False, print_shape=False)\n",
    "\n",
    "mnist = Mnist(path=\"..//\", train=True)\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)    \n",
    "data = np.expand_dims(data, -1)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "   \n",
    "    init = end_points[\"init\"]\n",
    "    X = end_points[\"X\"]\n",
    "    Y = end_points[\"Y\"]\n",
    "    enable_b_prior_update = end_points[\"enable_b_prior_update\"]\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    feed_dict = {X:data, Y:lbls, enable_b_prior_update:1}\n",
    "    \n",
    "    #Test routing\n",
    "    \n",
    "    b_prior_var = graph.get_tensor_by_name(\"b_prior:0\")\n",
    "    \n",
    "    _, v_before, s_before, c_before, v_routed,s_routed, \\\n",
    "    c_routed, u_hat, u_hat_routed, b_prior_routed, aggr, batch_item_count = sess.run([\n",
    "                                     end_points[\"route_op\"],\n",
    "                                     end_points[\"v\"],\n",
    "                                     end_points[\"s\"],\n",
    "                                     end_points[\"c\"],\n",
    "                                     end_points[\"v_routed\"],\n",
    "                                     end_points[\"s_routed\"],\n",
    "                                     end_points[\"c_routed\"],\n",
    "                                     end_points[\"u_hat\"],\n",
    "                                     end_points[\"u_hat_routed\"],\n",
    "                                     end_points[\"b_prior_routed\"],\n",
    "                                     end_points[\"aggr\"],\n",
    "                                     end_points[\"batch_item_cnt_after\"]], feed_dict=feed_dict)\n",
    "    assert(batch_item_count == 2)\n",
    "    assert(np.linalg.norm(b_prior_routed) != 0)\n",
    "    \n",
    "    #Test that variable got updated\n",
    "\n",
    "    b_prior = sess.run(end_points[\"b_prior\"], feed_dict=feed_dict)\n",
    "    \n",
    "    assert(np.linalg.norm(b_prior) != 0)\n",
    "    assert(np.all(u_hat_routed==u_hat))\n",
    "    \n",
    "print(\"routing test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong/> Testing for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 2\n",
    "graph, end_points = build_graph(batch_size, is_train=True, add_summaries=False, print_shape=False)\n",
    "\n",
    "mnist = Mnist(path=\"..//\", train=True)\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)    \n",
    "data = np.expand_dims(data, -1)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "   \n",
    "    init = end_points[\"init\"]\n",
    "    X = end_points[\"X\"]\n",
    "    Y = end_points[\"Y\"]\n",
    "    enable_b_prior_update = end_points[\"enable_b_prior_update\"]\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    feed_dict = {X:data, Y:lbls, enable_b_prior_update:1}\n",
    "    #Test the losses\n",
    "    X,  margin_loss_present, margin_loss_not_present, total_margin_loss, \\\n",
    "    fc_out, reconstruction_loss, loss, v_norm, b_prior, v_out =\\\n",
    "                                                            sess.run([end_points[k] \n",
    "                                                              for k in [\"X\", \"margin_loss_present\",\n",
    "                                                                        \"margin_loss_not_present\",\n",
    "                                                                        'total_margin_loss', \"fc_out\",\n",
    "                                                                        'reconstruction_loss', \n",
    "                                                                        'loss', \"v_norm\", \"b_prior_routed\", \"v_routed\"]], feed_dict=feed_dict)\n",
    "   \n",
    "    \n",
    "    assert(margin_loss_present[0][5] - np.power(np.max([0, 0.9 - np.linalg.norm(v_out[0][5])]), 2)<0.00001)\n",
    "    \n",
    "    assert(np.linalg.norm(margin_loss_not_present[0][0] - \n",
    "                          0.5*np.power(np.max([0., np.linalg.norm(v_out[0][0])-0.1]), 2))<0.00001)\n",
    "    \n",
    "    assert(np.linalg.norm(total_margin_loss[0] - np.sum(margin_loss_present[0]+ margin_loss_not_present[0]))<0.00001)\n",
    "    \n",
    "    assert(np.abs(np.sum(np.square(fc_out[0] - X[0].reshape(784,)))*0.0005- reconstruction_loss[0])<0.01)\n",
    "    \n",
    "    assert(loss == np.mean(np.add(reconstruction_loss, total_margin_loss)))\n",
    "\n",
    "    print(\"Loss test passed\")\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong/> Testing if the batch_update_count gets reset for the next batch passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 2\n",
    "graph, end_points = build_graph(batch_size, is_train=True, add_summaries=False, print_shape=False)\n",
    "\n",
    "mnist = Mnist(path=\"..//\", train=True)\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)    \n",
    "data = np.expand_dims(data, -1)\n",
    "\n",
    "#test to see if batch_update_count gets reset for the next batch\n",
    "with tf.Session(graph=graph) as sess:\n",
    "   \n",
    "    init = end_points[\"init\"]\n",
    "    X = end_points[\"X\"]\n",
    "    Y = end_points[\"Y\"]\n",
    "    enable_b_prior_update = end_points[\"enable_b_prior_update\"]\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    feed_dict = {X:data, Y:lbls, enable_b_prior_update:1}\n",
    "    \n",
    "    batch_count_aft, batch_count_before = sess.run([end_points[\"batch_item_cnt_after\"],\n",
    "                                                   end_points[\"batch_item_cnt_before\"]], feed_dict=feed_dict) \n",
    "    assert(batch_count_before == 0)\n",
    "    assert(batch_count_aft == 2)\n",
    "    batch_count_aft, batch_count_before = sess.run([end_points[\"batch_item_cnt_after\"],\n",
    "                                                   end_points[\"batch_item_cnt_before\"]], feed_dict=feed_dict) \n",
    "    assert(batch_count_before == 0)\n",
    "    assert(batch_count_aft == 2)\n",
    "    \n",
    "print(\"test to see if batch_update_count gets reset for the next batch passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong/> Testing if b_prior changes during routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 2\n",
    "graph, end_points = build_graph(batch_size, is_train=True, add_summaries=False, print_shape=False)\n",
    "\n",
    "mnist = Mnist(path=\"..//\", train=True)\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size, scale=True)    \n",
    "data = np.expand_dims(data, -1)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "   \n",
    "    init = end_points[\"init\"]\n",
    "    X = end_points[\"X\"]\n",
    "    Y = end_points[\"Y\"]\n",
    "    enable_b_prior_update = end_points[\"enable_b_prior_update\"]\n",
    "    \n",
    "    feed_dict = {X:data, Y:lbls, enable_b_prior_update:1}\n",
    "    sess.run(init)\n",
    "   \n",
    "    v_routed, b_prior = sess.run([end_points[\"v_routed\"], end_points[\"b_prior_routed\"]], feed_dict=feed_dict)\n",
    "    assert(np.linalg.norm(b_prior)!=0)\n",
    "    \n",
    "    feed_dict[enable_b_prior_update] = 0\n",
    "    v_routed_not_routed, b_prior_aft_not_routed = sess.run([end_points[\"v_routed\"], end_points[\"b_prior\"]], feed_dict=feed_dict)\n",
    "    assert(np.all(b_prior==b_prior_aft_not_routed))\n",
    "    \n",
    "    print(\"b_prior update test passed\")\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images magic no: 2051,  No of images: 10000, Image rows: 28, Image cols: 28\n",
      "Labels magic no: 2049, No of items: 10000\n",
      "INFO:tensorflow:Restoring parameters from ../saved_model/capsnet\n",
      "Accuracy on test set: 0.9874\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = Mnist(path=\"..//\", train=False)\n",
    "batch_size = 200\n",
    "graph, end_points = build_graph(batch_size, is_train=False, add_summaries=False, print_shape=False)\n",
    "\n",
    "X = end_points[\"X\"]\n",
    "predicted_labels = end_points[\"predicted_labels\"]\n",
    "b_prior = end_points\n",
    "\n",
    "trainable_variables = np.load(file=\"../saved_model/trainable_variables.npy\")\n",
    "trainable_variables = [v for v in graph.get_collection(\"variables\") if v.name in trainable_variables]\n",
    "\n",
    "n_loops = int(10000/batch_size)\n",
    "\n",
    "assert(10000/batch_size == n_loops)\n",
    "corrects = 0\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    restorer = tf.train.Saver(trainable_variables)\n",
    "    restorer.restore(sess, \"../saved_model/capsnet\")\n",
    "    for i in range(n_loops):\n",
    "        data, lbls, _ = mnist.get_batch(batch_size=batch_size, scale=True)\n",
    "        data = np.expand_dims(data, axis=-1)\n",
    "        predicted_lbls = sess.run(end_points[\"predicted_labels\"], feed_dict={X:data})\n",
    "        corrects += np.sum(np.equal(lbls, predicted_lbls))\n",
    "\n",
    "        \n",
    "print(\"Accuracy on test set: {}\".format(corrects/10000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
