{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist:\n",
    "    def __init__(self, path, train=True):\n",
    "        if train:\n",
    "            image_file_name = \"train-images.idx3-ubyte\"\n",
    "            label_file_name = \"train-labels.idx1-ubyte\"\n",
    "        else:\n",
    "            image_file_name = \"t10k-images.idx3-ubyte\"\n",
    "            label_file_name = \"t10k-labels.idx1-ubyte\"\n",
    "            \n",
    "        self.f = open(\"{}\\\\{}\".format(path, image_file_name), \"rb\")\n",
    "        magic_number, number_of_images, image_rows, image_columns = int(self.f.read(4).hex(), 16), \\\n",
    "        int(self.f.read(4).hex(), 16), int(self.f.read(4).hex(), 16), int(self.f.read(4).hex(), 16)\n",
    "        print(\"Images magic no: {},  No of images: {}, Image rows: {}, Image cols: {}\".\\\n",
    "          format(magic_number, number_of_images, image_rows, image_columns))\n",
    "\n",
    "        self.flab = open(\"{}\\\\{}\".format(path, label_file_name), \"rb\")\n",
    "        magic_number, number_of_items = int(self.flab.read(4).hex(), 16), int(self.flab.read(4).hex(), 16)\n",
    "        print(\"Labels magic no: {}, No of items: {}\".format(magic_number, number_of_items))\n",
    "\n",
    "    def _get_next(self):\n",
    "        val = self.f.read(28*28)\n",
    "        if len(val)>0:\n",
    "            img = val.hex()\n",
    "            idxs = np.arange(0, len(img), 2).astype(int)\n",
    "            return(np.reshape([int(img[i:(i+2)], 16) for i in idxs], (28, 28)), int(self.flab.read(1).hex(), 16)) \n",
    "        else:\n",
    "            return(\"\", \"\")\n",
    "\n",
    "    def get_batch(self, batch_size=20):\n",
    "        im_lst = np.zeros(shape=[batch_size, 28, 28], dtype=\"float\")\n",
    "        lbl_lst = np.zeros(shape=[batch_size], dtype=\"int\")\n",
    "        lbl_one_hot_lst = np.zeros(shape=[batch_size, 10], dtype=\"int\")\n",
    "        for i in range(batch_size):\n",
    "            im, lbl = self._get_next()\n",
    "            if len(im)>0:\n",
    "                im_lst[i,...] = im\n",
    "                lbl_lst[i] = lbl\n",
    "                lbl_one_hot_lst[i,lbl] = 1\n",
    "        return im_lst, lbl_lst, lbl_one_hot_lst\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v shape (200, 10, 16)\n",
      "v shape (200, 10, 16)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "batch_size=200\n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(shape=[batch_size, 28, 28, 1], name=\"X\", dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=[batch_size], name=\"Y\", dtype=tf.float32)\n",
    "    Y_one_hot = tf.one_hot(indices=Y, )\n",
    "    \n",
    "    #First Layer\n",
    "    with tf.name_scope(\"Conv_Layer_1\"):\n",
    "        conv_1 = tf.contrib.layers.conv2d(inputs=X, \n",
    "                                          num_outputs=256,  \n",
    "                                          kernel_size=9, \n",
    "                                          stride=1, padding=\"VALID\", \n",
    "                                          activation_fn=tf.nn.relu, \n",
    "                                          weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                          biases_initializer=tf.zeros_initializer())\n",
    "    capsules = list()\n",
    "    \n",
    "    #In this implementation every capsule layer has unique weights\n",
    "    #I think the meaning of sharing is that in each plane [6, 6, 1]\n",
    "    #each cepsule output sharing the weights\n",
    "    \n",
    "    for n in range(32):\n",
    "        with tf.name_scope(\"Capsule_{}\".format(n)):\n",
    "            cap_1 = tf.contrib.layers.conv2d(inputs=conv_1,\n",
    "                                              num_outputs=8,\n",
    "                                              kernel_size=9, \n",
    "                                              stride=2, padding=\"VALID\", \n",
    "                                              activation_fn=None, \n",
    "                                              weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                              biases_initializer=tf.zeros_initializer())\n",
    "            \n",
    "        \n",
    "            capsules.append(tf.expand_dims(cap_1, axis=3))\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"DigitalCaps\"):\n",
    "        caps_all = tf.concat(capsules, axis=3)\n",
    "        caps_all = tf.reshape(caps_all, shape=[-1, 1152, 8])\n",
    "        caps_all = tf.expand_dims(caps_all, axis=2)\n",
    "\n",
    "        W = tf.get_variable(shape=[10, 1152, 8, 16], name=\"W\")\n",
    "        b = tf.get_variable(shape=[10, 1152, 1, 1], name=\"b\", initializer=tf.zeros_initializer())\n",
    "        b_prior = tf.get_variable(shape=[1, 1152, 10, 1], name=\"b_prior\", initializer=tf.zeros_initializer(), trainable=False)\n",
    "\n",
    "        caps_all_list = list()\n",
    "    \n",
    "        for cl in range(10):\n",
    "            caps_all_list.append(tf.map_fn(fn=lambda x: tf.add(tf.matmul(x, W[cl,:,:,:]), b[cl,:,:,:]), elems=caps_all))\n",
    "\n",
    "        u_hat = tf.concat(caps_all_list, axis=2)\n",
    "        \n",
    "    def get_s(b_prior, u_hat):\n",
    "        c = tf.nn.softmax(b_prior, dim=-1)\n",
    "        s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1)\n",
    "        return(s)\n",
    "\n",
    "\n",
    "    #Squash\n",
    "    def get_v(s):\n",
    "        norm_s = tf.norm(s)\n",
    "        norm_s_2 = tf.pow(norm_s, 2)\n",
    "        v = (norm_s_2*s)/((1+norm_s_2)*norm_s)\n",
    "        print(\"v shape {}\".format(v.shape))\n",
    "        return(v)\n",
    "    \n",
    "    s = get_s(b_prior, u_hat)\n",
    "    v = get_v(s)\n",
    " \n",
    "    with tf.name_scope(\"Routing\"):\n",
    "        \n",
    "        #Routing\n",
    "        batch_item_cnt = tf.constant(0)\n",
    "        end_of_batch = lambda batch_item_cnt, b_prior, u_hat: tf.less(batch_item_cnt, batch_size)\n",
    "\n",
    "        def update_prior(batch_item_cnt, b_prior, u_hat):\n",
    "\n",
    "            for i in range(3):\n",
    "                aggr = tf.reduce_sum(tf.multiply(v[batch_item_cnt, :, :], u_hat[batch_item_cnt, :, :]), axis=-1)\n",
    "                aggr = tf.expand_dims(aggr, axis=0)\n",
    "                aggr = tf.expand_dims(aggr, axis=3)\n",
    "                         \n",
    "                b_prior = tf.add(b_prior, aggr)\n",
    "\n",
    "            return([tf.add(batch_item_cnt, 1), b_prior, u_hat])\n",
    "\n",
    "\n",
    "        route_op = tf.while_loop(body=update_prior, cond=end_of_batch, loop_vars=[batch_item_cnt, b_prior, u_hat])\n",
    "        \n",
    "    with tf.control_dependencies(route_op):\n",
    "        v_out = get_v(get_s(b_prior, u_hat))\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images magic no: 2051,  No of images: 60000, Image rows: 28, Image cols: 28\n",
      "Labels magic no: 2049, No of items: 60000\n"
     ]
    }
   ],
   "source": [
    "mnist = Mnist(path=\".\\\\\", train=True)\n",
    "data, lbls, lbls_one = mnist.get_batch(batch_size)    \n",
    "data = np.expand_dims(data, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbls_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 12.332\n",
      "(200, 10, 16)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    #saver = tf.summary.FileWriter(filename_suffix=\"capsNet\", logdir=\"log\", graph=graph)\n",
    "    #saver.flush()\n",
    "    sess.run(init)\n",
    "\n",
    "    feed_dict = {X:data}\n",
    "    before = datetime.now()\n",
    "    rt = sess.run(v_out, feed_dict = feed_dict)\n",
    "    print(\"Time {}\\n{}\".format((datetime.now() - before).total_seconds(), rt.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = np.zeros(10)\n",
    "o[7]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
